# 35、微服务

## 微服务架构

![image-20250309131446408](5day-png/35微服务架构图.png)

![image-20250309131518993](5day-png/35微服务架构图1.png)

![image-20250309131700484](5day-png/35微服务架构图2.png)

![image-20250309131912127](5day-png/35微服务架构图5.png)



## 微服务

![image-20250309132319892](5day-png/35微服务架构演变.png)

### 单体(巨石)架构

- 传统架构（单机系统），一个项目一个工程：比如:商品、订单、支付、库存、登录、注册等等，统一部署，一个进程
- all in one的架构方式，把所有的功能单元放在一个应用里。然后把整个应用部署到一台服务器上。如果负载能力不行，将整个应用进行水平复制，进行扩展，然后通过负载均衡实现访问。
- Java实现：JSP、Servlet，打包成一个jar、war部署
- 易于开发和测试:也十分方便部署;当需要扩展时，只需要将war复制多份，然后放到多个服务器 上，再做个负载均衡就可以了。
- 如果某个功能模块出问题，有可能全站不可访问，修改Bug后、某模块功能修改或升级后，需要停掉整个服务，重新整体重新打包、部署这个应用war包，功能模块相互之间耦合度高,相互影响,不适合当今互联网业务功能的快速迭代。
- 特别是对于一个大型应用，我们不可能吧所有内容都放在一个应用里面，我们如何维护、如何分工合作都是问题。如果项目庞大，管理难度大
- Web应用服务器：开源的tomcat、jetty、glassfish。商用的有weblogic、websphere、Jboss
- 总结: 架构简单,部署方便,打包编译部署升级效率低下,各功能耦合度高,扩展困难,扩缩容不灵活,适合小型简单项目

### **MVC**

**JAVA的MVC**

MVC是模型(Model)、视图(View)、控制器(Controller)的简写，是一种软件设计规范。是将业务逻辑、数据、显示分离的方法来组织码。

MVC主要作用是降低了视图与业务逻辑间的双向偶合。

MVC不是一种设计模式，MVC是一种架构模式。当然不同的MVC存在差异。

MVC 采用template技术实现前端展示，前端和后端最终编译和整合为一个单体应用，和前后端分离方式分为两个独立项目是不同的。

- Model（模型）：数据模型，提供要展示的数据，因此包含数据和行为，可以认为是领域模型或JavaBean组件（包含数据和行为），不过现在一般都分离开来：Value Object（数据Dao） 和 服务层（行为Service）。也就是模型提供了模型数据查询和模型数据的状态更新等功能，包括数据和业务。
- View（视图）：负责进行模型的展示，一般就是我们见到的用户界面，客户想看到的东西。可通过JSP实现
- Controller（控制器）：接收用户请求，委托给模型进行处理（状态改变），处理完毕后把返回的模型数据返回给视图，由视图负责展示。也就是说控制器做了个调度员的工作。最终表现为Servlet

**Python的MTV**

- M: Model 数据模型，负责与数据库交互，
- T：Template 模板展示，负责呈现内容到浏览器，一般为.html文件
- V: view 流程逻辑，负责接收请求，相当于处理中枢，获取数据（来自用户） ，经过处理后选择合适的结果返回给用户。



### **SOA**

SOA（Service Oriented Architecture）是由多个服务组成的分布式系统

各个子系统之间没有采用统一的通信标准,导致系统间通信与数据交互变得异常复杂

各个服务之间通过**ESB(Enterprise Service Bus)**进行通信,ESB是一个由大量规则和原则集成的软件架构，可以将一系列不同的应用程集成到单个基础架构中，由于没有好的开源方案，只能使用商业公司的产品,因此成本很高

此外ESB属于重量级产品，部署规划异常笨重

ESB的单点依赖和商业ESB的费用问题反而成为了所有服务的瓶颈

![image-20250309132642648](5day-png/35SOAESB.png)

### 微服务

**微服务特点**

- 功能单一职责: 微服务拆分粒度更小，每一个服务都对应唯一的业务能力，做到单一职责，避免重复业务开发
- 访问面向服务:微服务对外暴露业务接口,通过接口实现服务的访问,而不能直接访问服务内的数据和功能
- 独立自治: 团队独立、技术独立、数据独立、部署独立
- 安全性: 服务调用需要实现隔离,容错,降级等安全保护,防止级联故障

**SOA和微服务比较**

<img src="5day-png/35SOA和微服务比较.png" alt="image-20250309132907747" style="zoom: 80%;" />

**微服务的优缺点**

**微服务优点：**

- 每个服务足够内聚，足够小，代码容易理解。这样能聚焦一个简单唯一的业务功能或业务需求。
- 开发简单、开发效率提高，一个服务可能就是专业的只干一件事，微服务能够被小团队单独开发，这个小团队可以是2到5人的开发人员组成
- 微服务是松耦合的，是有功能意义的服务，无论是在开发阶段或部署阶段都是独立的。
- 微服务能使用不同的语言开发
- 易于和第三方集成，微服务运行容易且灵活的方式集成自动部署，通过持续集成工具，如: Jenkins、Hudson、Bamboo
- 微服务易于被一个开发人员理解、修改和维护，这样小团队能够更关注自己的工作成果，无需通过合作才能体现价值
- 微服务允许你利用融合最新技术。微服务只是业务逻辑的代码，不会和HTML/CSS或其他界面组件混合，即前后端分离
- 每个微服务都有自己的存储能力，一般都有自己的独立的数据库，也可以有统一数据库

**微服务缺点**：

- 微服务把原有的一个项目拆分成多个独立工程，增加了开发、测试、运维、监控等的复杂度
- 微服务架构需要保证不同服务之间的数据一致性，引入了分布式事务和异步补偿机制，为设计和开发带来一定挑战
- 开发人员和运维需要处理分布式系统的复杂性，需要更强的技术能力
- 微服务适用于复杂的大系统，对于小型应用使用微服务，进行盲目的拆分只会增加其维护和开发成本

![image-20250309133305506](5day-png/35微服务.png)

**常见的微服务框架**

在多种编程语言中，有许多专门设计用来开发微服务的框架。以下是一些主流的微服务开发框架：

- **Spring Boot & Spring Cloud**：对于Java来说，Spring Boot是一个用来创建独立、基于Spring的生产级应用的框架。Spring Cloud则是一套基于Spring Boot的工具，专门用于构建分布式的、云原生的系统，包括配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等。
- **Go Kit & Micro**：对于Go来说，Go Kit和Go Micro都是专门设计用来创建微服务的工具集，提供了服务发现、负载均衡、同步调用、异步消息等功能。
- **Django & Flask**：对于Python来说，Django和Flask都是非常流行的框架，虽然它们原本并不专为微服务设计，但通过扩展和一些良好的设计实践，可以在这些框架上构建微服务。
- **Express.js**：对于JavaScript和Node.js来说，Express.js是一个非常流行的web应用框架，常被用来创建RESTful API服务。
- **.NET Core**：对于.NET来说，.NET Core是一个跨平台的、开源的框架，用于构建云应用和微服务。
- **Lagom**：Lagom是Lightbend公司为Scala和Java设计的一种用于创建响应式微服务的开源框架。
- **Akka**：Akka是一个用Scala和Java编写的开源工具包和运行时，用于构建并发和分布式应用。
- **gRPC**：gRPC是一个高性能、开源的通用远程过程调用（RPC）框架，由Google开发并开源，适合构建微服务。

这些框架都有各自的特点和优点，选择哪种框架取决于你的项目需求以及团队对某种语言或框架的熟悉度。



### **Spring Cloud**

![image-20250309135359238](5day-png/35spring cloud.png)

|              | Dubbo              | SpringCloud Netflix      | SpringCloudAlibaba       |
| ------------ | ------------------ | ------------------------ | ------------------------ |
| 注册中心     | zookeeper,Redis    | Eureka、Consul           | Nacos、Eureka            |
| 服务远程调用 | Dubbo 协议         | Feign (http协议)         | Dubbo、Feign             |
| 配置中心     | 无                 | SpringCloudConfig        | Nacos、SpringCloudConfig |
| 服务网关     | 无                 | SpringCloudGateway、Zuul | SpringCloudGateway、zuul |
| 服务监控安全 | dubbo-admin 功能少 | Hystrix                  | Sentinel                 |

![image-20250309135954636](5day-png/35SpringCloudAlibaba.png)

| 组件名称                 | 作用                                                       |
| ------------------------ | ---------------------------------------------------------- |
| Nacos                    | 服务注册和配置中心                                         |
| Sentinel                 | 客户端容错保护                                             |
| Dubbo                    | 远程调用                                                   |
| Seata                    | 分布式事务                                                 |
| RocketMQ                 | 消息队列                                                   |
| Alibaba Cloud SchedulerX | 分布式任务调度产品(商用)                                   |
| Alibaba Cloud OSS        | 阿里云对象存储服务（Object Storage Service)(商用)          |
| Alibaba Cloud SMS        | 阿里云短信服务(商用)                                       |
| Alibaba Cloud ACM        | 应用配置中心产品Application Configuration Management(商用) |

Spring Cloud原生及其他组件

| 组件名称      | 作用           |
| ------------- | -------------- |
| consul        | 服务注册中心   |
| config        | 分布式配置中心 |
| Gateway       | API服务网关    |
| Sleuth/Zipkin | 分布式链路追踪 |

### **服务网格** **Mesh** 边车

服务网格 Mesh 优势

- 把原本程序员需要关注的安全策略、负载均衡。流量控制、路由选择等基础能力下沉到了底层组件中，并提供了自动恢复的能力，让开发人员只需关注业务本身即可
- 提供各类服务编排,和服务调用的一整套完整解决方案，不仅仅针对JAVA语言写的服务,可以支持任何语言
- 提供了更灵活，可观察和安全的云原生解决方案，可以适应更复杂的应用程序需求
- 结合Kubernetes的相关云原生技术是未来的发展趋势

### **无服务架构** **Serverless**

Kuberetes 结合 ISTIO和Knative 是实现 Serverless 的流行的实现方案



### 微服务还是单体

单体优先策略一：模块化单体

合乎逻辑的方法是仔细设计一个单体应用，注意软件内部的模块化，包括 API 边界和数据存储方式。

做好这一点之后，从单体应用转向微服务是一件相对简单的事情。

单体优先策略二：边缘剥离

一种更常见的方法是从单体开始，逐渐剥离边缘的微服务。

这种方法可以在微服务架构的核心留下一个实质性的单体。

大多数新的开发都发生在微服务中，而单体是相对静止的。

单体优先策略三：整体替换

很少有人把这种做法看成是一种值得骄傲的做法，然而把单体作为一种牺牲性的架构来建造是有好处的。

不要害怕建造一个你会丢弃的单体应用，特别是如果一个单体应用能让你快速进入市场。

单体优先策略四：粗粒度服务

从几个粗粒度的服务开始，这些服务比你最终得到的微服务要大。

使用这些粗粒度的服务来习惯与多个服务一起工作。

同时享受这样一个事实，即这种粗粒度减少了你必须做的服务间重构的数量。

然后，随着边界的稳定，分解为更细粒度的服务。

“单体优先”的思想，目前已逐渐开始成为是业界普遍共识。现在已经属于后微服务时代!

在一个人数不多、资金不是无限充裕，需要快速将产品推向市场的团队，建议使用“单体优先”的实现方式。

在很多团队中，使用微服务其实是一种Hype Driven Development（炒作/简历驱动开发），不是为了真正为了解决业务问题。

使用“单体优先”，是一个务实的选择！试想如果是你自己创业，你会是选择“单体”还是“微服务”，那么请为你的企业进行务实的选择。



## 有状态和无状态

### **1. 什么是有状态（Stateful）？**

有状态系统会**记住**之前的操作或数据，并依赖这些信息来处理后续请求。

#### **特点**

✅ **依赖历史数据**（状态信息存储在服务器或数据库中）
 ✅ **必须维护会话信息**（Session、用户登录、事务）
 ✅ **扩展和故障恢复更复杂**（需要共享或同步状态）

#### **示例**

- **数据库（MySQL, PostgreSQL）**
   维护连接状态，事务操作依赖之前的执行记录。
- **有状态应用（Tomcat Session 服务器）**
   用户登录后，服务器保存会话（Session），下次访问需依赖该状态。
- **Kafka 消费者组**
   记录消息的消费偏移量（offset），确保消费进度。

#### **有状态架构示意**

```
1 ---> 服务器A（存储状态）
请求2 ---> 服务器A（需要之前的状态才能正确响应）
```

如果**服务器 A 挂掉**，用户会话丢失，必须有共享存储（如 Redis）来恢复状态。

#### **适用场景**

✔ **需要保持用户会话**（如购物车、登录认证）
 ✔ **事务处理**（如银行转账，必须保证一致性）
 ✔ **流式计算**（如 Kafka 消费偏移量）

------

### **2. 什么是无状态（Stateless）？**

无状态系统**不存储**任何请求的历史信息，每次请求都是独立的。

#### **特点**

✅ **请求独立**（服务器不存储用户状态，每次请求都包含完整信息）
 ✅ **易扩展**（任何服务器都能处理请求，负载均衡更简单）
 ✅ **故障恢复简单**（无状态服务可随时重启）

#### **示例**

- **HTTP 协议**（每次请求都是独立的，不存状态）
- **REST API**（所有请求必须包含必要信息，如 Token 认证）
- **DNS 解析**（查询某个域名的 IP 地址，每次查询都是独立的）
- **无状态 Web 服务器（Nginx, Cloudflare）**
   仅代理请求，不存储用户状态。

#### **无状态架构示意**

```
请求1 ---> 服务器A（无状态处理）
请求2 ---> 服务器B（任意服务器都能处理）
```

如果 **服务器 A 挂了**，服务器 B 仍能处理请求，不受影响。

#### **适用场景**

✔ **高并发、分布式系统**（微服务架构、云原生应用）
 ✔ **Web API 设计**（RESTful API，客户端携带所有信息）
 ✔ **负载均衡系统**（如 Nginx，任意后端都能处理请求）

------

### **3. 有状态 vs 无状态 对比**

| **对比项**       | **有状态（Stateful）**             | **无状态（Stateless）**      |
| ---------------- | ---------------------------------- | ---------------------------- |
| **是否存储状态** | 是，需要存储历史数据               | 否，每次请求独立             |
| **扩展性**       | 难扩展，需同步状态                 | 易扩展，请求可由任何实例处理 |
| **故障恢复**     | 复杂，需恢复状态                   | 简单，直接替换失败实例       |
| **典型应用**     | 数据库、Kafka 消费者               | HTTP、REST API、CDN          |
| **请求方式**     | 依赖之前的交互                     | 每次都包含完整信息           |
| **负载均衡**     | 需要会话粘性（Session Stickiness） | 任何服务器都能处理           |

------

### **4. 典型应用场景**

| **应用**       | **有状态（Stateful）**        | **无状态（Stateless）**        |
| -------------- | ----------------------------- | ------------------------------ |
| **数据库**     | MySQL、PostgreSQL（连接状态） | DynamoDB（无状态 API）         |
| **缓存**       | Redis 集群（主从同步）        | Redis 单实例（无状态 GET/SET） |
| **Web 服务器** | Tomcat（Session 管理）        | Nginx（无状态代理）            |
| **消息队列**   | Kafka（消费偏移量）           | RabbitMQ（消息即发即用）       |
| **认证系统**   | 服务器存 Session（有状态）    | JWT Token 认证（无状态）       |

------

### **5. 什么时候选择有状态 vs 无状态？**

#### **选择有状态（Stateful）**

✅ 需要**保持会话**（用户购物车、登录态）
 ✅ 需要**事务一致性**（银行转账、订单支付）
 ✅ 需要**持久化存储**（数据库、文件存储）

#### **选择无状态（Stateless）**

✅ 需要**高并发、高扩展**（云计算、负载均衡）
 ✅ 需要**轻量级请求**（REST API、微服务架构）
 ✅ 需要**自动恢复**（服务器无状态，随时可重启）

------

### **6. 结合实际案例**

#### **REST API（无状态） vs 传统 Session 认证（有状态）**

- 有状态方式（Session 认证）
  - 用户登录后，服务器生成一个 **Session ID** 存在内存中。
  - 客户端请求时携带 **Session ID**，服务器检查状态是否有效。
  - **问题**：如果服务器崩溃，Session 丢失，用户需要重新登录。
- 无状态方式（JWT Token）
  - 用户登录后，服务器生成一个 **JWT Token** 并返回给客户端。
  - 客户端每次请求都携带 **JWT Token**，服务器直接解析 Token 验证身份。
  - **优势**：无需服务器存储 Session，任何服务器都能处理请求。

------

### **7. 总结**

- **有状态（Stateful）**：需要记住历史数据，适用于**数据库、事务、会话管理**等。
- **无状态（Stateless）**：不存储请求状态，每次都是全新的，适用于**REST API、云原生应用、高并发系统**等。

一般情况下： ✅ **数据库、Session、Kafka** 需要有状态。
 ✅ **Web API、CDN、负载均衡** 采用无状态架构。
 在云计算和微服务架构中，**无状态更容易扩展**，但某些业务（如支付系统）仍然需要**有状态的事务处理**。

**最佳实践：**

- **尽可能设计成无状态（Stateless）**，方便扩展和恢复。
- **必须有状态的应用（Stateful）**，需要考虑如何同步状态（如数据库集群、Redis 共享会话）。



## zookeeper

### 工作原理

ZooKeeper 是一个分布式服务框架，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：命名服务、状态同步、配置中心、集群管理等。

### 功能

#### 命名服务

命名服务是分布式系统中比较常见的一类场景。命名服务是分布式系统最基本的公共服务之一。在分布式系统中，被命名的实体通常可以是集群中的机器、提供的服务地址或远程对象等——这些我们都可以统称它们为名字（Name），其中较为常见的就是一些分布式服务框架（如RPC、RMI）中的服务地址列表，通过使用命名服务，客户端应用能够根据指定名字来获取资源的实体、服务地址和提供者的信息等。

**数据模型**

```
在 Zookeeper 中，节点分为两类
第一类是指构成Zookeeper集群的主机，称之为主机节点
第二类则是指内存中zookeeper数据模型中的数据单元，用来存储各种数据内容，称之为数据节点 ZNode。
Zookeeper内部维护了一个层次关系(树状结构)的数据模型，它的表现形式类似于Linux的文件系统，甚至操作的种类都一致。
Zookeeper数据模型中有自己的根目录(/)，根目录下有多个子目录，每个子目录后面有若干个文件,由斜杠(/)进行分割的路径，就是一个ZNode,每个 ZNode上都会保存自己的数据内容和一系列属性信息.
```

![image-20250309143031879](5day-png/35数据模型.png)

#### 状态同步

每个节点除了存储数据内容和 node 节点状态信息之外，还存储了已经注册的APP 的状态信息，当有些节点或APP 不可用，就将当前状态同步给其他服务。

#### **配置中心**

现在我们大多数应用都是采用的是分布式开发的应用，搭建到不同的服务器上，我们的配置文件，同一个应用程序的配置文件一样，还有就是多个程序存在相同的配置，当我们配置文件中有个配置属性需要改变，需要改变每个程序的配置属性，这样会很麻烦的去修改配置，那么可用使用ZooKeeper 来实现配置中心

ZooKeeper 采用的是推拉相结合的方式：客户端向服务端注册自己需要关注的节点，一旦该节点的数据发生变更，那么服务端就会向相应的客户端发送Watcher事件通知，客户端接收到这个消息通知后，需要主动到服务端获取最新的数据。

Apollo（阿波罗）是携程框架部门研发的开源配置管理中心,此应用比较流行

#### **集群管理**

所谓集群管理，包括集群监控与集群控制两大块，前者侧重对集群运行时状态的收集，后者则是对集群进行操作与控制，在日常开发和运维过程中，我们经常会有类似于如下的需求：

- 希望知道当前集群中究竟有多少机器在工作。
- 对集群中每台机器的运行时状态进行数据收集。对集群中机器进行上下线操作。

ZooKeeper 具有以下两大特性:

- 客户端如果对ZooKeeper 的一个数据节点注册 Watcher监听，那么当该数据节点的内容或是其子节点列表发生变更时，ZooKeeper 服务器就会向已注册订阅的客户端发送变更通知。
- 对在ZooKeeper上创建的临时节点，一旦客户端与服务器之间的会话失效，那么该临时节点也就被自动清除。

Watcher（事件监听器）是 Zookeeper 中的一个很重要的特性。Zookeeper 允许用户在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候， ZooKeeper 服务端会将事件通知到感兴趣的客户端上去，该机制是 Zookeeper 实现分布式协调服务的重要特性。



### zookeeper服务流程

<img src="5day-png/35zookeeper服务流程.png" alt="image-20250309143433405" style="zoom:80%;" />

1. 生产者启动
2. 生产者注册至zookeeper
3. 消费者启动并订阅频道
4. zookeeper 通知消费者事件
5. 消费者调用生产者
6. 监控中心负责统计和监控服务状态



### **ZooKeeper** **单机部署**

```powershell
官方依赖介绍
https://zookeeper.apache.org/doc/r3.9.0/zookeeperAdmin.html#sc_systemReq
官方文档
https://zookeeper.apache.org/doc/r3.6.2/zookeeperStarted.html#sc_InstallingSingleMode
```

```bash
#配置java环境
[root@ubuntu2404 ~]#apt update && apt install -y openjdk-21-jdk
```

#### 包安装

```bash
#部署 ZooKeeper
#包安装
[root@ubuntu2404 ~]#apt list zookeeper
Listing... Done
zookeeper/noble,noble 3.9.1-1build2 all
[root@ubuntu2404 ~]#apt install -y zookeeper
```

#### 二进制安装

```bash
#二进制安装
https://zookeeper.apache.org/
版本： stable 和 current ，生产建议使用stable版本
#国内镜像源
https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/apache-zookeeper-3.8.4-bin.tar.gz

[root@ubuntu2404 ~]#wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/apache-zookeeper-3.8.4-bin.tar.gz
[root@ubuntu2404 ~]#ls
apache-zookeeper-3.8.4-bin.tar.gz
[root@ubuntu2404 ~]#tar xf apache-zookeeper-3.8.4-bin.tar.gz -C /usr/local/
[root@ubuntu2404 ~]#cd /usr/local/
[root@ubuntu2404 local]#ls
apache-zookeeper-3.8.4-bin  bin  etc  games  include  lib  man  sbin  share  src
[root@ubuntu2404 local]#ln -s apache-zookeeper-3.8.4-bin/ zookeeper
[root@ubuntu2404 local]#cd zookeeper
[root@ubuntu2404 zookeeper]#cp conf/zoo_sample.cfg conf/zoo.cfg 
[root@ubuntu2404 zookeeper]#cat conf/zoo.cfg | grep -Ev '^$|^#'
tickTime=2000			#服务器与服务器之间的单次心跳检测时间间隔，单位为毫秒
initLimit=10			#集群中leader 服务器与follower服务器初始连接心跳次数，即多少个 2000 毫秒
syncLimit=5				#leader 与follower之间检测发送和应答的心跳次数，如果该follower在时间段5*2000不能与leader进行通信，此follower将不可用
dataDir=/tmp/zookeeper	#自定义的zookeeper保存数据的目录
clientPort=2181			#客户端连接 Zookeeper 服务器的端口，Zookeeper会监听这个端口，接受客户端的访问请求
[root@ubuntu2404 zookeeper]#ls bin/
README.txt    zkCli.cmd  zkEnv.cmd  zkServer.cmd            zkServer.sh             zkSnapshotComparer.sh  zkSnapShotToolkit.sh  zkTxnLogToolkit.sh
zkCleanup.sh  zkCli.sh   zkEnv.sh   zkServer-initialize.sh  zkSnapshotComparer.cmd  zkSnapShotToolkit.cmd  zkTxnLogToolkit.cmd
```

```bash
[root@ubuntu2404 zookeeper]#cat conf/zoo.cfg
# The number of milliseconds of each tick
tickTime=2000		#这是 Zookeeper 的基本时间单位，单位是毫秒。它定义了 Zookeeper 的心跳时间，也就是 Zookeeper 中节点之间通信的最小时间间隔。
# The number 
of ticks that the initial 
# synchronization phase can take
initLimit=10		#这是 Zookeeper 集群中初始化连接的最大限制。它指的是在 Zookeeper 启动时，集群成员之间可以用来完成初始化连接的时间限制。
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5			#这是 Zookeeper 集群成员之间同步的最大时间限制，单位是 tickTime。它设置了 Zookeeper 集群同步请求的最长时间。
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir=/data/zookeeper	#这是 Zookeeper 数据存储目录。Zookeeper 会将事务日志和快照文件保存在这个目录下。
dataLogDir=/usr/local/zookeeper/logs	#指定日志路径，默认与 dataDir 一致,事务日志对性能影响非常大，强烈建议事务日志目录和数据目录分开，如果后续修改路径，需要先删除中dataDir中旧的事务日志，否则可能无法启动
# the port at which the clients will connect
clientPort=2181		#这是 Zookeeper 客户端连接的端口，默认值是 2181。
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60		#控制每个客户端连接到 Zookeeper 的最大连接数。
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3	
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1
#autopurge.snapRetainCount=3 和 autopurge.purgeInterval=1: 用来自动清理过期的快照文件和事务日志文件。

## Metrics Providers
#
# https://prometheus.io Metrics Exporter
metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider		#这个配置启用了 Prometheus 监控。PrometheusMetricsProvider 是用来从 Zookeeper 导出 Prometheus 监控数据的类。
metricsProvider.httpHost=0.0.0.0	#这个配置表示 Prometheus 将通过绑定到所有可用的网络接口来提供监控数据。
metricsProvider.httpPort=7000		#Prometheus 用于监听的端口。
metricsProvider.exportJvmInfo=true	#设置为 true，表示会导出 JVM 相关的信息，供 Prometheus 监控。
```

```bash
[root@ubuntu2404 zookeeper]#cat conf/zoo.cfg | grep -Ev '^$|^#'
tickTime=2000
of ticks that the initial 
initLimit=10
syncLimit=5
dataDir=/data/zookeeper
clientPort=2181
metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider
metricsProvider.httpHost=0.0.0.0
metricsProvider.httpPort=7000
metricsProvider.exportJvmInfo=true
```

**启动Zookeeper**

```bash
[root@ubuntu2404 zookeeper]#./bin/zkServer.sh 
/usr/bin/java
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
Usage: ./bin/zkServer.sh [--config <conf-dir>] {start|start-foreground|stop|version|restart|status|print-cmd}
[root@ubuntu2404 zookeeper]#./bin/zkServer.sh start
/usr/bin/java
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[root@ubuntu2404 zookeeper]#./bin/zkServer.sh status
/usr/bin/java
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: standalone

#前台启动观察启动过程
[root@ubuntu2404 zookeeper]#./bin/zkServer.sh start-foreground
```

```bash
#先停止服务
[root@ubuntu2404 zookeeper]#./bin/zkServer.sh stop
/usr/bin/java
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
Stopping zookeeper ... STOPPED

#准备service文件
[root@ubuntu2404 zookeeper]#vim /lib/systemd/system/zookeeper.service
[Unit]
Description=zookeeper.service
After=network.target

[Service]
Type=forking
ExecStart=/usr/local/zookeeper/bin/zkServer.sh start
ExecStop=/usr/local/zookeeper/bin/zkServer.sh stop
ExecReload=/usr/local/zookeeper/bin/zkServer.sh restart

[Install]
WantedBy=multi-user.target

[root@ubuntu2404 ~]#systemctl daemon-reload 
[root@ubuntu2404 ~]#systemctl enable --now zookeeper.service 
```



#### 脚本安装

```bash
[root@ubuntu2404 ~]#cat install_zookeeper_single_node.sh 
#!/bin/bash
#

#支持在线和离线安装

ZK_VERSION=3.9.3
#ZK_VERSION=3.9.2
#ZK_VERSION=3.9.1
#ZK_VERSION=3.9.0
#ZK_VERSION=3.8.1
#ZK_VERSION=3.8.0
#ZK_VERSION=3.6.3
#ZK_VERSION=3.7.1
#ZK_URL=https://archive.apache.org/dist/zookeeper/zookeeper-${ZK_VERSION}/apache-zookeeper-${ZK_VERSION}-bin.tar.gz
ZK_URL=https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-${ZK_VERSION}/apache-zookeeper-${ZK_VERSION}-bin.tar.gz
#ZK_URL="https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/apache-zookeeper-${ZK_VERSION}-bin.tar.gz"
#ZK_URL="https://downloads.apache.org/zookeeper/stable/apache-zookeeper-${ZK_VERSION}-bin.tar.gz"

INSTALL_DIR=/usr/local/zookeeper


HOST=`hostname -I|awk '{print $1}'`

.  /etc/os-release

color () {
    RES_COL=60
    MOVE_TO_COL="echo -en \\033[${RES_COL}G"
    SETCOLOR_SUCCESS="echo -en \\033[1;32m"
    SETCOLOR_FAILURE="echo -en \\033[1;31m"
    SETCOLOR_WARNING="echo -en \\033[1;33m"
    SETCOLOR_NORMAL="echo -en \E[0m"
    echo -n "$1" && $MOVE_TO_COL
    echo -n "["
    if [ $2 = "success" -o $2 = "0" ] ;then
        ${SETCOLOR_SUCCESS}
        echo -n $"  OK  "    
    elif [ $2 = "failure" -o $2 = "1"  ] ;then 
        ${SETCOLOR_FAILURE}
        echo -n $"FAILED"
    else
        ${SETCOLOR_WARNING}
        echo -n $"WARNING"
    fi
    ${SETCOLOR_NORMAL}
    echo -n "]"
    echo 
}

install_jdk() {
    java -version &>/dev/null && { color "JDK 已安装!" 1 ; return;  }
    if command -v yum &>/dev/null ; then
        yum -y install java-1.8.0-openjdk-devel || { color "安装JDK失败!" 1; exit 1; }
    elif command -v apt &>/dev/null ; then
        apt update
        #apt install openjdk-11-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        apt install openjdk-8-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
    else
       color "不支持当前操作系统!" 1
       exit 1
    fi
    java -version && { color "安装 JDK 完成!" 0 ; } || { color "安装JDK失败!" 1; exit 1; } 
}


install_zookeeper() {
    if [ -f apache-zookeeper-${ZK_VERSION}-bin.tar.gz ] ;then
        cp apache-zookeeper-${ZK_VERSION}-bin.tar.gz /usr/local/src/
    else
        wget -P /usr/local/src/ --no-check-certificate $ZK_URL || { color  "下载失败!" 1 ;exit ; }
    fi
    tar xf /usr/local/src/${ZK_URL##*/} -C /usr/local
    ln -s /usr/local/apache-zookeeper-*-bin/ ${INSTALL_DIR}
   #echo "PATH=${INSTALL_DIR}/bin:$PATH" >  /etc/profile.d/zookeeper.sh
    echo "PATH=${INSTALL_DIR}/bin:$PATH" >>  /etc/profile
    #.  /etc/profile.d/zookeeper.sh
    mkdir -p ${INSTALL_DIR}/data 
    cat > ${INSTALL_DIR}/conf/zoo.cfg <<EOF
tickTime=2000
initLimit=10
syncLimit=5
dataDir=${INSTALL_DIR}/data
dataLogDir=${INSTALL_DIR}/logs
clientPort=2181
maxClientCnxns=128
autopurge.snapRetainCount=3
autopurge.purgeInterval=24
EOF
        cat > /lib/systemd/system/zookeeper.service <<EOF
[Unit]
Description=zookeeper.service
After=network.target

[Service]
Type=forking
#Environment=${INSTALL_DIR}
ExecStart=${INSTALL_DIR}/bin/zkServer.sh start
ExecStop=${INSTALL_DIR}/bin/zkServer.sh stop
ExecReload=${INSTALL_DIR}/bin/zkServer.sh restart

[Install]
WantedBy=multi-user.target
EOF
    systemctl daemon-reload
    systemctl enable --now  zookeeper.service
    systemctl is-active zookeeper.service
        if [ $? -eq 0 ] ;then 
        color "zookeeper 安装成功!" 0  
    else 
        color "zookeeper 安装失败!" 1
        exit 1
    fi   
}


install_jdk

install_zookeeper
```

 

### zookeeper集群

<img src="5day-png/35zookeeper集群结构.png" alt="image-20250309165328414" style="zoom:67%;" />

Zookeeper集群基于Master/Slave的模型

处于主要地位负责处理写操作)的主机称为Leader节点，处于次要地位主要负责处理读操作的主机称为 follower 节点

Zookeeper的读写机制

- **写操作**：在Zookeeper中，写操作（如创建、删除、更新节点等）通常由Leader节点处理。当客户端发送写请求时，Leader会先将其写入本地日志，并尝试将其同步到集群中的其他Follower节点。一旦大多数在总数过半的节点成功应用了这个写操作，Leader就会认为这次写操作是成功的，并返回结果给客户端。
- **读操作**：读操作可以由Leader或Follower节点处理。为了提高读取性能，Zookeeper通常会采用异步复制的方式，即Follower节点会异步地从Leader节点复制最新的数据。这样，即使Follower节点的数据不是最新的，它仍然可以处理读请求，并在后续从Leader节点获取更新的数据。生产中读取的方式在提高性能和容错性方面具有显著优势，同时也在一定程度上牺牲了数据的一致性。然而，对于大多数应用场景来说，这种折衷是合理的。

对于n台server,每个server都知道彼此的存在。只要有>n/2台server节点可用，整个zookeeper系统保持可用。因此zookeeper集群通常由奇数台Server节点组成

**集群中节点数越多，写性能越差，读性能越好**

<img src="5day-png/35zookeeper集群1.png" alt="image-20250309165601384" style="zoom:50%;" />

<img src="5day-png/35zookeeper集群角色.png" alt="image-20250309165644841" style="zoom:67%;" />

| 序号 | 角色             | 职责描述                                                     |
| ---- | ---------------- | ------------------------------------------------------------ |
| 1    | 领导者(Leader)   | 负责处理写入请求的，事务请求的唯一调度和处理者,负责进行投票发起和决议，更新系统状态 |
| 2    | 跟随者(Follower) | 接收客户请求并向客户端返回结果，在选Leader过程中参与投票     |
| 3    | 观察者(Observer) | 转交客户端写请求给leader节点，和同步leader状态和Follower唯一区别就是不参与Leader投票,也不参与写操作的"过半写成功"策略 |
| 4    | 学习者(Learner)  | 和leader进行状态同步的节点统称Learner，包括:Follower和Observer |
| 5    | 客户端(client)   | 请求发起方                                                   |

**选举过程**

**节点角色状态：**

- LOOKING：寻找 Leader 状态，处于该状态需要进入选举流程
- LEADING：领导者状态，处于该状态的节点说明是角色已经是Leader 
- FOLLOWING：跟随者状态，表示 Leader已经选举出来，当前节点角色是follower 
- OBSERVER：观察者状态，表明当前节点角色是 observer

**选举** **ID**：

- ZXID（zookeeper transaction id）：每个改变 Zookeeper状态的操作都会自动生成一个对应的zxid。ZXID最大的节点优先选为Leader
- myid：服务器的唯一标识(SID)，通过配置 myid 文件指定，集群中唯一,当ZXID一样时,myid大的节点优先选为Leader

**ZooKeeper** **集群选举过程：**

- 当集群中的 zookeeper 节点启动以后，会根据配置文件中指定的 zookeeper节点地址进行leader 选择操作，过程如下：
- 每个zookeeper 都会发出投票，由于是第一次选举leader，因此每个节点都会把自己当做leader 角色进行选举，每个zookeeper 的投票中都会包含自己的myid和zxid，此时zookeeper 1 的投票为myid 为 1，初始zxid有一个初始值0x0，后期会随着数据更新而自动变化，zookeeper 2 的投票为myid 为2，初始zxid 为初始生成的值。
- 每个节点接受并检查对方的投票信息，比如投票时间、是否状态为LOOKING状态的投票。

- 对比投票，优先检查zxid，如果zxid 不一样则 zxid 大的为leader，如果zxid相同则继续对比myid，myid 大的一方为 leader

  成为 Leader 的必要条件： Leader 要具有最高的zxid；当集群的规模是 n 时，集群中大多数的机器（至少n/2+1）得到响应并从follower 中选出的 Leader。

心跳机制：Leader 与 Follower 利用 PING 来感知对方的是否存活，当 Leader无法响应PING 时，将重新发起 Leader 选举。

当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB(Zookeeper Atomic Broadcast) 协议就会进入恢复模式并选举产生新的Leader服务器。这个过程大致如下：

- Leader Election（选举阶段）：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。
- Discovery（发现阶段）：在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。
- Synchronization（同步阶段）:同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准leader 才会成为真正的 leader。
- Broadcast（广播阶段） ：到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步

在分布式系统中，有多种协议被设计来解决一致性问题，Paxos、Raft、ZAB 等分布式算法经常会被称作是“强一致性”的分布式共识协议

强一致指的是尽管系统内部节点可以存在不一致的状态，但从系统外部看来，不一致的情况并不会被观察到，所以整体上看系统是强一致性的,对于CAP模型，实现的CP。

- **ZAB**（**ZooKeeper Atomic Broadcast 原子广播）**：Zab协议是由Apache ZooKeeper项目提出的一种原子广播协议，是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。它主要用于构建一个高可用、高性能的分布式协调服务。Zab协议保证了分布式过程中的消息顺序一致性和崩溃恢复能力。它主要用在主-备模式的系统中，例如Kafka在2.8.0版本之前依赖ZooKeeper作为其元数据的存储，并使用Zab协议来保证数据的一致性。
- **Paxos**：Paxos（帕克索斯）是Leslie Lamport( 在2013 年获得计算界的最高奖图灵奖) 早在1990年提出的分布式一致性算法之一，它能解决分布式环境中的一致性问题。Paxos算法可以保证在非同步和非可靠的分布式系统中，节点间可以达成一致的决定。不过，Paxos的描述和实现被公认为相对复杂，这也是Raft等其他一致性算法出现的原因。使用Paxos或其变体协议的有如下应用Google Chubby(Google的一个锁服务),Apache Cassandra(度可扩展的分布式数据库),Microsoft Azure Cosmos DB(Microsoft Azure提供的一个全球分布式、多模型的数据库服务),Riak(Microsoft Azure提供的一个全球分布式、多模型的数据库服务)
- **Raft**：Raft协议由Diego Ongaro和John Ousterhout在2013年提出的。他们在斯坦福大学就读博士期间设计并研究了Raft。他们的主要目标是创建一个与Paxos功能相同，但更容易理解和实现的一致性算法。Raft 是一个为分布式系统提供一致性的算法。与Paxos相比，Raft的主要目标是提供一种更加易于理解和实现的一致性算法。Raft通过选举算法确保了分布式系统中的领导者唯一性。所有的写操作都通过领导者完成，这样就可以确保所有复制节点上的数据一致性。一些知名的分布式系统，如：kafka，etcd，nacos和Consul，都采用了Raft算法。

**Gossip** **谣言协议**

相比 Paxos、Raft 等算法，Gossip 的过程十分简单，它可以看作是以下两个步骤的简单循环：

- 如果有某一项信息需要在整个网络中所有节点中传播，那从信息源开始，选择一个固定的传播周期（譬如 1 秒），随机选择它相连接的k 个节点（称为 Fan-Out）来传播消息。
- 每一个节点收到消息后，如果这个消息是它之前没有收到过的，将在下一个周期内，选择除了发送消息给它的那个节点外的其他相邻 k 个节点发送相同的消息，直到最终网络中所有节点都收到了消息，尽管这个过程需要一定时间，但是理论上最终网络的所有节点都会拥有相同的消息。

**ZooKeeper** **集群特性**

整个集群中只要有超过集群数量一半的 zookeeper工作是正常的，那么整个集群对外就是可用的

假如有 2 台服务器做了一个 Zookeeper 集群，只要有任何一台故障或宕机，那么这个 ZooKeeper集群就不可用了，因为剩下的一台没有超过集群一半的数量，但是假如有三台zookeeper 组成一个集群， 那么损坏一台就还剩两台，大于 3台的一半，所以损坏一台还是可以正常运行的，但是再损坏一台就只剩一台集群就不可用了。那么要是 4 台组成一个zookeeper集群，损坏一台集群肯定是正常的，那么损坏两台就还剩两台，那么2台不大于集群数量的一半，所以 3 台的 zookeeper 集群和 4 台的 zookeeper集群损坏两台的结果都是集群不可用，以此类推 5 台和 6 台以及 7 台和 8台都是同理

**Zookeeper** **事务日志和快照**

ZooKeeper集群中的每个服务器节点每次接收到写操作请求时，都会先将这次请求发送给leader

leader将这次写操作转换为带有状态的事务，然后leader会对这次写操作广播出去以便进行协调。

当协调通过(大多数节点允许这次写)后，leader通知所有的服务器节点，让它们将这次写操作应用到内存数据库中，并将其记录到事务日志中。

当事务日志记录的次数达到一定数量后(默认10W次)，就会将内存数据库序列化一次，使其持久化保存到磁盘上，序列化后的文件称为"快照文件"。

每次拍快照都会生成新的事务日志。



```bash
#安装java环境
[root@ubuntu2404 ~]#apt update && apt install -y openjdk-21-jdk

#下载二进制文件
[root@ubuntu2404 ~]#wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/apache-zookeeper-3.8.4-bin.tar.gz
[root@ubuntu2404 ~]#tar xf apache-zookeeper-3.8.4-bin.tar.gz -C /usr/local/
[root@ubuntu2404 ~]#cd /usr/local/
[root@ubuntu2404 local]#ln -s apache-zookeeper-3.8.4-bin/ zookeeper
[root@ubuntu2404 local]#cd zookeeper
```

```bash
[root@ubuntu2404 zookeeper]#mv conf/zoo_sample.cfg conf/zoo.cfg 
[root@ubuntu2404 zookeeper]#vim conf/zoo.cfg 
[root@ubuntu2404 zookeeper]#cat conf/zoo.cfg 
# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir=/usr/local/zookeeper/data
# the port at which the clients will connect
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1

## Metrics Providers
#
# https://prometheus.io Metrics Exporter
#metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider
#metricsProvider.httpHost=0.0.0.0
#metricsProvider.httpPort=7000
#metricsProvider.exportJvmInfo=true

server.1=10.0.0.201:2888:3888
server.2=10.0.0.202:2888:3888
server.3=10.0.0.203:2888:3888

[root@ubuntu2404 zookeeper]#mkdir /usr/local/zookeeper/data/
[root@ubuntu2404 zookeeper]#echo 1 > /usr/local/zookeeper/data/myid

#另外两台执行上面的操作
echo 2 > /usr/local/zookeeper/data/myid
echo 3 > /usr/local/zookeeper/data/myid
```

```bash
#创建service文件
[root@ubuntu2404 zookeeper]#vim /lib/systemd/system/zookeeper.service
[Unit]
Description=zookeeper.service
After=network.target

[Service]
Type=forking
ExecStart=/usr/local/zookeeper/bin/zkServer.sh start
ExecStop=/usr/local/zookeeper/bin/zkServer.sh stop
ExecReload=/usr/local/zookeeper/bin/zkServer.sh restart

[Install]
WantedBy=multi-user.target

[root@ubuntu2404 ~]#systemctl daemon-reload 
[root@ubuntu2404 ~]#systemctl enable --now zookeeper.service 
```



### 客户端连接

```bash
#客户端连接
[root@ubuntu2404 ~]#/usr/local/zookeeper/bin/zkCli.sh
[zk: localhost:2181(CONNECTED) 13] help
ZooKeeper -server host:port -client-configuration properties-file cmd args
        addWatch [-m mode] path # optional mode is one of [PERSISTENT, PERSISTENT_RECURSIVE] - default is PERSISTENT_RECURSIVE
        addauth scheme auth
        close 
        config [-c] [-w] [-s]
        connect host:port
        create [-s] [-e] [-c] [-t ttl] path [data] [acl]
        delete [-v version] path
        deleteall path [-b batch size]
        delquota [-n|-b|-N|-B] path
        get [-s] [-w] path
        getAcl [-s] path
        getAllChildrenNumber path
        getEphemerals path
        history 
        listquota path
        ls [-s] [-w] [-R] path
        printwatches on|off
        quit 
        reconfig [-s] [-v version] [[-file path] | [-members serverID=host:port1:port2;port3[,...]*]] | [-add serverId=host:port1:port2;port3[,...]]* [-remove serverId[,...]*]
        redo cmdno
        removewatches path [-c|-d|-a] [-l]
        set [-s] [-v version] path data
        setAcl [-s] [-v version] [-R] path acl
        setquota -n|-b|-N|-B val path
        stat [-w] path
        sync path
        version 
        whoami 
[zk: localhost:2181(CONNECTED) 0] ls /
[zookeeper]          
[zk: localhost:2181(CONNECTED) 1] get /zookeeper
[zk: localhost:2181(CONNECTED) 2] create /app1
Created /app1
[zk: localhost:2181(CONNECTED) 3] ls /
[app1, zookeeper]
[zk: localhost:2181(CONNECTED) 4] set /app1 hello
[zk: localhost:2181(CONNECTED) 6] ls /
[app1, zookeeper]
[zk: localhost:2181(CONNECTED) 7] get /app1 
hello
[zk: localhost:2181(CONNECTED) 8] create /app1/cless
Created /app1/cless
[zk: localhost:2181(CONNECTED) 9] ls /app1 
[cless]
[zk: localhost:2181(CONNECTED) 10] set /app1/cless 61
[zk: localhost:2181(CONNECTED) 12] get /app1/cless 
61
```

### 图形工具连接

**图形化客户端** **ZooInspector**

```
https://github.com/zzhang5/zooinspector
https://gitee.com/lbtooth/zooinspector.git
```

```bash
编译 zooinspector
build
$git clone https://github.com/zzhang5/zooinspector.git
$cd zooinspector/
$mvn clean package -Dmaven.test.skip=true

Run
$chmod +x target/zooinspector-pkg/bin/zooinspector.sh
$target/zooinspector-pkg/bin/zooinspector.sh
```

```bash
#注意：只支持JAVA-8，不支持JAVA-11以上版本
[root@ubuntu2404 ~]#apt update && apt -y install openjdk-8-jdk
[root@ubuntu2404 ~]#apt update && apt -y install maven
[root@ubuntu2404 ~]#vim /etc/maven/settings.xml 
<mirrors>
    <mirror>
        <id>aliyunmaven</id>
        <mirrorOf>*</mirrorOf>
        <name>Aliyun Maven</name>
        <url>https://maven.aliyun.com/repository/public</url>
    </mirror>
</mirrors>

```



### python连接

```python
[root@ubuntu2404 ~]#apt update && apt -y install python3 python3-kazoo
[root@ubuntu2404 ~]#cat zookeeper_test.py 
#!/usr/bin/python3
from kazoo.client import KazooClient

zk = KazooClient(hosts='10.0.0.200:2181')
zk.start()
# 创建节点：makepath 设置为 True ，父节点不存在则创建，其他参数不填均为默认
zk.create('/zkapp/test',b'this is a test',makepath=True)

#查看指定数据
data=zk.get('/zkapp/test')
print(data)

#查看所有数据
all_data=zk.get_children('/')
print(all_data)

# 操作完后关闭zk连接
zk.stop()

[root@ubuntu2404 ~]#python3 zookeeper_test.py 
```



### **nc** **访问** **ZooKeeper** **管理**

ZooKeeper支持某些特定的四字命令字母与其的交互。它们大多是查询命令，用来获取 ZooKeeper服务的当前状态及相关信息。用户在客户端可以通过 netcat 或telnet向zookeeper发送下面命令

```bash
#常见命令
conf #输出相关服务配置的详细信息
cons #列出所有连接到服务器的客户端的完全的连接/会话的详细信息
envi #输出关于服务环境的详细信息
dump #列出未经处理的会话和临时节点
stat #查看哪个节点被选择作为Follower或者Leader
ruok #测试是否启动了该Server，若回复imok表示已经启动
mntr #输出一些运行时信息
reqs #列出未经处理的请求
wchs #列出服务器watch的简要信息
wchc #通过session列出服务器watch的详细信息
wchp #通过路径列出服务器watch的详细信息
srvr #输出服务的所有信息
srst #重置服务器统计信息
kill #关掉Server，当前版本无法关闭
isro #查看该服务的节点权限信息
```

**命令的安全限制**

```bash
#默认情况下，这些4字命令有可能会被拒绝，提示如下报错
xxxx is not executed because it is not in the whitelist.

#解决办法:在 zoo.cfg文件中添加如下配置,如果是集群需要在所有节点上添加下面配置
# vim conf/zoo.cfg
4lw.commands.whitelist=*

#在服务状态查看命令中有很多存在隐患的命令，为了避免生产中的安全隐患，要对这些"危险"命令进行一些安全限制，只需要编辑服务的zoo.cfg文件即可

# vim conf/zoo.cfg
4lw.commands.whitelist=conf,stat,ruok,isro
```

```bash
[root@ubuntu2404 ~]#nc 127.1 2181
conf
clientPort=2181
secureClientPort=-1
dataDir=/usr/local/zookeeper/logs/version-2
dataDirSize=67110187
dataLogDir=/usr/local/zookeeper/logs/version-2
dataLogSize=67110187
tickTime=2000
maxClientCnxns=60
minSessionTimeout=4000
maxSessionTimeout=40000
clientPortListenBacklog=-1
serverId=0

[root@ubuntu2404 ~]#echo conf | nc 127.1 2181
clientPort=2181
secureClientPort=-1
dataDir=/usr/local/zookeeper/logs/version-2
dataDirSize=67110187
dataLogDir=/usr/local/zookeeper/logs/version-2
dataLogSize=67110187
tickTime=2000
maxClientCnxns=60
minSessionTimeout=4000
maxSessionTimeout=40000
clientPortListenBacklog=-1
serverId=0
```





## kafka

在2006 年的时候，AMQP (Advanced Message Queuing Protocol)规范发布了。它是跨语言和跨平台的，真正地促进了消息队列的繁荣发展。

消息队列主要有以下应用场景

- 削峰填谷

  诸如电商业务中的秒杀、抢红包、企业开门红等大型活动时皆会带来较高的流量脉冲，或因没做相应的保护而导致系统超负荷甚至崩溃，或因限制太过导致请求大量失败而影响用户体验，消息队列可提供削峰填谷的服务来解决该问题。

- 异步解耦

  交易系统作为淘宝等电商的最核心的系统，每笔交易订单数据的产生会引起几百个下游业务系统的关注，包括物流、购物车、积分、流计算分析等等，整体业务系统庞大而且复杂，消息队列可实现异步通信和应用解耦，确保主站业务的连续性。

- 顺序收发

  细数日常中需要保证顺序的应用场景非常多，例如证券交易过程时间优先原则，交易系统中的订单创建、支付、退款等流程，航班中的旅客登机消息处理等等。与先进先出FIFO（First In First Out）原理类似，消息队列提供的顺序消息即保证消息FIFO。

- 分布式事务一致性

  交易系统、支付红包等场景需要确保数据的最终一致性，大量引入消息队列的分布式事务，既可以实现系统之间的解耦，又可以保证最终的数据一致性。

- 大数据分析

  数据在“流动”中产生价值，传统数据分析大多是基于批量计算模型，而无法做到实时的数据分析，利用消息队列与流式计算引擎相结合，可以很方便的实现业务数据的实时分析。

- 分布式缓存同步

  电商的大促，各个分会场琳琅满目的商品需要实时感知价格变化，大量并发访问数据库导致会场页面响应时间长，集中式缓存因带宽瓶颈，限制了商品变更的访问流量，通过消息队列构建分布式缓存，实时通知商品数据的变化

- 蓄流压测

  线上有些链路不方便做压力测试，可以通过堆积一定量消息再放开来压测

**Kafka** **特点和优势**

特点

- 分布式: 支持分布式多主机部署实现
- 分区: 一个消息.可以拆分出多个，分别存储在多个位置
- 多副本: 防止信息丢失，可以多来几个备份
- 多订阅者: 可以有很多应用连接kafka
- Zookeeper: 早期版本的Kafka依赖于zookeeper， 2021年4月19日Kafka 2.8.0正式发布，此版本包括了很多重要改动，最主要的是kafka通过自我管理的仲裁来替代ZooKeeper，即Kafka将不再需要ZooKeeper！

优势

- Kafka 通过 O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以 TB 级别以上的消息存储也能够保持长时间的稳定性能。
- 高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。支持通过Kafka 服务器分区消息。
- 分布式： Kafka 基于分布式集群实现高可用的容错机制，可以实现自动的故障转移
- 顺序保证：在大多数使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。 Kafka保证一个Partiton内的消息的有序性（分区间数据是无序的，如果对数据的顺序有要求，应将在创建主题时将分区数partitions设置为1）
- 支持 Hadoop 并行数据加载
- 通常用于大数据场合,传递单条消息比较大，而Rabbitmq 消息主要是传输业务的指令数据,单条数据较小

### kafkaVSRabbitMQ

**核心架构对比** 

| 特性             | Kafka                                        | RabbitMQ                                    |
| ---------------- | -------------------------------------------- | ------------------------------------------- |
| **架构**         | 分布式日志系统，基于发布-订阅（Pub/Sub）模式 | 基于 AMQP（高级消息队列协议）的传统消息代理 |
| **消息存储**     | 持久化日志（文件存储），消息会被保留一定时间 | 队列模式，消息消费后默认被删除              |
| **数据处理方式** | 批量处理（吞吐量高）                         | 即时消息处理（低延迟）                      |
| **分布式支持**   | 天然支持分布式（多 Broker 架构）             | 需要集群模式支持分布式                      |

------

**2. 主要功能对比**

| 功能             | Kafka                                        | RabbitMQ                                |
| ---------------- | -------------------------------------------- | --------------------------------------- |
| **消息模型**     | 主要基于 **日志存储**（消费者读取偏移量）    | 典型的 **消息队列**（消费者确认后删除） |
| **数据持久性**   | 支持，数据按**时间段**存储，可回溯           | 支持，**确认后删除**                    |
| **吞吐量**       | 高吞吐量（百万级消息/秒）                    | 吞吐量适中（万级消息/秒）               |
| **延迟**         | 毫秒级别（适合大规模日志处理）               | 低于 1ms（适合低延迟应用）              |
| **消息顺序**     | **保证分区内有序**（多个分区时无法全局有序） | **默认无序**，但可以使用 FIFO 队列      |
| **消息确认机制** | 由消费者维护偏移量（offset），手动管理       | ACK 机制，支持自动或手动确认            |
| **消费模式**     | 发布-订阅（多个消费者可读取同一条消息）      | 点对点和发布-订阅模式                   |
| **协议**         | 自定义 Kafka 协议                            | AMQP（支持 STOMP、MQTT）                |

------

**3. 使用场景对比**

| 场景                         | Kafka                                       | RabbitMQ                           |
| ---------------------------- | ------------------------------------------- | ---------------------------------- |
| **日志收集**                 | ✅（ELK、日志分析）                          | ❌（RabbitMQ 不是为日志存储设计的） |
| **流式数据处理**             | ✅（配合 Flink、Spark 等大数据框架）         | ❌                                  |
| **微服务通信**               | ❌（不适合 RPC）                             | ✅（常用于微服务间解耦）            |
| **高吞吐消息队列**           | ✅（亿级 TPS 的应用场景，如日志、埋点）      | ❌                                  |
| **任务队列（RPC 任务调度）** | ❌（Kafka 适合批量处理，不适合短时任务队列） | ✅（延迟低，适合 RPC 调用）         |
| **金融交易/支付**            | ❌（Kafka 无法保证消息强一致性）             | ✅（消息确认机制更安全）            |

------

**4. 适用场景总结**

- **Kafka 适合：**
  - **大数据场景**（日志收集、行为分析）
  - **高吞吐量应用**（流式数据处理）
  - **事件驱动架构**（Event Sourcing）
- **RabbitMQ 适合：**
  - **微服务间通信**
  - **即时消息队列**（订单处理、RPC 任务队列）
  - **需要复杂消息路由的应用**（如基于 AMQP 交换机的多路分发）

------

**5. 选择建议**

| 需求                     | 选择       |
| ------------------------ | ---------- |
| **高吞吐，批量处理日志** | ✅ Kafka    |
| **流式处理，事件驱动**   | ✅ Kafka    |
| **低延迟，可靠交付**     | ✅ RabbitMQ |
| **微服务通信，RPC**      | ✅ RabbitMQ |

如果你在 **大数据** 或 **日志处理** 相关项目上，Kafka 更合适。
 如果你在 **企业系统、微服务通信** 领域，RabbitMQ 更好。

### **Kafka** **角色和流程**

![image-20250309173232345](C:/Users/zhaok/AppData/Roaming/Typora/typora-user-images/image-20250309173232345.png)

![image-20250309173317081](C:/Users/zhaok/AppData/Roaming/Typora/typora-user-images/image-20250309173317081.png)



**Producer**：Producer即生产者，消息的产生者，是消息的入口。负责发布消息到Kafka broker。

**Consumer**：消费者，用于消费消息，即处理消息

**Broker**：Broker是kafka实例，每个服务器上可以有一个或多个kafka的实例，假设每个broker对应一台服务器。每个kafka集群内的broker都有一个**不重复**的编号，如: broker-0、broker-1等……

**Controller：**是整个 Kafka 集群的管理者角色，任何集群范围内的状态变更都需要通过 Controller 进行，在整个集群中是个单点的服务，可以通过选举协议进行故障转移，负责集群范围内的一些关键操作：主题的新建和删除，主题分区的新建、重新分配，Broker 的加入、退出，触发分区 Leader 选举等，每个 Broker 里都有一个 Controller 实例，多个 Broker 的集群同时最多只有一个 Controller 可以对外提供集群管理服务，Controller 可以在 Broker 之间进行故障转移，Kafka 集群管理的工作主要是由 Controller 来完成的，而 Controller 又通过监听Zookeeper 节点的变动来进行监听集群变化事件，Controller 进行集群管理需要保存集群元数据，监听集群状态变化情况并进行处理，以及处理集群中修改集群元数据的请求，这些主要都是利用 Zookeeper 来实现

**Topic** ：消息的主题，可以理解为消息的分类，一个Topic相当于数据库中的一张表,一条消息相当于关系数据库的一条记录，或者一个Topic相当于Redis中列表数据类型的一个Key，一条消息即为列表中的一个元素。kafka的数据就保存在topic。在每个broker上都可以创建多个topic。

虽然一个 topic的消息保存于一个或多个broker 上同一个目录内, 物理上不同 topic 的消息分开存储在不同的文件夹，但用户只需指定消息的topic即可生产或消费数据而不必关心数据存于何处，topic 在逻辑上对record(记录、日志)进行分组保存，消费者需要订阅相应的topic 才能消费topic中的消息。

**Consumer group:** 每个consumer 属于一个特定的consumer group（可为每个consumer 指定 group name，若不指定 group name 则属于默认的group），同一topic的一条消息只能被同一个consumer group 内的一个consumer 消费，类似于一对一的单播机制，但多个consumer group 可同时消费这一消息，类似于一对多的多播机制，默认消费组的多个消费者是共享消息

<img src="5day-png/35kafka主题.png" alt="image-20250309175127705" style="zoom:50%;" />



### 生产者消费者和发布者订阅者

#### 1.**生产者-消费者模型（Producer-Consumer）**

生产者-消费者模式是一种**点对点（P2P）** 的消息队列模型，通常用于**任务分发和负载均衡**。

##### **特点**

- **消息是点对点的**：一个生产者（Producer）发送消息到队列，**一个消费者（Consumer）接收并处理**。
- **消息消费后即被删除**，不会被多个消费者重复消费。
- **典型应用**：任务队列、RPC 调用等。

##### **架构示意**

```
生产者 (Producer)  ->  消息队列 (Queue)  ->  消费者 (Consumer)
```

**示例（RabbitMQ）**

```
Producer ---> [ Queue ] ---> Consumer1
                             Consumer2 (负载均衡)
```

如果有多个消费者，消息会在它们之间**均匀分配**（负载均衡），但**同一条消息只会被一个消费者消费**。

##### **适用场景**

- **异步任务处理**（订单支付、日志存储）
- **负载均衡**（多消费者共享任务）
- **RPC 任务队列**（微服务之间的任务分发）

------

#### **2. 发布-订阅模式（Publish-Subscribe，Pub/Sub）**

发布-订阅模式是一种**广播式**的消息模型，通常用于**事件通知和日志分发**。

##### **特点**

- **多个订阅者可以同时收到相同的消息**（广播）。
- **订阅者可以动态加入或退出**，不会影响发布者的工作。
- **典型应用**：日志监控、事件驱动架构、消息推送。

##### **架构示意**

```
发布者 (Publisher)  ->  主题 (Topic)  ->  订阅者1 (Subscriber)
                                      ->  订阅者2 (Subscriber)
                                      ->  订阅者3 (Subscriber)
```

**示例（Kafka、Redis Pub/Sub）**

```
Publisher ---> [ Topic ] ---> Subscriber1
                               Subscriber2
                               Subscriber3
```

同一条消息可以被多个订阅者消费，不会影响其他订阅者的消费情况。

##### **适用场景**

- **日志收集**（ELK、Kafka 日志中心）
- **事件驱动架构**（微服务事件总线）
- **实时推送**（股票行情、聊天系统）

------

#### **3. 生产者-消费者 vs 发布-订阅 对比**

| **对比项**     | **生产者-消费者（P2P）**         | **发布-订阅（Pub/Sub）**     |
| -------------- | -------------------------------- | ---------------------------- |
| **消息流转**   | **点对点**，一个消费者消费       | **广播**，多个订阅者消费     |
| **消息存储**   | **消费后删除**                   | **可以存储（如 Kafka）**     |
| **消费者角色** | 负载均衡，每条消息只给一个消费者 | 每个订阅者都能收到相同的消息 |
| **典型应用**   | 任务队列、RPC 调度               | 事件驱动、日志收集、推送服务 |
| **消息丢失**   | 消费后即删除，需手动存储         | 通常有持久化机制             |

------

#### **4. Kafka vs RabbitMQ**

| 特性         | **Kafka（Pub/Sub）**               | **RabbitMQ（P2P）**                |
| ------------ | ---------------------------------- | ---------------------------------- |
| **模型**     | 发布-订阅                          | 生产者-消费者                      |
| **消息存储** | 持久化存储（可回溯）               | 消费后删除                         |
| **消费模式** | 订阅模式，多个消费者可消费同一消息 | 负载均衡，消息只能被一个消费者消费 |
| **适用场景** | 日志、流式计算、大数据             | 任务调度、RPC                      |

------

#### **5. 如何选择？**

| **需求**                         | **建议模型**              |
| -------------------------------- | ------------------------- |
| **任务队列**（订单、支付、邮件） | 生产者-消费者（RabbitMQ） |
| **日志收集、数据流处理**         | 发布-订阅（Kafka）        |
| **股票行情、新闻推送**           | 发布-订阅（Kafka/Redis）  |
| **微服务任务分发**               | 生产者-消费者（RabbitMQ） |
| **事件驱动架构**（Event-Driven） | 发布-订阅（Kafka）        |

#### **总结：**

- **RabbitMQ（生产者-消费者）** 适用于任务调度、RPC、微服务通信。
- **Kafka（发布-订阅）** 适用于日志处理、事件驱动、大数据流处理。

### 消费组

Kafka 的 **消费组（Consumer Group）** 是 Kafka 消费者架构的核心概念，它用于实现**并行消费**和**负载均衡**。以下是 Kafka 消费组的图解解析：

------

#### **1. 消费组基本概念**

- **一个主题（Topic）可以被多个消费组订阅**，各个消费组之间**相互独立**。
- **同一消费组内的多个消费者（Consumer）会共同消费一个 Topic**，但**一个分区（Partition）只能被组内的一个消费者消费**。
- **消费组通过 "组协调器"（Group Coordinator）来管理**消费者的负载均衡。

------

#### **2. 消费组示意图**

##### **（1）单个消费者的情况**

当只有一个消费者（Consumer）时，它会消费整个 Topic 的所有分区：

```
Topic: my_topic (3个分区)
-----------------------------------
| P0 | P1 | P2 |   (分区 Partition)
-----------------------------------
        ↓
Consumer A  （消费所有分区）
```

- **缺点**：单个消费者的处理能力有限，无法充分利用 Kafka 的并行能力。

------

##### **（2）多个消费者组成消费组**

当多个消费者组成一个消费组时，每个分区会被**组内的不同消费者消费**：

```
Consumer Group: group_A
-----------------------------------
| P0 | P1 | P2 |   (分区 Partition)
-----------------------------------
  ↓     ↓     ↓
 C1     C2    C3   （3个消费者）
```

- **优势**：多个消费者分担负载，提升吞吐量。
- **规则**：一个分区只能被消费组内的一个消费者消费，但同一 Topic 可以被不同消费组消费。

------

##### **（3）消费者数量少于分区数**

当消费者（Consumer）数量 **少于** 分区（Partition）数量时，部分消费者需要消费多个分区：

```
Consumer Group: group_A
-----------------------------------
| P0 | P1 | P2 | P3 | P4 |  (分区)
-----------------------------------
  ↓     ↓     ↓     ↓    ↓
 C1     C2    C1    C2   C3
```

- **C1 负责 P0 和 P2**
- **C2 负责 P1 和 P3**
- **C3 负责 P4**

**特点**：

- 消费者 **数量小于等于分区数** 时，Kafka 会自动均衡分配分区。
- **消费者越多，负载越均衡**，但不应超过分区数。

------

##### **（4）消费者数量超过分区数**

当**消费者数量多于分区数**，多余的消费者会**闲置**：

```
Consumer Group: group_A
-----------------------------------
| P0 | P1 | P2 |  (分区)
-----------------------------------
  ↓     ↓     ↓   
 C1     C2    C3   
 C4 (空闲) C5 (空闲)
```

- **C1 负责 P0**
- **C2 负责 P1**
- **C3 负责 P2**
- **C4、C5 没有分区可消费（闲置）**

**特点**：

- **消费者数量超过分区数时，Kafka 不会进行分配**，多出的消费者会处于空闲状态。
- **优化方案**：增加 Kafka 主题的分区数，使消费者能够均衡消费。

------

#### **3. 消费组的特性**

- **负载均衡**：消费者组中的消费者数量可以动态调整，Kafka 会自动重新分配分区。
- **高可用性**：如果某个消费者崩溃，Kafka 会将它的分区重新分配给其他消费者。
- **多消费组支持**：多个消费组可以同时消费同一主题，互不影响。

------

#### **4. Kafka 消费组应用场景**

- **日志收集**（ELK）：一个消费组用于存储到 HDFS，另一个用于实时分析。
- **订单系统**：一个消费组处理订单，另一个消费组发送消息通知。
- **大数据流式处理**：Kafka + Flink/Spark 进行流计算。

------

#### **总结**

| **情况**          | **表现**                         |
| ----------------- | -------------------------------- |
| 消费者数 < 分区数 | 消费者承担多个分区的消费任务     |
| 消费者数 = 分区数 | 最优负载均衡，每个分区一个消费者 |
| 消费者数 > 分区数 | 多余的消费者闲置，未分配分区     |

通过**调整分区数**与**消费者数量**的比例，可以优化 Kafka 的消费性能。

### 副本

![image-20250309174810490](5day-png/35kafka副本.png)

**Partition** ：分区，是物理上的概念，每个 topic 分割为一个或多个partition，即一个topic切分为多份, 当创建 topic 时可指定 partition 数量，partition的表现形式就是一个一个的文件夹,该文件夹下存储该partition的数据和索引文件，分区的作用还可以实现负载均衡，提高kafka的吞吐量。同一个topic在不同的分区的数据是不重复的,一般Partition数不要超过节点数，注意同一个partition数据是有顺序的，但不同的partition则是无序的。

**Replication**: 副本，同样数据的副本，包括leader和follower的副本数,基本于数据安全,建议至少2个,是Kafka的高可靠性的保障，和ES的副本有所不同，**Kafka中的副(leader+follower）数包括主分片数,而Elasticsearch中的副本数(follower)**不包括主分片数**为了实现数据的高可用，比如将分区 0 的数据分散到不同的kafka 节点，每一个分区都有一个 broker 作为 Leader 和一个 broker 作为Follower，类似于ES中的主分片和副本分片，

假设分区为 3, 即分三个分区0-2，副本为3，即每个分区都有一个 leader，再加两个follower，分区 0 的leader为服务器A，则服务器 B 和服务器 C 为 A 的follower，而分区 1 的leader为服务器B，则服务器 A 和C 为服务器B 的follower，而分区 2 的leader 为C，则服务器A 和 B 为C 的follower。

**AR**： Assigned Replicas，分区中的所有副本的统称，包括leader和 follower，AR= lSR+ OSR

**lSR**：ln Sync Replicas，所有与leader副本保持同步的副本 follower和leader本身组成的集合，包括leader和 follower，是AR的子集

**OSR**：out-of-Sync Replied，所有与leader副本同步不能同步的 follower的集合，是AR的子集

**分区和副本的优势：**

- 实现存储空间的横向扩容，即将多个kafka服务器的空间组合利用
- 提升性能，多服务器并行读写
- 实区即 leader 分布在不同的kafka 服务器，并且有对应follower 分布在和leader不同的服务器上

![image-20250309175621267](5day-png/35kafka分区和副本.png)

在 Kafka 中，**分区（Partition）** 和 **副本（Replica）** 是核心概念，影响了 Kafka 的**高并发、负载均衡和高可用性**。下面详细解析它们的作用、架构和工作机制。

------

### 分区(Partition)和副本(Replica)

#### **1. Kafka 分区（Partition）**

Kafka 的 **主题（Topic）** 由多个 **分区（Partition）** 组成，每个分区可以独立存储和消费数据。
 **分区的作用：**

- **并行处理**：不同分区可以由不同的消费者消费，提高吞吐量。
- **分布式存储**：一个主题的数据被分布到多个分区，避免单点瓶颈。
- **顺序保证**：Kafka **保证同一分区内消息是有序的**（跨分区无全局顺序）。

##### **分区架构示意**

```
sql复制编辑Topic: my_topic
------------------------------------------------
| Partition 0 | Partition 1 | Partition 2 |
------------------------------------------------
|  msg1, msg2 |  msg3, msg4 |  msg5, msg6 |
```

多个消费者（Consumer）可以并行消费不同的分区：

```
sql复制编辑Consumer Group A:
  - Consumer 1 负责 Partition 0
  - Consumer 2 负责 Partition 1
  - Consumer 3 负责 Partition 2
```

##### **分区分配规则**

Kafka **根据 Key 进行 Hash 分区**，分区方式：

- **随机分区**（默认）
- **基于 Key 进行 Hash**（保证相同 Key 的消息进入同一分区）
- **手动指定分区**

##### **分区的优势**

✅ **提高吞吐量**（不同分区可由不同 Broker 处理）
 ✅ **支持水平扩展**（增加分区可提升并发）
 ✅ **保证局部有序性**（同一 Key 的消息顺序不变）

------

#### **2. Kafka 副本（Replica）**

副本是 Kafka **用于容错和高可用**的机制，每个分区都有多个副本。
 **副本的作用：**

- **防止数据丢失**（主副本宕机后从副本接管）
- **高可用性**（Leader 失效后 Follower 可选举为 Leader）

##### **副本架构**

```
pgsql复制编辑Topic: my_topic (3个分区，每个有3个副本)
------------------------------------------------
| Partition 0  (Leader)  | Replica 1 | Replica 2 |
| Partition 1  (Replica) | Leader    | Replica 3 |
| Partition 2  (Replica) | Replica 4 | Leader    |
------------------------------------------------
```

- **Leader 副本**：处理生产者和消费者的请求，所有写操作都先到 Leader。
- **Follower 副本**：同步 Leader 数据，Leader 宕机时可以选举新的 Leader。

##### **Kafka 副本同步机制**

Kafka 采用 **ISR（In-Sync Replicas）** 机制：

1. 生产者（Producer）只写入 **Leader 副本**。
2. 其他 **Follower 副本** 从 Leader 拉取数据，保持同步。
3. 当 Leader 宕机时，一个 ISR 中的 Follower 副本会被选为新的 Leader。

##### **副本机制的优势**

✅ **防止数据丢失**（至少一个副本存活时数据仍然可用）
 ✅ **Leader 选举保证高可用**（Leader 挂了自动选新的 Leader）
 ✅ **可调节副本数量**（提高可靠性，但会增加存储开销）

------

#### **3. Kafka 分区与副本关系总结**

| **特性**          | **分区（Partition）**     | **副本（Replica）**               |
| ----------------- | ------------------------- | --------------------------------- |
| **作用**          | 提高吞吐量、并行消费      | 容错、数据冗余、高可用            |
| **数量**          | 每个 Topic 由多个分区组成 | 每个分区有多个副本                |
| **存储位置**      | 分布在多个 Broker         | 同一个分区的副本分布在不同 Broker |
| **消息存储**      | 只存储该分区的部分数据    | 复制 Leader 数据，防止丢失        |
| **Leader 作用**   | 处理读写请求              | 负责同步数据给 Follower           |
| **Follower 作用** | 由不同消费者消费不同分区  | 仅复制数据，不处理读写            |

------

#### **4. Kafka 分区 & 副本 应用场景**

| **需求**                         | **解决方案**                |
| -------------------------------- | --------------------------- |
| **高吞吐、大数据处理**           | 增加 **分区数**             |
| **数据可靠性要求高**             | 增加 **副本数**（默认 3）   |
| **防止 Broker 挂掉导致数据丢失** | ISR 副本同步                |
| **保证特定 Key 的顺序性**        | 采用 **Key 进行 Hash 分区** |

##### **Kafka 分区 & 副本 经典配置**

| 需求       | **分区数** | **副本数** |
| ---------- | ---------- | ---------- |
| 日志收集   | 10+        | 2-3        |
| 订单交易   | 5-10       | 3          |
| 实时流处理 | 20+        | 3-5        |

------

#### **5. Kafka 分区 & 副本 设计最佳实践**

1. **分区数 ≈ 消费者数**（尽量均衡，提高并发）

2. **副本数 ≥ 3**（防止数据丢失）

3. **不同分区的副本分布在不同 Broker 上**（避免单点故障）

4. Kafka 配置参数优化

   ```
   properties复制编辑# 设置副本数
   default.replication.factor=3
   
   # 设置最小 ISR 副本数（防止数据丢失）
   min.insync.replicas=2
   ```

------

#### **总结**

- **分区（Partition）** 提供**并行消费、负载均衡**，提高吞吐量。
- **副本（Replica）** 提供**高可用、数据备份**，防止数据丢失。
- **最佳实践** 是合理设置分区和副本数量，以平衡**性能和可靠性**。

对于 **高吞吐大数据场景（日志、流计算）**，Kafka 通过 **分区并行化** 处理数据，而 **副本机制** 确保数据安全和高可用。



### **Kafka** **工作机制**

Kafka的领导者选举过程发生在以下情况中：

1. 当新的分区创建时，Kafka选择ISR（In-Sync Replica）列表中的第一个副本作为领导者。
2. 当领导者失败或无法与ZooKeeper通信时，会重新选举新的领导者。这个过程被称为领导者故障转移（leader failover）。

Kafka领导者选举的详细步骤：

1. 当领导者发生故障，ZooKeeper将会检测到它的会话过期。
2. ZooKeeper接着将通知所有的副本进行领导者选举。
3. 副本们会查看它们在ZooKeeper中存储的元数据并确定新的领导者，选择规则是选取副本集合（ISR）中最新的副本。
4. 一旦新的领导者被选出，ZooKeeper将通知所有的副本更新它们的元数据。

![image-20250309180556092](5day-png/35kafka工作机制.png)

**Kafka的复制配置**

Kafka的复制方式可以通过acks（acknowledgments）配置来实现：

- **acks=0**：生产者发送消息后不等确认即认为成功。这种方式相当于没有复制，只有单个副本存储在Leader上。
- **acks=1**：消息发送到Leader即认为成功，相当于异步复制。在这种情况下，生产者只需要等待Leader的确认，而不需要等待ISR（In-Sync Replicas，同步副本集）中其他副本的确认。
- **acks=all**：需要等待ISR列表中所有



### kafka部署

```bash
#下载链接
http://kafka.apache.org/downloads

kafka_<scala 版本>_<kafka 版本>
#示例:kafka_2.13-2.7.0.tgz
```



#### 单机部署

```bash
#官方文档
http://kafka.apache.org/quickstart
```

##### 基于zookeeper部署

```bash
#安装java
[root@ubuntu2404 ~]#apt update && apt install -y openjdk-21-jdk
[root@ubuntu2404 ~]#tar xf kafka_2.13-3.9.0.tgz -C /usr/local/
[root@ubuntu2404 ~]#cd /usr/local/
[root@ubuntu2404 local]#ls
bin  etc  games  include  kafka_2.13-3.9.0  lib  man  sbin  share  src
[root@ubuntu2404 local]#ln -s kafka_2.13-3.9.0/ kafka
[root@ubuntu2404 local]#cd kafka
[root@ubuntu2404 kafka]#ls 
bin  config  libs  LICENSE  licenses  NOTICE  site-docs

#修改zookeeper配置文件
[root@ubuntu2404 kafka]#vim config/zookeeper.properties 
dataDir=/usr/local/kafka/data/zookeeper

#启动zookeeper
[root@ubuntu2404 kafka]#bin/zookeeper-server-start.sh config/zookeeper.properties 
[root@ubuntu2404 ~]#ss -ntl
LISTEN                0                     50                                               *:2181                                            *:* 
[root@ubuntu2404 ~]#ls /usr/local/kafka/data/
zookeeper

#修改kafka配置文件
[root@ubuntu2404 kafka]#vim config/server.properties 
log.dirs=/usr/local/kafka/data/kafka-logs

#启动kafka
[root@ubuntu2404 kafka]#bin/kafka-server-start.sh config/server.properties 
[root@ubuntu2404 ~]#ss -tnl                                       
LISTEN                0                     50                                               *:37909                                           *:*                                          
LISTEN                0                     50                                               *:2181                                            *:*                                          
LISTEN                0                     50                                               *:35457                                           *:*                                          
LISTEN                0                     50                                               *:9092                                            *:*  
```

```bash
#生产中这样后台启动
[root@ubuntu2404 kafka]#nohup bin/kafka-server-start.sh config/server.properties  &> /dev/null &
[root@ubuntu2404 kafka]#nohup bin/kafka-server-start.sh config/server.properties  &> /dev/null &
```



##### 基于KRaft 部署

**Kafka-v4.0开始即将不再支持Zookeeper**

```
#https://kafka.apache.org/quickstart
```

```bash
#安装java
[root@ubuntu2404 ~]#apt update && apt install -y openjdk-21-jdk
#生成集群唯一ID,Generate a Cluster UUID,集群内的多个节点需使用同一集群ID
[root@ubuntu2404 kafka]#KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
[root@ubuntu2404 kafka]#echo $KAFKA_CLUSTER_ID
OZ_eAd7JSpqwfwdhq2zxBQ
#配置文件
[root@ubuntu2404 kafka]#vim config/kraft/server.properties
log.dirs=/data/kafka
advertised.listeners=PLAINTEXT://10.0.0.200:9092,CONTROLLER://10.0.0.200:9093
#数据初始化
[root@ubuntu2404 kafka]#bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties
Formatting metadata directory /data/kraft-combined-logs with metadata.version 3.9-IV0.
[root@ubuntu2404 kafka]#ls /data/kraft-combined-logs/
bootstrap.checkpoint  meta.properties
#启动kafka
[root@ubuntu2404 kafka]#bin/kafka-server-start.sh config/kraft/server.properties
```

```
[root@ubuntu2404 kafka]#ss -tnulp | grep java
tcp   LISTEN 0      50                 *:9093             *:*    users:(("java",pid=16812,fd=146))        
tcp   LISTEN 0      50                 *:9092             *:*    users:(("java",pid=16812,fd=174))        
tcp   LISTEN 0      50                 *:34443            *:*    users:(("java",pid=16812,fd=127))       
```

```
[root@ubuntu2404 kafka]#/usr/local/kafka/bin/kafka-topics.sh --create --topic kang --bootstrap-server 10.0.0.200:9092
Created topic kang.
[root@ubuntu2404 kafka]#/usr/local/kafka/bin/kafka-topics.sh --list --bootstrap-server 10.0.0.200:9092
kang
```

##### 脚本部署

```bash
#!/bin/bash
# 支持在线和离线安装
KAFKA_VERSION=3.6.0
#KAFKA_VERSION=3.5.1
#KAFKA_VERSION=3.5.0
#KAFKA_VERSION=3.4.0
#KAFKA_VERSION=3.3.2
#KAFKA_VERSION=3.2.0
#KAFKA_VERSION=-3.0.0
SCALA_VERSION=2.13
KAFKA_URL="https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/${KAFKA_VERSION}/kafka_${SCALA_VERSION}-${KAFKA_VERSION}.tgz"
#KAFKA_URL="https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.8.1/kafka_2.13-2.8.1.tgz"
#KAFKA_URL="https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.7.1/kafka_2.13-2.7.1.tgz"
KAFKA_INSTALL_DIR=/usr/local/kafka
HOST=`hostname -I|awk '{print $1}'`
. /etc/os-release

color () {
    RES_COL=60
    MOVE_TO_COL="echo -en \\033[${RES_COL}G"
    SETCOLOR_SUCCESS="echo -en \\033[1;32m"
    SETCOLOR_FAILURE="echo -en \\033[1;31m"
    SETCOLOR_WARNING="echo -en \\033[1;33m"
    SETCOLOR_NORMAL="echo -en \E[0m"
    echo -n "$1" && $MOVE_TO_COL
    echo -n "["
    if [ $2 = "success" -o $2 = "0" ] ;then
        ${SETCOLOR_SUCCESS}
        echo -n $" OK "    
    elif [ $2 = "failure" -o $2 = "1" ] ;then 
        ${SETCOLOR_FAILURE}
        echo -n $"FAILED"
    else
        ${SETCOLOR_WARNING}
        echo -n $"WARNING"
    fi
    ${SETCOLOR_NORMAL}
    echo -n "]"
    echo
}

env () {
    echo $HOST `hostname` >> /etc/hosts
}

install_jdk() {
    java -version &>/dev/null && { color "JDK 已安装!" 1 ; return; }
    if command -v yum &>/dev/null ; then
        yum -y install java-1.8.0-openjdk-devel || { color "安装JDK失败!" 1; exit 1; }
    elif command -v apt &>/dev/null ; then
        apt update
        #apt install openjdk-11-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        apt install openjdk-8-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
    else
        color "不支持当前操作系统!" 1
        exit 1
    fi
    java -version && { color "安装 JDK 完成!" 0 ; } || { color "安装JDK失败!" 1; exit 1; } 
}

install_zookeeper() {
    cat > ${KAFKA_INSTALL_DIR}/bin/zookeeper-startup.sh <<EOF
#!/bin/bash
nohup ${KAFKA_INSTALL_DIR}/bin/zookeeper-server-start.sh ${KAFKA_INSTALL_DIR}/config/zookeeper.properties &
EOF
    chmod +x ${KAFKA_INSTALL_DIR}/bin/zookeeper-startup.sh
    cat > /lib/systemd/system/zookeeper.service <<EOF
[Unit]
Description=zookeeper.service
After=network.target
[Service]
Type=forking
ExecStart=${KAFKA_INSTALL_DIR}/bin/zookeeper-startup.sh
ExecStop=${KAFKA_INSTALL_DIR}/bin/zookeeper-server-stop.sh 
[Install]
WantedBy=multi-user.target
EOF
    systemctl daemon-reload
    systemctl enable --now zookeeper.service
    systemctl is-active zookeeper.service
    if [ $? -eq 0 ] ;then 
        color "zookeeper 安装成功!" 0  
    else
        color "zookeeper 安装失败!" 1
        exit 1
    fi  
}

install_kafka(){
    if [ ! -f kafka_${SCALA_VERSION}-${KAFKA_VERSION}.tgz ];then
        wget -P /usr/local/src/  --no-check-certificate $KAFKA_URL || { color  "下载失败!" 1 ;exit ; }
    fi
    tar xf /usr/local/src/${KAFKA_URL##*/}  -C /usr/local/
    ln -s /usr/local/kafka_${SCALA_VERSION}-${KAFKA_VERSION}  ${KAFKA_INSTALL_DIR}
    install_zookeeper
    echo PATH=${KAFKA_INSTALL_DIR}/bin:'$PATH' >> /etc/profile
    cat > ${KAFKA_INSTALL_DIR}/bin/kafka-startup.sh <<EOF
#!/bin/bash
nohup ${KAFKA_INSTALL_DIR}/bin/kafka-server-start.sh  ${KAFKA_INSTALL_DIR}/config/server.properties &
EOF
    chmod +x ${KAFKA_INSTALL_DIR}/bin/kafka-startup.sh
    cat > /lib/systemd/system/kafka.service <<EOF
[Unit]                                                                           
Description=Apache kafka
After=network.target
[Service]
Type=forking
ExecStart=${KAFKA_INSTALL_DIR}/bin/kafka-startup.sh
ExecStop=/bin/kill  -TERM \${MAINPID}
Restart=always
RestartSec=20
[Install]
WantedBy=multi-user.target
EOF
    systemctl daemon-reload
    systemctl enable --now kafka.service
    #kafka-server-start.sh -daemon ${KAFKA_INSTALL_DIR}/config/server.properties 
    systemctl is-active kafka.service
    if [ $? -eq 0 ] ;then 
        color "kafka 安装成功!" 0  
    else
        color "kafka 安装失败!" 1
        exit 1
    fi    
}

env 
install_jdk
install_kafka
```

#### 集群部署

##### 基于zookeeper的集群

```bash
#修改每个kafka节点的主机名
[root@ubuntu2404 ~]#hostnamectl hostname node1
[root@ubuntu2404 ~]#hostnamectl hostname node2
[root@ubuntu2404 ~]#hostnamectl hostname node3

#在所有kafka节点上实现主机名称解析
[root@node3 ~]#vim /etc/hosts
10.0.0.201 node1
10.0.0.202 node2
10.0.0.203 node3
```

```bash
#安装 JAVA
apt update && apt install -y openjdk-21-jdk

#下载kafka二进制文件，解压缩
https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/3.9.0/kafka_2.13-3.9.0.tgz
[root@node1 ~]#tar xf kafka_2.13-3.9.0.tgz  -C /usr/local/
[root@node1 ~]#cd /usr/local/
[root@node1 local]#ln -s kafka_2.13-3.9.0/ kafka
[root@node1 local]#cd kafka
[root@node1 kafka]#ls
bin  config  libs  LICENSE  licenses  NOTICE  site-docs

#修改zookeeper配置文件
[root@node1 kafka]#vim config/zookeeper.properties 
#必须添加时间相关配置
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5
dataDir=/data/zookeeper
server.1=10.0.0.201:2888:3888
server.2=10.0.0.202:2888:3888
server.3=10.0.0.203:2888:3888

[root@node1 kafka]#mkdir /usr/local/kafka/data/zookeeper/ -p
[root@node1 kafka]#echo 1 > /data/zookeeper/myid

[root@node1 kafka]#rsync -av /usr/local/kafka* 10.0.0.202:/usr/local
[root@node1 kafka]#rsync -av /usr/local/kafka* 10.0.0.203:/usr/local

#修改各个节点的MyID
[root@node2 ~]#echo 2 > /usr/local/kafka/data/zookeeper/myid
[root@node3 ~]#echo 3 > /usr/local/kafka/data/zookeeper/myid

#在所有节点上启动
/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties
```

**配置文件说明**

```bash
#配置文件 ./conf/server.properties内容说明
############################# Server Basics###############################
# broker的id，值为整数，且必须唯一，在一个集群中不能重复，此行必须修改
broker.id=1
############################# Socket ServerSettings ######################
# kafka监听端口，默认9092
listeners=PLAINTEXT://10.0.0.201:9092
# 处理网络请求的线程数量，默认为3个
num.network.threads=3
# 执行磁盘IO操作的线程数量，默认为8个
num.io.threads=8
# socket服务发送数据的缓冲区大小，默认100KB
socket.send.buffer.bytes=102400
# socket服务接受数据的缓冲区大小，默认100KB
socket.receive.buffer.bytes=102400
# socket服务所能接受的一个请求的最大大小，默认为100M
socket.request.max.bytes=104857600
############################# Log Basics###################################
# kafka存储消息数据的目录
log.dirs=../data
# 每个topic默认的partition
num.partitions=1
# 设置副本数量为3,当Leader的Replication故障，会进行故障自动转移。
default.replication.factor=3
# 在启动时恢复数据和关闭时刷新数据时每个数据目录的线程数量
num.recovery.threads.per.data.dir=1
############################# Log FlushPolicy #############################
# 消息刷新到磁盘中的消息条数阈值
log.flush.interval.messages=10000
# 消息刷新到磁盘中的最大时间间隔1s,单位是ms
log.flush.interval.ms=1000
############################# Log RetentionPolicy #########################
# 日志保留小时数，超时会自动删除，默认为7天
log.retention.hours=168
# 日志保留大小，超出大小会自动删除，默认为1G
#log.retention.bytes=1073741824
# 日志分片策略，单个日志文件的大小最大为1G，超出后则创建一个新的日志文件
log.segment.bytes=1073741824
# 每隔多长时间检测数据是否达到删除条件,300s
log.retention.check.interval.ms=300000
############################# Zookeeper ####################################
# Zookeeper连接信息，如果是zookeeper集群，则以逗号隔开，此行必须修改
zookeeper.connect=10.0.0.201:2181,10.0.0.202:2181,10.0.0.203:2181
# 连接zookeeper的超时时间,6s
zookeeper.connection.timeout.ms=6000
# 是否允许删除topic，默认为false，topic只会标记为marked for deletion
delete.topic.enable=true
```

**各节点部署** **Kafka**

```bash
#修改配置文件
[root@node1 kafka]#vim config/server.properties 
broker.id=1
listeners=PLAINTEXT://10.0.0.201:9092
log.dirs=/usrlocal/kafka/data/kafka-logs
zookeeper.connect=10.0.0.201:2181,10.0.0.202:2181,10.0.0.203,2181

[root@node2 kafka]#vim config/server.properties 
broker.id=2
listeners=PLAINTEXT://10.0.0.202:9092
log.dirs=/usrlocal/kafka/data/kafka-logs
zookeeper.connect=10.0.0.201:2181,10.0.0.202:2181,10.0.0.203,2181

[root@node3 kafka]#vim config/server.properties 
broker.id=3
listeners=PLAINTEXT://10.0.0.203:9092
log.dirs=/usrlocal/kafka/data/kafka-logs
zookeeper.connect=10.0.0.201:2181,10.0.0.202:2181,10.0.0.203,2181

#可以调整内存 生产中建议调为内存的一半，最大不超过32G
[root@node1|2|3 ~]#vim /usr/local/kafka/bin/kafka-server-start.sh
...
if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then
    export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"
fi
...

#启动服务
#前台启动
/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties
#后台启动
/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties
```

**kafka的service文件**

```bash
[root@node1 ~]#cat /lib/systemd/system/kafka.service
[unit]
Description=Apache kafka
After=network.target

[service]
Type=simple
#Environment=JAVA_HOME=/data/server/java
PIDFile=/usr/local/kafka/kafka.pid
Execstart=/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server. properties
Execstop=/bin/kill  -TERM ${MAINPID}
Restart=always
RestartSec=20

[Install]
wantedBy=multi-user.target

[root@node1 ~]#systemctl daemon-load
[root@node1 ~]#systemctl restart kafka.service
```

**每个kafka上都执行**

```bash
[root@ubuntu2404 ~]#cat install_kafka_cluster.sh 
#!/bin/bash

#支持在线和离线安装安装

NODE1=10.0.0.211
NODE2=10.0.0.212
NODE3=10.0.0.213

KAFKA_VERSION=3.9.0
#KAFKA_VERSION=3.8.0
#KAFKA_VERSION=3.7.0
#KAFKA_VERSION=3.6.1
#KAFKA_VERSION=3.5.1
#KAFKA_VERSION=3.5.0
#KAFKA_VERSION=3.4.0
#KAFKA_VERSION=3.3.2
#KAFKA_VERSION=3.2.0
#KAFKA_VERSION=-3.0.0

SCALA_VERSION=2.13

KAFKA_URL="https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/${KAFKA_VERSION}/kafka_${SCALA_VERSION}-${KAFKA_VERSION}.tgz"
#KAFKA_URL="https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.8.1/kafka_2.13-2.8.1.tgz"
#KAFKA_URL="https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.7.1/kafka_2.13-2.7.1.tgz"


KAFKA_INSTALL_DIR=/usr/local/kafka


HOST=`hostname -I|awk '{print $1}'`

.  /etc/os-release


color () {
    RES_COL=60
    MOVE_TO_COL="echo -en \\033[${RES_COL}G"
    SETCOLOR_SUCCESS="echo -en \\033[1;32m"
    SETCOLOR_FAILURE="echo -en \\033[1;31m"
    SETCOLOR_WARNING="echo -en \\033[1;33m"
    SETCOLOR_NORMAL="echo -en \E[0m"
    echo -n "$1" && $MOVE_TO_COL
    echo -n "["
    if [ $2 = "success" -o $2 = "0" ] ;then
        ${SETCOLOR_SUCCESS}
        echo -n $"  OK  "    
    elif [ $2 = "failure" -o $2 = "1"  ] ;then 
        ${SETCOLOR_FAILURE}
        echo -n $"FAILED"
    else
        ${SETCOLOR_WARNING}
        echo -n $"WARNING"
    fi
    ${SETCOLOR_NORMAL}
    echo -n "]"
    echo 
}

env () {
    if hostname -I |grep -q $NODE1;then
	    ID=1
	   hostnamectl set-hostname node1
	elif hostname -I |grep -q $NODE2;then
	    ID=2
	   hostnamectl set-hostname node2
	elif hostname -I |grep -q $NODE3;then
	    ID=3
	    hostnamectl set-hostname node3
    else
	    color 'IP地址错误' 1
	    exit
	fi
    cat >> /etc/hosts <<EOF
	
$NODE1   node1
$NODE2   node2
$NODE3   node3

EOF
}

install_jdk() {
    java -version &>/dev/null && { color "JDK 已安装!" 1 ; return;  }
    if command -v yum &>/dev/null ; then
        yum -y install java-1.8.0-openjdk-devel || { color "安装JDK失败!" 1; exit 1; }
    elif command -v apt &>/dev/null ; then
        apt update
        #apt install openjdk-11-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        #apt install openjdk-8-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        apt install openjdk-21-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
    else
       color "不支持当前操作系统!" 1
       exit 1
    fi
    java -version && { color "安装 JDK 完成!" 0 ; } || { color "安装JDK失败!" 1; exit 1; } 
}



install_zookeeper() {
	mv ${KAFKA_INSTALL_DIR}/config/zookeeper.properties{,.bak}
    cat > ${KAFKA_INSTALL_DIR}/config/zookeeper.properties <<EOF
tickTime=2000
initLimit=10
syncLimit=5
dataDir=${KAFKA_INSTALL_DIR}/data
clientPort=2181
maxClientCnxns=128
autopurge.snapRetainCount=3
autopurge.purgeInterval=24
server.1=${NODE1}:2888:3888
server.2=${NODE2}:2888:3888
server.3=${NODE3}:2888:3888
EOF
    mkdir -p ${KAFKA_INSTALL_DIR}/data
	
    echo $ID > ${KAFKA_INSTALL_DIR}/data/myid

    cat > ${KAFKA_INSTALL_DIR}/bin/zookeeper-startup.sh <<EOF
#!/bin/bash
nohup ${KAFKA_INSTALL_DIR}/bin/zookeeper-server-start.sh ${KAFKA_INSTALL_DIR}/config/zookeeper.properties   &
EOF
    chmod +x ${KAFKA_INSTALL_DIR}/bin/zookeeper-startup.sh
    cat > /lib/systemd/system/zookeeper.service <<EOF
[Unit]
Description=zookeeper.service
After=network.target

[Service]
Type=forking
ExecStart=${KAFKA_INSTALL_DIR}/bin/zookeeper-startup.sh
ExecStop=${KAFKA_INSTALL_DIR}/bin/zookeeper-server-stop.sh 

[Install]
WantedBy=multi-user.target
EOF
    systemctl daemon-reload
    systemctl enable --now  zookeeper.service
    systemctl is-active zookeeper.service
    if [ $? -eq 0 ] ;then 
        color "zookeeper 安装成功!" 0  
    else 
        color "zookeeper 安装失败!" 1
        exit 1
    fi		

} 
   

install_kafka(){
    if [ ! -f kafka_${SCALA_VERSION}-${KAFKA_VERSION}.tgz ];then
        wget -P /usr/local/src/  --no-check-certificate $KAFKA_URL  || { color  "下载失败!" 1 ;exit ; }
    fi
    tar xf /usr/local/src/${KAFKA_URL##*/}  -C /usr/local/
    ln -s /usr/local/kafka_${SCALA_VERSION}-${KAFKA_VERSION}  ${KAFKA_INSTALL_DIR}
    install_zookeeper
	
    echo PATH=${KAFKA_INSTALL_DIR}/bin:'$PATH' >> /etc/profile
	
    mv ${KAFKA_INSTALL_DIR}/config/server.properties{,.bak}
    cat > ${KAFKA_INSTALL_DIR}/config/server.properties <<EOF
broker.id=$ID
listeners=PLAINTEXT://${HOST}:9092
log.dirs=${KAFKA_INSTALL_DIR}/data
num.partitions=1
log.retention.hours=168
zookeeper.connect=${NODE1}:2181,${NODE2}:2181,${NODE3}:2181
zookeeper.connection.timeout.ms=6000
delete.topic.enable=true
EOF
    	
    cat > ${KAFKA_INSTALL_DIR}/bin/kafka-startup.sh <<EOF
#!/bin/bash
nohup ${KAFKA_INSTALL_DIR}/bin/kafka-server-start.sh  ${KAFKA_INSTALL_DIR}/config/server.properties &
EOF
    chmod +x ${KAFKA_INSTALL_DIR}/bin/kafka-startup.sh
	
    cat > /lib/systemd/system/kafka.service <<EOF
[Unit]                                                                          
Description=Apache kafka
After=network.target

[Service]
Type=forking
ExecStart=${KAFKA_INSTALL_DIR}/bin/kafka-startup.sh
ExecStop=/bin/kill  -TERM \${MAINPID}
Restart=always
RestartSec=20

[Install]
WantedBy=multi-user.target

EOF
    systemctl daemon-reload
    systemctl enable --now kafka.service
    #kafka-server-start.sh -daemon ${KAFKA_INSTALL_DIR}/config/server.properties 
    systemctl is-active kafka.service
    if [ $? -eq 0 ] ;then 
        color "kafka 安装成功!" 0  
    else 
        color "kafka 安装失败!" 1
        exit 1
    fi    
}


env

install_jdk

install_kafka

```





##### kafka基于KRaft协议

```
https://kafka.apache.org/quickstart
```

```bash
#修改每个kafka节点的主机名
[root@ubuntu2404 ~]#hostnamectl hostname node1
[root@ubuntu2404 ~]#hostnamectl hostname node2
[root@ubuntu2404 ~]#hostnamectl hostname node3

#在所有kafka节点上实现主机名称解析
[root@node3 ~]#vim /etc/hosts
10.0.0.201 node1
10.0.0.202 node2
10.0.0.203 node3
#安装 JAVA
apt update && apt install -y openjdk-21-jdk
```

```bash
#配置文件说明
[root@ubuntu2404 kafka]#vim config/kraft/server.properties
# 角色定义（同时作为Broker和Controller）
process.roles=broker,controller
# 节点唯一ID（不同节点需不同）
node.id=1
# 控制器节点列表（格式：node.id@host:port）
controller.quorum.voters=1@kafka01:9093,2@kafka02:9093,3@kafka03:9093
# 监听地址（Broker通信端口和Controller端口）
listeners=PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
# 对外暴露的地址（根据实际IP或域名配置）注意:不支持0.0.0.0
advertised.listeners=PLAINTEXT://kafka01:9092,CONTROLLER://kafka01:9093
# 日志存储目录
log.dirs=/data/kafka-logs
```

```bash
#修改配置文件
[root@node1 kafka]#vim config/kraft/server.properties 
node.id=1
controller.quorum.voters=1@10.0.0.201:9093,2@10.0.0.202:9093,3@10.0.0.203:9093
advertised.listeners=PLAINTEXT://10.0.0.201:9092,CONTROLLER://10.0.0.201:9093
log.dirs=/data/kraft-combined-logs

[root@node2 kafka]#vim config/kraft/server.properties 
node.id=2
controller.quorum.voters=1@10.0.0.201:9093,2@10.0.0.202:9093,3@10.0.0.203:9093
advertised.listeners=PLAINTEXT://10.0.0.202:9092,CONTROLLER://10.0.0.202:9093
log.dirs=/data/kraft-combined-logs

[root@node3 kafka]#vim config/kraft/server.properties 
node.id=3
controller.quorum.voters=1@10.0.0.201:9093,2@10.0.0.202:9093,3@10.0.0.203:9093
advertised.listeners=PLAINTEXT://10.0.0.203:9092,CONTROLLER://10.0.0.203:9093
log.dirs=/data/kraft-combined-logs
```

```bash
# 生成UUID
[root@node1 kafka]#./bin/kafka-storage.sh random-uuid
BP1xgpHnRkmioefCr-FQVg

所有节点需使用同一集群ID。
```

```bash
#初始化元数据存储
每个节点均需执行此操作
# 使用生成的UUID初始化存储目录
./bin/kafka-storage.sh format -t <生成的UUID> -c ./config/kraft/server.properties

[root@node1 kafka]#./bin/kafka-storage.sh format -t BP1xgpHnRkmioefCr-FQVg -c ./config/kraft/server.properties
Formatting metadata directory /data/kraft-combined-logs with metadata.version 3.9-IV0.
[root@node2 kafka]#./bin/kafka-storage.sh format -t BP1xgpHnRkmioefCr-FQVg -c ./config/kraft/server.properties
Formatting metadata directory /data/kraft-combined-logs with metadata.version 3.9-IV0.
[root@node3 kafka]#./bin/kafka-storage.sh format -t BP1xgpHnRkmioefCr-FQVg -c ./config/kraft/server.properties
Formatting metadata directory /data/kraft-combined-logs with metadata.version 3.9-IV0.
```

```bash
#启动服务
#前台启动
/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/kraft/server.properties
#后台启动
/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/kraft/server.properties
```



**测试**

```bash
[root@node1 kafka]#/usr/local/kafka/bin/kafka-topics.sh --describe --bootstrap-server 10.0.0.201:9092 --topic kang
Topic: kang     TopicId: rNtFDdTLQRGPr5pnW7IXtw PartitionCount: 3       ReplicationFactor: 2    Configs: 
        Topic: kang     Partition: 0    Leader: 3       Replicas: 3,1   Isr: 3,1        Elr: N/A        LastKnownElr: N/A
        Topic: kang     Partition: 1    Leader: 1       Replicas: 1,2   Isr: 1,2        Elr: N/A        LastKnownElr: N/A
        Topic: kang     Partition: 2    Leader: 2       Replicas: 2,3   Isr: 2,3        Elr: N/A        LastKnownElr: N/A
```

![image-20250311151525212](C:/Users/zhaok/AppData/Roaming/Typora/typora-user-images/image-20250311151525212.png)

| 分区 | **Leader（主副本）** | Follower（从副本） |
| ---- | -------------------- | ------------------ |
| 0    | **Broker 3**         | Broker 1           |
| 1    | **Broker 1**         | Broker 2           |
| 2    | **Broker 2**         | Broker 3           |



```bash
[root@node1 ~]#ls /usr/local/kafka/data/kafka-logs/
cleaner-offset-checkpoint  kang-0  kang-1  log-start-offset-checkpoint  meta.properties  recovery-point-offset-checkpoint  replication-offset-checkpoint
[root@node2 ~]#ls /usr/local/kafka/data/kafka-logs/
cleaner-offset-checkpoint  kang-1  kang-2  log-start-offset-checkpoint  meta.properties  recovery-point-offset-checkpoint  replication-offset-checkpoint
[root@node3 ~]#ls /usr/local/kafka/data/kafka-logs/
cleaner-offset-checkpoint  kang-0  kang-2  log-start-offset-checkpoint  meta.properties  recovery-point-offset-checkpoint  replication-offset-checkpoint
```



```bash
[root@node2 kafka]#/usr/local/kafka/bin/kafka-topics.sh --create --topic hzk --bootstrap-server 10.0.0.201:9092 --partitions 3 --replication-factor 3
Created topic hzk.
[root@node2 kafka]#/usr/local/kafka/bin/kafka-topics.sh --describe --bootstrap-server 10.0.0.202:9092 --topic hzk
Topic: hzk      TopicId: Y16ZvnfrTHSM1OJV3cdT_Q PartitionCount: 3       ReplicationFactor: 3    Configs: segment.bytes=1073741824
        Topic: hzk      Partition: 0    Leader: 1       Replicas: 1,2,3 Isr: 1,2,3      Elr:    LastKnownElr: 
        Topic: hzk      Partition: 1    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1      Elr:    LastKnownElr: 
        Topic: hzk      Partition: 2    Leader: 3       Replicas: 3,1,2 Isr: 3,1,2      Elr:    LastKnownElr: 
```



### kafka读写数据

```bash
kafka-topics.sh 			#消息的管理命令
kafka-console-producer.sh	#生产者的模拟命令
kafka-console-consumer.sh	#消费者的模拟命令
```

**修改host文件**，**要不去互联网上反向解析地址**

```bash
[root@ubuntu2404 ~]#cat /etc/hosts
10.0.0.200 ubuntu2404.wang.org
```

#### **创建** **Topic**

```bash
#新版命令,通过--bootstrap-server指定kafka的地址
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --create --topic kang --bootstrap-server 10.0.0.200:9092
Created topic kang.

#创建多副本
[root@node1 kafka]#/usr/local/kafka/bin/kafka-topics.sh --create --topic kang --bootstrap-server 10.0.0.201:9092 --partitions 3 --replication-factor 2
Created topic kang.

#旧版命令，通过--zookeeper指定zookeeper的地址
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --create --topic kang --zookeeper 10.0.0.200:9092
```

#### **获取所有** **Topic**

```bash
#新版命令
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --list --bootstrap-server 10.0.0.200:9092
kang

#旧版命令
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --list --zookeeper 10.0.0.200:9092
kang
```

#### **查看** **Topic** **详情**

```bash
#新版命令
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --describe --bootstrap-server 10.0.0.200:9092 --topic kang
Topic: kang     TopicId: YNAhx6ghQQ2j9nsWeQKCvw PartitionCount: 1       ReplicationFactor: 1    Configs: 
        Topic: kang     Partition: 0    Leader: 0       Replicas: 0     Isr: 0  Elr: N/A        LastKnownElr: N/A

#旧版命令
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --describe --zookeeper 10.0.0.200:9092 --topic kang
```

#### **生产** **Topic**

```bash
#发送消息命令格式:
kafka-console-producer.sh --broker-list <kafkaIP1>:<端口>,<kafkaIP2>:<端口> --topic <topic名称> --producer-property group.id=<组名>
```

范例：

```bash
#交互式输入消息,按Ctrl+C退出
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-console-producer.sh --broker-list 10.0.0.200:9092 --topic kang
>hello1
>hello2
>hello
```

#### **消费** **Topic**

```
#接收消息命令格式:
kafka-console-consumer.sh --bootstrap-server <host>:<post> --topic <topic名称> --from-beginning --consumer-property group.id=<组名称>
```

注意：

- 消息者先生产消息，消费者后续启动，也能收到之前生产的消息
- 同一个消息在同一个group内的消费者只有被一个消费者消费，比如：共100条消息，在一个group内有A，B两个消费者，其中A消费50条，B消费另外的50条消息。从而实现负载均衡，不同group内的消费者则可以同时消费同一个消息
- --from-beginning 表示消费前发布的消息也能收到，默认只能收到消费后发布的新消息

范例：

```bash
#交互式持续接收消息,按Ctrl+C退出
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-console-consumer.sh --topic kang --bootstrap-server 10.0.0.200:9092

[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-console-consumer.sh --topic kang --bootstrap-server 10.0.0.200:9092 --from-beginning
hello1
hello2
hello

#一个消息同时只能被同一个组内一个消费者消费（单播机制），实现负载均衡，而不能组可以同时消费同一个消息（多播机制）
[root@node2 ~]#/usr/local/kafka/bin/kafka-console-consumer.sh --topic kang --bootstrap-server 10.0.0.200:9092 --from-beginning --consumer-property group.id=group1
[root@node2 ~]#/usr/local/kafka/bin/kafka-console-consumer.sh --topic kang --bootstrap-server 10.0.0.200:9092 --from-beginning --consumer-property group.id=group1
```

#### **删除** **Topic**

```bash
#注意：需要修改配置文件server.properties中的delete.topic.enable=true并重启 逻辑上删除  
#新版本
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --delete --bootstrap-server 10.0.0.200:9092 --topic kang
```



### 消息积压

**Kafka 消息积压可能由多种原因引起，以下是一些可能的原因：**

1. **消费者处理速度慢：** 如果消费者处理消息的速度不足以跟上生产者的速度，就会导致消息积压。这可能是因为消费者逻辑复杂、消费者数量不足、消费者宕机或者网络延迟等原因引起的。
2. **消费者宕机：** 如果某个消费者宕机，其负责处理的分区将没有消费者来消费消息，导致消息在该分区上积压
3. **网络问题：** 网络故障可能导致生产者和消费者之间的通信延迟或中断，从而影响消息的传递速度
4. **硬件资源不足：** Kafka 集群所在的机器，包括生产者、消费者和 Broker 所在的机器，可能由于 CPU、内存或磁盘等资源不足，导致消息处理速度变慢
5. **分区不均匀：** 如果某些分区的负载比其他分区更高，可能导致这些分区上的消息积压。这可能是由于分区数量设置不合理、数据分布不均匀等原因引起的
6. **生产者速度过快：** 如果生产者生产消息的速度过快，而消费者无法及时处理，就会导致消息积压
7. **配置不当：** Kafka 的一些配置参数，如副本数、分区数、消费者数量等，需要根据实际情况进行合理的配置。如果配置不当，可能导致消息积压问题。
8. **异常情况：** 突发性的异常情况，如硬件故障、网络故障、软件 bug 等，都可能导致消息积压

**Kafka 消息积压可能的解决方案：**

- **增加消费者数量**：如果消费者处理速度不足导致消息积压，可以增加消费者的数量来提高处理速度。
- **扩展****Kafka****集群**：如果消息积压是由于Kafka集群的吞吐量达到极限导致的，考虑扩展Kafka集群的规模来增加其处理能力。
- **数据分区**：合理划分数据分区可以提高并行处理能力，从而减少消息积压。
- **数据清理**：定期清理过期的数据和日志文件，以释放磁盘空间并提高性能。
- **优化消费者代码**：检查消费者代码，确保它们是高效的。可能存在一些性能瓶颈或不必要的延迟。
- **调整****Kafka****配置**：根据需要调整Kafka的配置参数，例如增加分区数量、调整副本数量、调整日志清理策略等。
- **监控和警报**：设置监控系统，及时发现消息积压问题并发送警报。这样可以在问题出现之前采取行动。
- **故障排除**：检查系统日志，查找可能的问题源，并采取相应的措施解决问题。

**kafka 要发现消息积压，可以考虑以下方法：**

- **监控工具：** Kafka 提供了一些监控工具，例如 Kafka Manager、Burrow、Kafka Offset Monitor 等。这些工具可以帮助你监控每个分区的偏移量（offset）和消费者组的状态。通过检查偏移量的增长速度，你可以判断是否有消息积压
- **Consumer Lag****：** Consumer Lag 是指消费者组相对于生产者的消息偏移量的差异。通过监控 Consumer Lag，你可以了解消费者是否跟上了生产者的速度。如果 Consumer Lag 增长较快，可能表示消息积压
- **Kafka Logs** **目录：** Kafka 的每个分区都有一个日志目录，其中包含了该分区的消息数据。可以检查每个分区的日志目录，查看是否有大量的未消费的消息
- **Kafka Broker Metrics****：** Kafka 提供了一系列的 broker metrics，包括消息入队速率、出队速率等。通过监控这些指标，可以了解Kafka 集群的负载状况
- **操作系统资源：** 如果 Kafka 所在的机器资源不足，可能导致消息积压。监控 CPU、内存、磁盘等系统资源，确保它们没有达到极限
- **警报系统：** 设置警报系统，当某些指标达到预定的阈值时触发警报，通知运维人员或相关团队及时处理

范例：生成消息

```bash
#模拟生产者生产大量消息
[root@ubuntu2404 ~]#( while true; do echo $i;let i++;done ) | /usr/local/kafka/bin/kafka-console-producer.sh --broker-list 10.0.0.200:9092 --topic kang

#下面命令查看消费组中每个Topic的堆积消息数。“LAG”表示每个Topic的总堆积数
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server 10.0.0.200:9092 --describe --all-groups

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                           HOST            CLIENT-ID
group1          kang            0          1213331         1287994         74663           console-consumer-8a552f58-205a-4a6a-9e33-8bad79e751a6 /10.0.0.200     console-consumer
Consumer group 'group2' has no active members.

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
group2          kang            0          6               1285823         1285817         -               -               -
```



```bash
#查看消息积压
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server 10.0.0.200:9092 --describe --all-groups
```



### kafka 工具

#### offset explorer

```
https://www.kafkatool.com/download.html
```

#### **kafka-eagle**

```
http://www.kafka-eagle.org/
https://github.com/smartloli/kafka-eagle-bin
https://www.cnblogs.com/smartloli/
```

#### **kafka-UI**

```
https://docs.kafka-ui.provectus.io/
https://github.com/provectus/kafka-ui
https://hub.docker.com/r/provectuslabs/kafka-ui/
```

```
[root@ubuntu2404 ~]#docker run --name kafka-ui -it -p 8080:8080 -e DYNAMIC_CONFIG_ENABLED=true provectuslabs/kafka-ui
[root@ubuntu2404 ~]#docker run --name kafka-ui -it -p 8080:8080 -e DYNAMIC_CONFIG_ENABLED=true registry.cn-beijing.aliyuncs.com/wangxiaochun/kafka-ui:v0.7.2
```

#### **Kafka Manager**

```
https://github.com/yahoo/CMAK
```



## **RabbitMQ**

### RabbitMQ简介

Erlang 语言

**RabbitMQ** **优势：**

- 基于Erlang 语言开发，具有高并发优点、支持分布式
- 具有消息确认机制、消息持久化机制，消息可靠性和集群可靠性高、简单易用、运行稳定、跨平台、多语言
- 自带图形 Web 管理功能
- 开源

**RabbitMQ** **架构**

![image-20250311161746687](5day-png/35rabbirMQ构架.png)

**Message:** 消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。

**Publisher:** 消息的生产者，就是一个向交换器发布消息的客户端应用程序。

**Consumer:** 消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。

**Broker**: 接收和分发消息的应用，表示消息队列服务器实体。RabbitMQ Server 就是Message Broker。

**Virtual host**: 为了支持多租户和安全原因，当多个不同的用户使用同一个RabbitMQ Server时，可以先划分出多个 vhost，每个用户使独立的vhost创建exchange／queue 等。每一个虚拟主机表示一批交换器、消息队列和相关对象的集合。虚拟主机是共享相同的身份认证加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost是AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。

**Exchange**: 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。message 到达 broker 会由Exchange 根据分发规则，匹配查询表中的路由信息，分发消息到队列中。常用的类型：direct (point to point 点对点),topic (publish subscribe发布者订阅者)和fanout (multicast多播)等

**Queue:** 消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直队列里面，等待消费者连接到这个队列将其取走。

**Binding**: 绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将换器理解成一个由绑定构成的路由表。exchange 和 queue 之间的虚拟连接，binding 中可以包含 routing key。Binding 信息被保存到exchange 中的查询表中，用于message 的分发依据。

**Connection**: 网络连接，比如一个TCP连接。publisher／consumer 和 broker 之间的TCP 连接。

**Channel**: 信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内部的虚拟连接，AMQP 命令都是通过道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。Channel 是在 connection内部建立的逻辑连接，如果应用程序支持多线程，通常每个 thread创建单独的channel 进行通讯，AMQP method 包含了channel id 帮助客户端和messagebroker极大减少了操作系统建立TCP connection 的开销

**RabbitMQ** **生产者消费者**

<img src="5day-png/35RabbiotMQ生产者消费者.png" alt="image-20250311162023413" style="zoom:50%;" />

- 生产者发送消息到 broker server（RabbitMQ）
- 在 Broker 内部，用户创建Exchange／Queue，通过 Binding 规则将两者联系在一起
- Exchange 分发消息，根据类型／binding 的不同分发策略有区别
- 消息最后来到Queue 中，等待消费者取走。



### **RabbitMQ** **单机部署**

```
红帽系统安装
https://rabbitmq.com/install-rpm.html
Debian和Ubuntu安装
https://rabbitmq.com/install-debian.html
通用二进制安装
https://rabbitmq.com/install-generic-unix.html
github 下载地址
https://github.com/rabbitmq/rabbitmq-server/releases
```



```
https://www.rabbitmq.com/download.html
```

#### 包安装

```bash
[root@ubuntu2404 ~]#apt-cache madison rabbitmq-server
rabbitmq-server | 3.12.1-1ubuntu1.1 | http://mirrors.ustc.edu.cn/ubuntu noble-proposed/main amd64 Packages
rabbitmq-server | 3.12.1-1ubuntu1 | http://mirrors.ustc.edu.cn/ubuntu noble/main amd64 Packages
rabbitmq-server | 3.12.1-1ubuntu1 | http://cn.archive.ubuntu.com/ubuntu noble/main amd64 Packages
rabbitmq-server | 3.12.1-1ubuntu1 | http://mirrors.ustc.edu.cn/ubuntu noble/main Sources
rabbitmq-server | 3.12.1-1ubuntu1.1 | http://mirrors.ustc.edu.cn/ubuntu noble-proposed/main Sources
```

**主机名解析(可选)**

早期版本安装必须做主机名解析,否则无法启动RabbitMQ服务,并且后续不能再修改主机名

注意: 当前版本不再有此限制

在当前MQ 服务器配置本地主机名解析：

```bash
[root@ubuntu2404 ~]#hostnamectl hostname rabbitmq
```

```bash
#apt包安装
[root@rabbitmq ~]#apt install rabbitmq-server -y
```

```bash
#官方安装脚本
https://www.rabbitmq.com/install-debian.html#apt-bintray-quick-start
```

```bash
#!/bin/bash

. /etc/os-release

sudo apt update && apt-get install curl gnupg apt-transport-https -y

## Team RabbitMQ's main signing key
curl -1sLf "https://keys.openpgp.org/vks/v1/by-fingerprint/0A9AF2115F4687BD29803A206B73A36E6026DFCA" | sudo gpg --dearmor | sudo tee /usr/share/keyrings/com.rabbitmq.team.gpg > /dev/null
## Community mirror of Cloudsmith: modern Erlang repository
curl -1sLf https://github.com/rabbitmq/signing-keys/releases/download/3.0/cloudsmith.rabbitmq-erlang.E495BB49CC4BBE5B.key | sudo gpg --dearmor | sudo tee /usr/share/keyrings/rabbitmq.E495BB49CC4BBE5B.gpg > /dev/null
## Community mirror of Cloudsmith: RabbitMQ repository
curl -1sLf https://github.com/rabbitmq/signing-keys/releases/download/3.0/cloudsmith.rabbitmq-server.9F4587F226208342.key | sudo gpg --dearmor | sudo tee /usr/share/keyrings/rabbitmq.9F4587F226208342.gpg > /dev/null

## Add apt repositories maintained by Team RabbitMQ
sudo tee /etc/apt/sources.list.d/rabbitmq.list <<EOF
## Provides modern Erlang/OTP releases
##
deb [signed-by=/usr/share/keyrings/rabbitmq.E495BB49CC4BBE5B.gpg] https://ppa1.novemberain.com/rabbitmq/rabbitmq-erlang/deb/ubuntu ${VERSION_CODENAME} main
deb-src [signed-by=/usr/share/keyrings/rabbitmq.E495BB49CC4BBE5B.gpg] https://ppa1.novemberain.com/rabbitmq/rabbitmq-erlang/deb/ubuntu ${VERSION_CODENAME} main

# another mirror for redundancy
deb [signed-by=/usr/share/keyrings/rabbitmq.E495BB49CC4BBE5B.gpg] https://ppa2.novemberain.com/rabbitmq/rabbitmq-erlang/deb/ubuntu ${VERSION_CODENAME} main
deb-src [signed-by=/usr/share/keyrings/rabbitmq.E495BB49CC4BBE5B.gpg] https://ppa2.novemberain.com/rabbitmq/rabbitmq-erlang/deb/ubuntu ${VERSION_CODENAME} main

## Provides RabbitMQ
##
deb [signed-by=/usr/share/keyrings/rabbitmq.9F4587F226208342.gpg] https://ppa1.novemberain.com/rabbitmq/rabbitmq-server/deb/ubuntu ${VERSION_CODENAME} main
deb-src [signed-by=/usr/share/keyrings/rabbitmq.9F4587F226208342.gpg] https://ppa1.novemberain.com/rabbitmq/rabbitmq-server/deb/ubuntu ${VERSION_CODENAME} main

# another mirror for redundancy
deb [signed-by=/usr/share/keyrings/rabbitmq.9F4587F226208342.gpg] https://ppa2.novemberain.com/rabbitmq/rabbitmq-server/deb/ubuntu ${VERSION_CODENAME} main
deb-src [signed-by=/usr/share/keyrings/rabbitmq.9F4587F226208342.gpg] https://ppa2.novemberain.com/rabbitmq/rabbitmq-server/deb/ubuntu ${VERSION_CODENAME} main
EOF

## Update package indices
sudo apt-get update -y

## Install Erlang packages
sudo apt-get install -y erlang-base \
                        erlang-asn1 erlang-crypto erlang-eldap erlang-ftp erlang-inets \
                        erlang-mnesia erlang-os-mon erlang-parsetools erlang-public-key \
                        erlang-runtime-tools erlang-snmp erlang-ssl \
                        erlang-syntax-tools erlang-tftp erlang-tools erlang-xmerl

## Install rabbitmq-server and its dependencies
sudo apt-get install rabbitmq-server -y --fix-missing
```

```bash
[root@rabbitmq ~]#systemctl status rabbitmq-server
```



#### 二进制安装

```
通用二进制安装官方说明
RabbitMQ版本和Erlang版本对应要求
范例: 二进制安装一键安装脚本
https://www.rabbitmq.com/install-generic-unix.html
https://www.rabbitmq.com/which-erlang.html
```

### **RabbitMQ** **管理**

#### **启用远程登陆** **Web** **管理界面**

```bash
#官方文档
https://www.rabbitmq.com/management.html
```

```bash
#端口说明
5672   #客户端访问的端口
15672  #web 管理端口,默认没有打开,需要启用插件
25672  #集群状态通信端口
```

**安装** **Web** **管理插件** **rabbitmq_management**

```bash
[root@rabbitmq ~]#rabbitmq-plugins list
Listing plugins with pattern ".*" ...
 Configured: E = explicitly enabled; e = implicitly enabled
 | Status: * = running on rabbit@rabbitmq
 |/
[  ] rabbitmq_amqp1_0                  4.0.7
[  ] rabbitmq_auth_backend_cache       4.0.7
[  ] rabbitmq_auth_backend_http        4.0.7
[  ] rabbitmq_auth_backend_ldap        4.0.7
[  ] rabbitmq_auth_backend_oauth2      4.0.7
[  ] rabbitmq_auth_mechanism_ssl       4.0.7
[  ] rabbitmq_consistent_hash_exchange 4.0.7
[  ] rabbitmq_event_exchange           4.0.7
[  ] rabbitmq_federation               4.0.7
[  ] rabbitmq_federation_management    4.0.7
[  ] rabbitmq_federation_prometheus    4.0.7
[  ] rabbitmq_jms_topic_exchange       4.0.7
[  ] rabbitmq_management               4.0.7
[  ] rabbitmq_management_agent         4.0.7
[  ] rabbitmq_mqtt                     4.0.7
[  ] rabbitmq_peer_discovery_aws       4.0.7
[  ] rabbitmq_peer_discovery_common    4.0.7
[  ] rabbitmq_peer_discovery_consul    4.0.7
[  ] rabbitmq_peer_discovery_etcd      4.0.7
[  ] rabbitmq_peer_discovery_k8s       4.0.7
[  ] rabbitmq_prometheus               4.0.7
[  ] rabbitmq_random_exchange          4.0.7
[  ] rabbitmq_recent_history_exchange  4.0.7
[  ] rabbitmq_sharding                 4.0.7
[  ] rabbitmq_shovel                   4.0.7
[  ] rabbitmq_shovel_management        4.0.7
[  ] rabbitmq_shovel_prometheus        4.0.7
[  ] rabbitmq_stomp                    4.0.7
[  ] rabbitmq_stream                   4.0.7
[  ] rabbitmq_stream_management        4.0.7
[  ] rabbitmq_top                      4.0.7
[  ] rabbitmq_tracing                  4.0.7
[  ] rabbitmq_trust_store              4.0.7
[  ] rabbitmq_web_dispatch             4.0.7
[  ] rabbitmq_web_mqtt                 4.0.7
[  ] rabbitmq_web_mqtt_examples        4.0.7
[  ] rabbitmq_web_stomp                4.0.7
[  ] rabbitmq_web_stomp_examples       4.0.7
[root@rabbitmq ~]#rabbitmq-plugins enable rabbitmq_management
Enabling plugins on node rabbit@rabbitmq:
rabbitmq_management
The following plugins have been configured:
  rabbitmq_management
  rabbitmq_management_agent
  rabbitmq_web_dispatch
Applying plugin configuration to rabbit@rabbitmq...
The following plugins have been enabled:
  rabbitmq_management
  rabbitmq_management_agent
  rabbitmq_web_dispatch

started 3 plugins.
[root@rabbitmq ~]#rabbitmq-plugins list
Listing plugins with pattern ".*" ...
 Configured: E = explicitly enabled; e = implicitly enabled
 | Status: * = running on rabbit@rabbitmq
 |/
[  ] rabbitmq_amqp1_0                  4.0.7
[  ] rabbitmq_auth_backend_cache       4.0.7
[  ] rabbitmq_auth_backend_http        4.0.7
[  ] rabbitmq_auth_backend_ldap        4.0.7
[  ] rabbitmq_auth_backend_oauth2      4.0.7
[  ] rabbitmq_auth_mechanism_ssl       4.0.7
[  ] rabbitmq_consistent_hash_exchange 4.0.7
[  ] rabbitmq_event_exchange           4.0.7
[  ] rabbitmq_federation               4.0.7
[  ] rabbitmq_federation_management    4.0.7
[  ] rabbitmq_federation_prometheus    4.0.7
[  ] rabbitmq_jms_topic_exchange       4.0.7
[E*] rabbitmq_management               4.0.7
[e*] rabbitmq_management_agent         4.0.7
[  ] rabbitmq_mqtt                     4.0.7
[  ] rabbitmq_peer_discovery_aws       4.0.7
[  ] rabbitmq_peer_discovery_common    4.0.7
[  ] rabbitmq_peer_discovery_consul    4.0.7
[  ] rabbitmq_peer_discovery_etcd      4.0.7
[  ] rabbitmq_peer_discovery_k8s       4.0.7
[  ] rabbitmq_prometheus               4.0.7
[  ] rabbitmq_random_exchange          4.0.7
[  ] rabbitmq_recent_history_exchange  4.0.7
[  ] rabbitmq_sharding                 4.0.7
[  ] rabbitmq_shovel                   4.0.7
[  ] rabbitmq_shovel_management        4.0.7
[  ] rabbitmq_shovel_prometheus        4.0.7
[  ] rabbitmq_stomp                    4.0.7
[  ] rabbitmq_stream                   4.0.7
[  ] rabbitmq_stream_management        4.0.7
[  ] rabbitmq_top                      4.0.7
[  ] rabbitmq_tracing                  4.0.7
[  ] rabbitmq_trust_store              4.0.7
[e*] rabbitmq_web_dispatch             4.0.7
[  ] rabbitmq_web_mqtt                 4.0.7
[  ] rabbitmq_web_mqtt_examples        4.0.7
[  ] rabbitmq_web_stomp                4.0.7
[  ] rabbitmq_web_stomp_examples       4.0.7
```

**注意: rabbitmq从3.3.0开始禁止使用guest/guest权限通过除localhost外的访问，直接访问报错**

**开启用户** **guest** **远程登录功能**

```bash
#配置文件说明
https://www.rabbitmq.com/configure.html
```

默认管理页面只能能过本机127.0.0.1:15672访问，可以开启guest用户远程登录功能

```bash
[root@rabbitmq ~]#ls /etc/rabbitmq/
enabled_plugins

#创建配置文件开启远程登录功能
[root@rabbitmq ~]#echo "loopback_users = none" > /etc/rabbitmq/rabbitmq.conf
[root@rabbitmq ~]#systemctl restart rabbitmq-server
```

```
#打开浏览器访问下面地址,默认用用户和密码都是guest登录
http://10.0.0.200服务器:15672
```



#### **修改guest密码**

登录ui界面——》Admin——》guest——》Update this user

#### **创建用户**

- management

  用户可以访问管理插件

- policymaker

  用户可以访问管理插件并管理他们有权访问的虚拟主机的策略和参数。

- monitoring

  用户可以访问管理插件并查看所有连接和频道以及节点相关的信息。

- administrator

  用户可以执行监控可以执行的所有操作，管理用户、虚拟主机和权限，关闭其他用户的连接，以及管理所有虚拟主机的策略和参数。

```bash
[root@rabbitmq ~]#rabbitmqctl list_users
Listing users ...
user    tags
guest   [administrator]

#创建用户,但默认无权限无法登录web
[root@rabbitmq ~]#rabbitmqctl add_user admin 123456
Adding user "admin" ...
Done. Don't forget to grant the user permissions to some virtual hosts! See 'rabbitmqctl help set_permissions' to learn more.

#确认用户
[root@rabbitmq ~]#rabbitmqctl list_users
Listing users ...
user    tags
guest   [administrator]
admin   []

#修改密码
[root@rabbitmq ~]#rabbitmqctl change_password admin 666666

#加入管理员角色
[root@rabbitmq ~]#rabbitmqctl set_user_tags admin administrator

#验证加入角色，可以登录Web管理页面进行管理，但仍然无法访问virtual hosts,需要授权才可以
[root@rabbitmq ~]#rabbitmqctl list_users
Listing users ...
user    tags
guest   [administrator]
admin   [administrator]

#查看权限
[root@rabbitmq ~]#rabbitmqctl list_user_permissions admin
Listing permissions for user "admin" ...

#设置vhost权限，格式如下
rabbitmqctl [--node <node>] [--longnames] [--quiet] set_permissions [--vhost <vhost>] <username> <conf> <write> <read>
#设置对vhost “/” 给admin用户授予:configure配置，write写，read读的权限，“.*”表示正则表达式的任意字符串
#默认vhost为 "/"

#创建 deamo-vhost 虚拟主机
[root@rabbitmq ~]#rabbitmqctl add_vhost deamo-vhost
Adding vhost "deamo-vhost" ...

#给指定vhost设置权限
[root@rabbitmq ~]#rabbitmqctl set_permissions --vhost deamo-vhost admin ".*" ".*" ".*"
Setting permissions for user "admin" in vhost "deamo-vhost" ...

#验证权限
[root@rabbitmq ~]#rabbitmqctl list_user_permissions admin
Listing permissions for user "admin" ...
vhost   configure       write   read
deamo-vhost     .*      .*      .*

#列出虚拟主机
[root@rabbitmq ~]#rabbitmqctl list_vhosts
Listing vhosts ...
name
deamo-vhost
/

#删除用户
[root@rabbitmq ~]#rabbitmqctl delete_user admin
Deleting user "admin" ...
```



#### **创建** **queue**

Queues and Streams ——》Add a new queue

![image-20250311171649790](5day-png/35创建queue.png)

```bash
#创建虚拟主机
[root@rabbitmq ~]#rabbitmqctl add_vhost vhost1
Adding vhost "vhost1" ...

#验证虚拟主机
[root@rabbitmq ~]#rabbitmqctl list_vhosts
Listing vhosts ...
name
vhost1
deamo-vhost
/

#删除虚拟主机
[root@rabbitmq ~]#rabbitmqctl delete_vhost vhost1
Deleting vhost "vhost1" ...

#默认查看/ 虚拟主机的队列
[root@rabbitmq ~]#rabbitmqctl list_queues
Timeout: 60.0 seconds ...
Listing queues for vhost / ...
name messages
queue-test 0

#列出指定vhost的queue队列
[root@rabbitmq ~]#rabbitmqctl list_queues -p deamo-vhost
Timeout: 60.0 seconds ...
Listing queues for vhost deamo-vhost ...
name    messages
test    0

[root@rabbitmq ~]#rabbitmqctl list_queues --vhost /demo-vhost
Timeout: 60.0 seconds ...
Listing queues for vhost /demo-vhost ...
name messages
test    0

#删除队列
[root@rabbitmq ~]#rabbitmqctl delete_queue queue-test
```

### **Python** **客户端访问** **RabbitMQ**

#### **生产者（producter）**

```bash
[root@rabbitmq ~]#apt -y install python3 python3-pip
#加速配置
[root@rabbitmq ~]#python3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
[root@rabbitmq ~]#apt install -y python3-pika
#以前版本
[root@mq-server ~]#pip3 install pika
```

```python
[root@rabbitmq ~]#cat rabbitmq_product.py 
#!/usr/bin/python3
import pika
import json

username = 'guest'
password = 'guest'
host = '127.0.0.1'
port = 5672

def publish_messages():
    credentials = pika.PlainCredentials(username, password)
    connection = pika.BlockingConnection(pika.ConnectionParameters(host=host, port=port, virtual_host='/', credentials=credentials))
    channel = connection.channel()
    
    queue_name = 'rabbit-test'
    channel.queue_declare(queue=queue_name)

    for i in range(10):
        message = json.dumps({'OrderId': f"1000{i}"})
        channel.basic_publish(exchange='', routing_key=queue_name, body=message)
        print(message)

    connection.close()

if __name__ == "__main__":
    publish_messages()

```

#### **消费者（consumer）**

```python
[root@rabbitmq ~]#cat rabbitmq_consumer.py 
#!/usr/bin/python3

import pika

# Define MQ parameters
username = 'guest'
password = 'guest'
host = '127.0.0.1'
port = 5672

def consume_messages():
    credentials = pika.PlainCredentials(username, password)
    connection = pika.BlockingConnection(pika.ConnectionParameters(host=host, port=port, virtual_host='/', credentials=credentials))
    channel = connection.channel()

    channel.queue_declare(queue='rabbit-test', durable=False)

    def callback(ch, method, properties, body):
        print(body.decode())
        ch.basic_ack(delivery_tag=method.delivery_tag)

    channel.basic_consume(queue='rabbit-test', on_message_callback=callback, auto_ack=False)

    channel.start_consuming()

if __name__ == "__main__":
    consume_messages()
```

```bash
[root@rabbitmq ~]#python3 rabbitmq_product.py 
{"OrderId": "10000"}
{"OrderId": "10001"}
{"OrderId": "10002"}
{"OrderId": "10003"}
{"OrderId": "10004"}
{"OrderId": "10005"}
{"OrderId": "10006"}
{"OrderId": "10007"}
{"OrderId": "10008"}
{"OrderId": "10009"}
[root@rabbitmq ~]#rabbitmqctl list_queues
Timeout: 60.0 seconds ...
Listing queues for vhost / ...
name    messages
rabbit-test     10

[root@rabbitmq ~]#python3 rabbitmq_consumer.py 
{"OrderId": "10000"}
{"OrderId": "10001"}
{"OrderId": "10002"}
{"OrderId": "10003"}
{"OrderId": "10004"}
{"OrderId": "10005"}
{"OrderId": "10006"}
{"OrderId": "10007"}
{"OrderId": "10008"}
{"OrderId": "10009"}
[root@rabbitmq ~]#rabbitmqctl list_queues
Timeout: 60.0 seconds ...
Listing queues for vhost / ...
name    messages
rabbit-test     0
```

**RabbitMQ API**

```
https://rawcdn.githack.com/rabbitmq/rabbitmq-management/rabbitmq_v3_6_9/priv/www/api/index.html
```

```bash
[root@node1 ~]#apt update && apt -y install jq
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/overview|jq
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/nodes|jq
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/cluster-name
{"name":"rabbit@node1.wang.org"}
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/connections|jq
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/channels|jq
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/queues|jq
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/whoami|jq
{"name":"guest","tags":"administrator"}
```

范例: 健康性检查

```bash
[root@rabbitmq ~]#curl -u guest:guest http://10.0.0.200:15672/api/healthchecks/node
{"status":"ok"}
[root@node1 ~]#curl -u guest:guest http://10.0.0.101:15672/api/healthchecks/node
{"status":"ok"}
[root@node1 ~]#curl -u guest:guest http://10.0.0.101:15672/api/healthchecks/node/rabbit@node3
{"status":"ok"}
[root@node3 ~]#rabbitmqctl stop_app
Stopping rabbit application on node rabbit@node3 ...
[root@node3 ~]#curl -u guest:guest http://10.0.0.101:15672/api/healthchecks/node/rabbit@node3
{"status":"failed","reason":"{'EXIT',{aborted,{no_exists,rabbit_vhost}}}"}
[root@node3 ~]#rabbitmqctl start_app
Starting node rabbit@node3 ...
[root@node1 ~]#curl -u guest:guest http://10.0.0.101:15672/api/healthchecks/node/rabbit@node3
{"status":"ok"}
[root@node3 ~]#systemctl stop rabbitmq-server.service 
[root@node1 ~]#curl -u guest:guest http://10.0.0.101:15672/api/healthchecks/node/rabbit@node3
{"status":"failed","reason":"nodedown"}
```

**利用** **RabbitMQ API** **监控集群状态**

```python
[root@rabbitmq ~]#cat rabbitmq_monitor.py 
#!/usr/bin/python3
# -*- coding: utf-8 -*-

import requests

username = "guest"
password = "guest"
host = "localhost"
port = 15672
min_running_nodes = 3  # Set the minimum number of running nodes here

def get_node_status():
    url = f"http://{username}:{password}@{host}:{port}/api/nodes"
    try:
        response = requests.get(url)
        response.raise_for_status()
        nodes = response.json()
        running_nodes = [node["name"] for node in nodes if node.get("running")]
        return running_nodes
    except Exception as e:
        print(f"Error: {e}")
        return []

def count_server():
    running_nodes = get_node_status()
    if len(running_nodes) < min_running_nodes:
        print("集群异常")
    else:
        print("集群正常")

def main():
    count_server()

if __name__ == "__main__":
    main()
```

### **RabbitMQ** **集群部署**

RabbitMQ集群模式分为两种:普通和镜像模式

- **普通集群模式：**

  创建好RabbitMQ集群之后的默认模式

  queue 创建之后，如果没有其它policy，消息实体只存在于其中一个节点

  假设 A、B 两个 Rabbitmq节点仅有相同的元数据，即队列结构，但队列的数据仅保存有一份，即创建该队列的rabbitmq节点（A 节点）

  当消息进入 A 节点的 Queue 中后，consumer 从 B节点拉取时，RabbitMQ 会临时在 A、B 间进行消息传输，把 A中的消息实体取出并经过 B 发送给consumer，所以 consumer可以连接每一个节点，从中取消息

  该模式存在一个问题就是当 A 节点故障后，B节点无法取到A 节点中还未消费的消息实体。可能会丢失数据

  缺点： 没有数据高可用

  优点：多个节点可以实现负载均衡

- **镜像集群模式：** 

  **注意: 此模式从RabbitMQ-4.X以后不再支持**

  把需要的队列做成镜像队列，存在于多个节点，属于 RabbitMQ 的 HA方案（镜像模式是在普通模式的基础上，增加一些镜像策略）

  该模式解决了普通模式中的数据丢失问题，其实质和普通模式不同之处在于，消息实体会主动在镜像节点间同步，而不是在consumer取数据时临时拉取

  该模式带来的副作用也很明显，除了降低系统性能外，如果镜像队列数量过多，加之大量的消息进入，集群内部的网络带宽将会被种同步通讯大大消耗掉，所以在对可靠性要求较高的场合中适用

  一个队列想做成镜像队列，需要先设置policy， 然后客户端创建队列的时候，rabbitmq集群根据“队列名称”自动设置是普通集群模或镜像队列。

  优点：此模式因数据有冗余

  缺点：性能不佳

**集群中两种节点类型**:

- 内存节点：只将数据保存到内存。
- 磁盘节点：保存数据到内存和磁盘。

内存节点执行效率比磁盘节点要高，集群中只需要一个磁盘节点来保存数据就可以

如果集群中只有内存节点，那么不能全部停止它们，否则所有数据消息在服务器全部停机之后都会丢失。

<img src="5day-png/35集群设计构架.png" alt="image-20250311183909034" style="zoom:50%;" />

**环境准备**

```bash
[root@ubuntu2404 ~]#hostnamectl hostname node1
[root@ubuntu2404 ~]#hostnamectl hostname node2
[root@ubuntu2404 ~]#hostnamectl hostname node3
[root@ubuntu2404 ~]#vim /etc/hosts
10.0.0.201 node1
10.0.0.202 node2
10.0.0.203 node3
```

```bash
#使用官方脚本安装Rabbitmq单机版
[root@node1 ~]#bash install_rabbitmq_for_ubuntu.sh
[root@node2 ~]#bash install_rabbitmq_for_ubuntu.sh
[root@node3 ~]#bash install_rabbitmq_for_ubuntu.sh

#停止rabbitmq
[root@node1 ~]#systemctl stop rabbitmq-server.service 
[root@node2 ~]#systemctl stop rabbitmq-server.service 
[root@node3 ~]#systemctl stop rabbitmq-server.service 

#启用 Web 管理
[root@node1 ~]#rabbitmq-plugins enable rabbitmq_management
[root@node2 ~]#rabbitmq-plugins enable rabbitmq_management
[root@node3 ~]#rabbitmq-plugins enable rabbitmq_management
#启用远程管理功能
[root@node1 ~]#echo "loopback_users = none" > /etc/rabbitmq/rabbitmq.conf
[root@node1 ~]#systemctl restart rabbitmq-server.service
[root@node2 ~]#echo "loopback_users = none" > /etc/rabbitmq/rabbitmq.conf
[root@node2 ~]#systemctl restart rabbitmq-server.service
[root@node3 ~]#echo "loopback_users = none" > /etc/rabbitmq/rabbitmq.conf
[root@node3 ~]#systemctl restart rabbitmq-server.service
```

**同步** **cookie** **文件**

RabbitMQ 的集群是依赖于 erlang 的集群来工作的，所以必须先构建起 erlang的集群环境

而 Erlang 的集群中各节点是通过一个 magic cookie来实现的

这个 cookie 存放在 /var/lib/rabbitmq/.erlang.cookie 中

文件是 400的权限,所以必须保证各节点cookie 保持一致，否则节点之间就无法通信。

```bash
[root@node1 ~]#ll /var/lib/rabbitmq/.erlang.cookie
-r-------- 1 rabbitmq rabbitmq 57 Mar 11 18:54 /var/lib/rabbitmq/.erlang.cookie
[root@node1 ~]#cat /var/lib/rabbitmq/.erlang.cookie
o2WO16Vjq1TY3ypSLDvoMSQgepSmAthPzuukYrInnn381ydrr0CXSbha


[root@node1 ~]#scp /var/lib/rabbitmq/.erlang.cookie node2:/var/lib/rabbitmq/  
[root@node1 ~]#scp /var/lib/rabbitmq/.erlang.cookie node3:/var/lib/rabbitmq/

#重启服务生效
[root@node2 ~]#systemctl restart rabbitmq-server.service
[root@node3 ~]#systemctl restart rabbitmq-server.service
```

**创建集群**

```bash
#查看每一个节点都是独立运行的
[root@node1 ~]#rabbitmqctl cluster_status
```

```bash
#在 node2,node3 节点执行以下命令作为内存节点添加到 node1 所在集群
#将node2也加入集群
[root@node2 ~]#rabbitmqctl stop_app       #停止app服务
[root@node2 ~]#rabbitmqctl reset           #清空元数据
#将node2添加到集群当中，并成为内存节点，不加--ram 默认是磁盘节点,此步依赖于/etc/hosts的名称解析
[root@node2 ~]#rabbitmqctl join_cluster rabbit@node1 --ram

#启动app服务
[root@node2 ~]#rabbitmqctl start_app      
#将node3也加入集群
[root@node3 ~]#rabbitmqctl stop_app
[root@node3 ~]#rabbitmqctl reset
[root@node3 ~]#rabbitmqctl join_cluster rabbit@node1 --ram
[root@node3 ~]#rabbitmqctl start_app
```

查看集群状态

```bash
[root@node1 ~]#rabbitmqctl cluster_status
Cluster status of node rabbit@node1 ...
Basics

Cluster name: rabbit@node1
Total CPU cores available cluster-wide: 6

Disk Nodes

rabbit@node1

RAM Nodes

rabbit@node2
rabbit@node3

Running Nodes

rabbit@node1
rabbit@node2
rabbit@node3

Versions

rabbit@node1: RabbitMQ 3.12.1 on Erlang 25.3.2.8
rabbit@node2: RabbitMQ 3.12.1 on Erlang 25.3.2.8
rabbit@node3: RabbitMQ 3.12.1 on Erlang 25.3.2.8

CPU Cores

Node: rabbit@node1, available CPU cores: 2
Node: rabbit@node2, available CPU cores: 2
Node: rabbit@node3, available CPU cores: 2

Maintenance status

Node: rabbit@node1, status: not under maintenance
Node: rabbit@node2, status: not under maintenance
Node: rabbit@node3, status: not under maintenance

Alarms

(none)

Network Partitions

(none)

Listeners

Node: rabbit@node1, interface: [::], port: 15672, protocol: http, purpose: HTTP API
Node: rabbit@node1, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication
Node: rabbit@node1, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0
Node: rabbit@node2, interface: [::], port: 15672, protocol: http, purpose: HTTP API
Node: rabbit@node2, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication
Node: rabbit@node2, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0
Node: rabbit@node3, interface: [::], port: 15672, protocol: http, purpose: HTTP API
Node: rabbit@node3, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication
Node: rabbit@node3, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0

Feature flags

Flag: classic_mirrored_queue_version, state: enabled
Flag: classic_queue_type_delivery_support, state: enabled
Flag: direct_exchange_routing_v2, state: enabled
Flag: drop_unroutable_metric, state: enabled
Flag: empty_basic_get_metric, state: enabled
Flag: feature_flags_v2, state: enabled
Flag: implicit_default_bindings, state: enabled
Flag: listener_records_in_ets, state: enabled
Flag: maintenance_mode_status, state: enabled
Flag: quorum_queue, state: enabled
Flag: restart_streams, state: enabled
Flag: stream_queue, state: enabled
Flag: stream_sac_coordinator_unblock_group, state: enabled
Flag: stream_single_active_consumer, state: enabled
Flag: tracking_records_in_ets, state: enabled
Flag: user_limits, state: enabled
Flag: virtual_host_metadata, state: enabled
```



## **Nacos**

- **服务注册与发现**：适配 Spring Cloud 服务注册与发现标准，默认集成对应 Spring Cloud 版本所支持的负载均衡组件的适配。
- **分布式配置管理**：支持分布式系统中的外部化配置，配置更改时自动刷新。

```
https://spring.io/projects/spring-cloud-alibaba
https://sca.aliyun.com/en/
https://github.com/alibaba/spring-cloud-alibaba/blob/2022.x/README-zh.md
```

### Spring Cloud Alibaba

#### 主要功能

- **服务限流降级**：默认支持 WebServlet、WebFlux、OpenFeign、RestTemplate、Spring Cloud Gateway、Dubbo 和 RocketMQ 限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级 Metrics 监控。
- **服务注册与发现**：适配 Spring Cloud 服务注册与发现标准，默认集成对应 Spring Cloud 版本所支持的负载均衡组件的适配。
- **分布式配置管理**：支持分布式系统中的外部化配置，配置更改时自动刷新。
- **消息驱动能力**：基于 Spring Cloud Stream 为微服务应用构建消息驱动能力。
- **分布式事务**：使用 @GlobalTransactional 注解， 高效并且对业务零侵入地解决分布式事务问题。
- **阿里云对象存储**：阿里云提供的海量、安全、低成本、高可靠的云存储服务。支持在任何应用、任何时间、任何地点存储和访问任意类型的数据。
- **分布式任务调度**：提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。同时提供分布式的任务执行模型，如网格任务。网格任务支持海量子任务均匀分配到所有 Worker（schedulerx-client）上执行。
- **阿里云短信服务**：覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。

#### 组件

**[Sentinel](https://github.com/alibaba/Sentinel)**：把流量作为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。

**[Nacos](https://github.com/alibaba/Nacos)**：一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。

**[RocketMQ](https://rocketmq.apache.org/)**：一款开源的分布式消息系统，基于高可用分布式集群技术，提供低延时的、高可靠的消息发布与订阅服务。

**[Seata](https://github.com/apache/incubator-seata)**：阿里巴巴开源产品，一个易于使用的高性能微服务分布式事务解决方案。

**[Alibaba Cloud OSS](https://www.aliyun.com/product/oss)**: 阿里云对象存储服务（Object Storage Service，简称 OSS），是阿里云提供的海量、安全、低成本、高可靠的云存储服务。您可以在任何应用、任何时间、任何地点存储和访问任意类型的数据。

**[Alibaba Cloud SchedulerX](https://cn.aliyun.com/aliware/schedulerx)**: 阿里中间件团队开发的一款分布式任务调度产品，提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。

**[Alibaba Cloud SMS](https://www.aliyun.com/product/sms)**: 覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。

 ![image-20250311191849787](5day-png/35Spring Cloud Alibaba 的微服务架构.png)

### nacos

Nacos提供四大功能。

- **服务发现和服务健康检查**

  Nacos 使服务能够轻松地通过 DNS 或 HTTP 接口注册自己并发现其他服务。Nacos 还提供服务的实时健康检查，以防止向不健康的主机或服务实例发送请求。

- **动态配置管理**

  动态配置服务让您能够跨所有环境以集中、动态的方式管理所有服务的配置。Nacos 让您在配置更新时无需重新部署应用和服务，让配置变更更加高效、敏捷。

- **动态 DNS 服务**

  Nacos 支持加权路由，让您能够更轻松地在数据中心内的生产环境中实现中间层负载均衡、灵活的路由策略、流量控制和简单的 DNS 解析服务。它可以帮助您轻松实现基于 DNS 的服务发现，并防止应用程序与特定于供应商的服务发现 API 耦合。

- **服务和元数据管理**

  Nacos 提供了一个易于使用的服务仪表板，帮助您管理服务元数据、配置、kubernetes DNS、服务健康和指标统计。

**Nacos** **基本架构及概念**

![image-20250311192842206](5day-png/35Nacos 基本架构及概念.png)

**相关概念**

- 服务 (Service)

  服务是指一个或一组软件功能（例如特定信息的检索或一组操作的执行），其目的是不同的客户端可以为不同的目的重用（例如通过跨进程的网络调用）。Nacos 支持主流的服务生态，如 Kubernetes Service、gRPC|Dubbo RPC Service 或者 Spring Cloud RESTful service。

- 服务注册中心 (Service Registry)

  服务注册中心，它是服务，其实例及元数据的数据库。服务实例在启动时注册到服务注册表，并在关闭时注销。服务和路由器的客户端查询服务注册表以查找服务的可用实例。服务注册中心可能会调用服务实例的健康检查 API 来验证它是否能够处理请求。

- 服务元数据 (Service Metadata)

  服务元数据是指包括服务端点(endpoints)、服务标签、服务版本号、服务实例权重、路由规则、安全策略等描述服务的数据。

- 服务提供方 (Service Provider)

  是指提供可复用和可调用服务的应用方。

- 服务消费方 (Service Consumer)

  是指会发起对某个服务调用的应用方。

- 配置 (Configuration)

  在系统开发过程中通常会将一些需要变更的参数、变量等从代码中分离出来独立管理，以独立的配置文件的形式存在。目的是让静态的系统工件或者交付物（如 WAR，JAR 包等）更好地和实际的物理运行环境进行适配。配置管理一般包含在系统部署的过程中，由系统管理员或者运维人员完成这个步骤。配置变更是调整系统运行时的行为的有效手段之一。

- 配置管理 (Configuration Management)

  在数据中心中，系统中所有配置的编辑、存储、分发、变更管理、历史版本管理、变更审计等所有与配置相关的活动统称为配置管理。

- 名字服务 (Naming Service)

  提供分布式系统中所有对象(Object)、实体(Entity)的“名字”到关联的元数据之间的映射管理服务，例如 ServiceName -> Endpoints Info, Distributed Lock Name -> Lock Owner/Status Info, DNS Domain Name -> IP List, 服务发现和 DNS 就是名字服务的2大场景。

- 配置服务 (Configuration Service)

  在服务或者应用运行过程中，提供动态配置或者元数据以及配置管理的服务提供者。

**Nacos** **逻辑架构及其组件介绍**

![image-20250311193018038](5day-png/35Nacos 基本架构及概念1.png)

- 服务管理：实现服务CRUD，域名CRUD，服务健康状态检查，服务权重管理等功能
- 配置管理：实现配置管CRUD，版本管理，灰度管理，监听管理，推送轨迹，聚合数据等功能
- 元数据管理：提供元数据CURD 和打标能力
- 插件机制：实现三个模块可分可合能力，实现扩展点SPI机制
- 事件机制：实现异步化事件通知，sdk数据变化异步通知等逻辑
- 日志模块：管理日志分类，日志级别，日志可移植性（尤其避免冲突），日志格式，异常码+帮助文档
- 回调机制：sdk通知数据，通过统一的模式回调用户处理。接口和数据结构需要具备可扩展性
- 寻址模式：解决ip，域名，nameserver、广播等多种寻址模式，需要可扩展
- 推送通道：解决server与存储、server间、server与sdk间推送性能问题
- 容量管理：管理每个租户，分组下的容量，防止存储被写爆，影响服务可用性
- 流量管理：按照租户，分组等多个维度对请求频率，长链接个数，报文大小，请求流控进行控制
- 缓存机制：容灾目录，本地缓存，server缓存机制。容灾目录使用需要工具
- 启动模式：按照单机模式，配置模式，服务模式，dns模式，或者all模式，启动不同的程序+UI
- 一致性协议：解决不同数据，不同一致性要求情况下，不同一致性机制
- 存储模块：解决数据持久化、非持久化存储，解决数据分片问题
- Nameserver：解决namespace到clusterid的路由问题，解决用户环境与nacos物理环境映射问题
- CMDB：解决元数据存储，与三方cmdb系统对接问题，解决应用，人，资源关系
- Metrics：暴露标准metrics数据，方便与三方监控系统打通
- Trace：暴露标准trace，方便与SLA系统打通，日志白平化，推送轨迹等能力，并且可以和计量计费系统打通
- 接入管理：相当于阿里云开通服务，分配身份、容量、权限过程
- 用户管理：解决用户管理，登录，sso等问题
- 权限管理：解决身份识别，访问控制，角色管理等问题
- 审计系统：扩展接口方便与不同公司审计系统打通
- 通知系统：核心数据变更，或者操作，方便通过SMS系统打通，通知到对应人数据变更
- OpenAPI：暴露标准Rest风格HTTP接口，简单易用，方便多语言集成
- Console：易用控制台，做服务管理、配置管理等操作
- SDK：多语言sdk
- Agent：dns-f类似模式，或者与mesh等方案集成
- CLI：命令行对产品进行轻量化管理，像git一样好用

**Nacos** **注册中心工作机制**

![image-20250311193059111](5day-png/35Nacos 注册中心工作机制.png)

随着应用架构向微服务架构迁移,服务数量的增加和动态部署、动态扩展的特性,使得服务地址和端口在运行时随时可变

需要一个组件管理当前注册到服务注册与发现中心的微服务实例元数据信息,包括服务实例的服务名、IP 地址、端口号、服务描述和服务状态等,与注册到服务发现与注册中心的微服务实例维持心跳定期检查注册表中的服务实例是否在线，并剔除无效服务实例信息,并提供服务发现能力，为服务调用方提供服务提供方的服务实例元数据

服务提供者分类

- 临时实例: 服务提供者定期周期主动向nacos 发送心跳监测报告,如果一段时间后,nacos无法收到心跳,则删除此实例
- 非临时实例(永久实例): nacos 主动定时监测此类实例,如果提供者实例异常,则并不会删除只是标记此实例异常,等待此实例恢复

服务消费者

- 消费者定时向nacos 注册中心PULL拉取提供者信息,并加以缓存
- 如果提供者有变化,nacos会主动向消费者PUSH推送消息通知,Eureka不支持主动PUSH

集群模式

- Nacos 默认使用 AP ( Availability和Partiton tolerance)模式,存在非临时实例时,会采用CP(Consistency和Partiton tolerance)模式
- Eureka 采用 AP( Availability和Partiton tolerance)模式

集群数据一致性实现

- CP模式基于强一致性协议 Raft
- AP模式基于 阿里的Distro(基于Gossip和Eureka协议优化而来)最终一致性的AP 分布式协议

### 临时实例和非临时实例

在 **Nacos（Naming and Configuration Service）** 中，实例分为 **临时实例（Ephemeral Instance）** 和 **非临时实例（Non-Ephemeral Instance）**。它们的主要区别如下：

#### 1. **临时实例（Ephemeral Instance）**

##### **特性**

- **默认情况下，Nacos 注册的实例是临时实例。**
- 依赖于 **心跳检测** 机制，如果服务实例在一定时间内（默认 5 秒）没有发送心跳，Nacos 认为该实例不可用，并从服务列表中移除。
- **适用于短时存在的实例**，如 **微服务、Kubernetes Pod、自动伸缩实例** 等。

##### **配置**

在 Nacos 客户端注册服务时，临时实例的默认值为 `true`：

```java
namingService.registerInstance("my-service", "127.0.0.1", 8080);
```

或者：

```java
Instance instance = new Instance();
instance.setIp("127.0.0.1");
instance.setPort(8080);
instance.setEphemeral(true);  // 设置为临时实例（默认值）
namingService.registerInstance("my-service", instance);
```

##### **适用场景**

- **微服务架构**：实例数量动态变化，如 Kubernetes 自动扩缩容、Spring Cloud 服务等。
- **短生命周期的应用**：比如**计算节点、任务调度**等。

------

#### 2. **非临时实例（Non-Ephemeral Instance）**

##### **特性**

- 非临时实例**不会因心跳丢失而被自动移除**，即使服务崩溃或宕机，Nacos 仍然认为它是存在的，直到被**手动下线**。
- 适用于 **长生命周期的服务**，例如 **数据库、缓存服务器、消息队列等**。

##### **配置**

在注册时，显式设置 `ephemeral=false`：

```java
Instance instance = new Instance();
instance.setIp("127.0.0.1");
instance.setPort(8080);
instance.setEphemeral(false);  // 设置为非临时实例
namingService.registerInstance("my-service", instance);
```

如果是 **Spring Cloud Alibaba Nacos** 版本，可在 `application.properties` 配置：

```
spring.cloud.nacos.discovery.ephemeral=false
```

##### **适用场景**

- **有状态服务（Stateful Services）**：如数据库、缓存服务器、消息队列等，不希望因短暂网络波动导致服务下线。
- **人工维护的服务**：运维人员希望手动管理实例状态，而不是依赖 Nacos 的心跳机制。

------

#### 3. **对比总结**

| 类型           | 适用场景                         | 心跳机制 | 失联后自动剔除           | 需要手动注销 |
| -------------- | -------------------------------- | -------- | ------------------------ | ------------ |
| **临时实例**   | 微服务、K8s 容器、短生命周期实例 | 需要     | 是（5 秒无心跳自动剔除） | 否           |
| **非临时实例** | 数据库、缓存、人工维护的服务     | 不需要   | 否（必须手动删除）       | 是           |

------

##### 4. **临时实例 vs 非临时实例 选用建议**

- **如果服务是短时运行、弹性扩缩容的，建议使用** **临时实例**（默认）。
- **如果服务需要持久化、不会自动删除，建议使用** **非临时实例**。

例如：

- **Spring Cloud 微服务** → `ephemeral=true`（默认）
- **MySQL、Redis 等长时间运行的服务** → `ephemeral=false`

##### **如何注销非临时实例？**

如果使用了 **非临时实例**，在实例关闭时需要手动注销：

```
namingService.deregisterInstance("my-service", "127.0.0.1", 8080);
```

否则，Nacos 会一直认为这个实例是存在的，即使它已经宕机。

------

#### 5. **总结**

- **临时实例**：默认类型，基于心跳，失联会被自动剔除，适用于短生命周期的微服务。
- **非临时实例**：不会被自动剔除，需要手动管理，适用于持久化服务。

如果你的 **服务可能随时会崩溃或重启**，且 **希望它在崩溃时被自动剔除**，就用 **临时实例（默认）**。
 如果你的 **服务需要长期存在**，即使宕机了 **也不希望它自动被 Nacos 删除**，就用 **非临时实例**。



### Nacos部署

```
https://nacos.io/docs/v2.4/manual/admin/deployment/deployment-overview/?spm=5238cd80.2ef5001f.0.0.3f613b7cmDsk8X
```

#### Nacos部署手册

> Nacos定义为一个IDC内部应用组件，并非面向公网环境的产品，建议在内部隔离网络环境中部署，强烈不建议部署在公共网络环境。
>
> 以下文档中提及的VIP，网卡等所有网络相关概念均处于内部网络环境。

##### 1. Nacos部署架构

Nacos2.X版本新增了gRPC的通信方式，因此需要增加2个端口。新增端口是在配置的主端口(server.port，默认8848)基础上，进行一定偏移量自动生成，具体端口内容及偏移量请参考如下：

| 端口 | 与主端口的偏移量 | 描述                                                       |
| ---- | ---------------- | ---------------------------------------------------------- |
| 9848 | 1000             | 客户端gRPC请求服务端端口，用于客户端向服务端发起连接和请求 |
| 9849 | 1001             | 服务端gRPC请求服务端端口，用于服务间同步等                 |
| 7848 | -1000            | Jraft请求服务端端口，用于处理服务端间的Raft相关请求        |

> **使用VIP/nginx请求时，需要配置成TCP转发，不能配置http2转发，否则连接会被nginx断开。**
>
> **对外暴露端口时，只需要暴露主端口（默认8848）和gRPC端口（默认9848），其他端口为服务端之间的通信端口，请勿暴露其他端口，同时建议所有端口均不暴露在公网下。**

![nacos2_port_exposure.png](https://nacos.io/img/doc/manual/admin/deployment/deploy-port-export.svg)

客户端拥有相同的计算逻辑，用户如同1.X的使用方式，配置主端口(默认8848)，通过相同的偏移量，计算对应gRPC端口(默认9848)。

因此如果客户端和服务端之前存在端口转发，或防火墙时，需要对端口转发配置和防火墙配置做相应的调整。

##### 2. Nacos支持三种部署模式

- 单机模式 - 又称单例模式，主要用于测试和单机试用。
- 集群模式 - 主要用于生产环境，确保高可用。
- 多集群模式（TODO） - 用于多数据中心场景。

![Nacos部署模式图](https://nacos.io/img/doc/overview/deploy-structure.svg)

###### 2.1. 单机模式

单机模式又称`单例模式`, 拥有所有Nacos的功能及特性，具有极易部署、快速启动等优点。但无法与其他节点组成集群，无法在节点或网络故障时提供高可用能力。单机模式同样可以使用内置Derby数据库（默认）和外置数据库进行存储。

单机模式主要适合于工程师于本地搭建或于测试环境中搭建Nacos环境，主要用于开发调试及测试使用；也能够兼顾部分对稳定性和可用性要求不高的业务场景。

单机模式的部署参考文档: [单机模式部署](https://nacos.io/docs/v2.4/manual/admin/deployment/deployment-standalone/)

###### 2.2. 集群模式

集群模式通过自研一致性协议Distro以及Raft协议，将多个Nacos节点构建成了高可用的Nacos集群。数据将在集群中各个节点进行同步，保证数据的一致性。集群模式具有高可用、高扩展、高并发等优点，确保在故障发生时不影响业务的运行。集群模式**默认**采用外置数据库进行存储，但也可以通过内置数据库进行存储。

该模式主要适合于生产环境，也是社区最为推荐的部署模式。

集群模式的部署参考文档: [集群模式部署](https://nacos.io/docs/v2.4/manual/admin/deployment/deployment-cluster/)

###### 2.3. 多集群模式（TODO）

Nacos支持NameServer路由请求模式，通过它您可以设计一个有用的映射规则来控制请求转发到相应的集群，在映射规则中您可以按命名空间或租户等分片请求…

##### 3. 多网卡IP选择

当本地环境比较复杂的时候，Nacos服务在启动的时候需要选择运行时使用的IP或者网卡。Nacos从多网卡获取IP参考Spring Cloud设计，通过nacos.inetutils参数，可以指定Nacos使用的网卡和IP地址。目前支持的配置参数有:

- ip-address参数可以直接设置nacos的ip

```
nacos.inetutils.ip-address=10.11.105.155
```

- use-only-site-local-interfaces参数可以让nacos使用局域网ip，这个在nacos部署的机器有多网卡时很有用，可以让nacos选择局域网网卡

```
nacos.inetutils.use-only-site-local-interfaces=true
```

- ignored-interfaces支持网卡数组，可以让nacos忽略多个网卡

```
nacos.inetutils.ignored-interfaces[0]=eth0
nacos.inetutils.ignored-interfaces[1]=eth1
```

- preferred-networks参数可以让nacos优先选择匹配的ip，支持正则匹配和前缀匹配

```
nacos.inetutils.preferred-networks[0]=30.5.124.
nacos.inetutils.preferred-networks[0]=30.5.124.(25[0-5]|2[0-4]\\d|((1d{2})|([1-9]?\\d))),30.5.124.(25[0-5]|2[0-4]\\d|((1d{2})|([1-9]?\\d))
```

#### 单机Nacos基于Derby数据库部署

二进制安装

```
https://github.com/alibaba/nacos/releases/download/2.5.0/nacos-server-2.5.0.tar.gz
https://download.nacos.io/nacos-server/nacos-server-2.5.0.zip?spm=5238cd80.2ef5001f.0.0.3f613b7cmDsk8X&file=nacos-server-2.5.0.zip
```

```bash
#安装java环境
[root@ubuntu2404 ~]#apt update && apt install -y openjdk-21-jdk
```

```bash
[root@ubuntu2404 ~]#ls
nacos-server-2.5.0.tar.gz  nacos-server-2.5.0.zip
[root@ubuntu2404 ~]#unzip nacos-server-2.5.0.zip -d /usr/local/
[root@ubuntu2404 ~]#cd /usr/local/
[root@ubuntu2404 local]#cd nacos/
[root@ubuntu2404 nacos]#ll bin/
total 28
drwxr-xr-x 2 root root 4096 Jan 21 10:56 ./
drwxr-xr-x 5 root root 4096 Jan 21 10:56 ../
-rwxr-xr-x 1 root root 1189 Nov 29 10:57 shutdown.cmd*
-rwxr-xr-x 1 root root  875 Nov 29 10:57 shutdown.sh*	#停止脚本
-rwxr-xr-x 1 root root 3560 Jan 21 10:54 startup.cmd*
-rwxr-xr-x 1 root root 5713 Jan 21 10:54 startup.sh*	#启动脚本

[root@ubuntu2404 nacos]#vim conf/application.properties
#启动Prometheus监控
management.endpoints.web.exposure.include=prometheus,health

#启动nacos
[root@ubuntu2404 nacos]#bash ./bin/startup.sh -m standalone
```

```bash
#修改配置，可选
[root@ubuntu2204 ~]#vi /usr/local/nacos/conf/application.properties
#修改默认的访问URL路径
#server.servlet.contextPath=/nacos
server.servlet.contextPath=/
#***********Expose prometheus and health **************************# 
#取消下面行注释开启prometheus监控，指标路径：http://127.0.0.1:8848/nacos/actuator/prometheus
management.endpoints.web.exposure.include=prometheus,health
```



```bash
访问
http://10.0.0.200:8848/nacos

prometheus监听地址
http://10.0.0.200:8848/nacos/actuator/prometheus
```



#### 脚本部署单机Nacos基于Derby数据库

```bash
[root@ubuntu2404 ~]#cat install_nacos_single_node.sh 
#!/bin/bash
#
#********************************************************************
#Author:            wangxiaochun
#QQ:                29308620
#Date:              2023-06-05
#FileName:          install_nacos_single_node.sh
#URL:               http://www.wangxiaochun.com
#Description:       The test script
#Copyright (C):     2023 All rights reserved
#********************************************************************

#支持在线和离线安装,建议离线安装,在线可能下载很慢

NACOS_VERSION=2.5.0
#NACOS_VERSION=2.4.3
#NACOS_VERSION=2.4.0.1
#NACOS_VERSION=2.3.2
#NACOS_VERSION=2.3.0
#NACOS_VERSION=2.2.3

#NACOS_FILE=nacos-server-${NACOS_VERSION}.tar.gz
NACOS_FILE=nacos-server-${NACOS_VERSION}.zip

GITHUB_PROXY=https://mirror.ghproxy.com/

NACOS_URL=https://github.com/alibaba/nacos/releases/download/${NACOS_VERSION}/${NACOS_FILE}

INSTALL_DIR=/usr/local/nacos


HOST=`hostname -I|awk '{print $1}'`

.  /etc/os-release

color () {
    RES_COL=60
    MOVE_TO_COL="echo -en \\033[${RES_COL}G"
    SETCOLOR_SUCCESS="echo -en \\033[1;32m"
    SETCOLOR_FAILURE="echo -en \\033[1;31m"
    SETCOLOR_WARNING="echo -en \\033[1;33m"
    SETCOLOR_NORMAL="echo -en \E[0m"
    echo -n "$1" && $MOVE_TO_COL
    echo -n "["
    if [ $2 = "success" -o $2 = "0" ] ;then
        ${SETCOLOR_SUCCESS}
        echo -n $"  OK  "    
    elif [ $2 = "failure" -o $2 = "1"  ] ;then 
        ${SETCOLOR_FAILURE}
        echo -n $"FAILED"
    else
        ${SETCOLOR_WARNING}
        echo -n $"WARNING"
    fi
    ${SETCOLOR_NORMAL}
    echo -n "]"
    echo 
}


install_jdk() {
    java -version &>/dev/null && { color "JDK 已安装!" 1 ; return;  }
    if command -v yum &>/dev/null ; then
        yum -y install java-1.8.0-openjdk-devel || { color "安装JDK失败!" 1; exit 1; }
    elif command -v apt &>/dev/null ; then
        apt update
            apt install openjdk-21-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
            #apt install openjdk-17-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        #apt install openjdk-11-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        #apt install openjdk-8-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
    else
       color "不支持当前操作系统!" 1
       exit 1
    fi
    java -version && { color "安装 JDK 完成!" 0 ; } || { color "安装JDK失败!" 1; exit 1; } 
}


install_nacos() {
    if [ -f ${NACOS_FILE} ] ;then
        cp ${NACOS_FILE} /usr/local/src/
    else
        wget -P /usr/local/src/ --no-check-certificate ${GITHUB_PROXY}$NACOS_URL || { color  "下载失败!" 1 ;exit ; }
    fi
    #tar xf /usr/local/src/${NACOS_FILE} -C /usr/local
    unzip ${NACOS_FILE} -d /usr/local
    id nacos &> /dev/null || useradd -r -s /sbin/nologin nacos
        chown -R nacos:nacos ${INSTALL_DIR}
    echo "PATH=${INSTALL_DIR}/bin:$PATH" >>  /etc/profile
    .  /etc/profile
   
start_nacos () {   
    cat > /lib/systemd/system/nacos.service <<EOF
[Unit]
Description=nacos.service
After=network.target

[Service]
Type=forking
#Environment=${INSTALL_DIR}
ExecStart=${INSTALL_DIR}/bin/startup.sh -m standalone
ExecStop=${INSTALL_DIR}/bin/shutdown.sh
User=nacos
Group=nacos

[Install]
WantedBy=multi-user.target
EOF
    systemctl daemon-reload
    systemctl enable --now  nacos.service
    systemctl is-active nacos.service
    if [ $? -eq 0 ] ;then 
        color "nacos 安装成功!" 0  
        echo "-------------------------------------------------------------------"
        echo -e "请访问链接: \E[32;1mhttp://$HOST:8848/nacos/\E[0m" 
    else 
        color "nacos 安装失败!" 1
        exit 1
    fi   
}
}



install_jdk

install_nacos

start_nacos
```



#### 单机Nacos基于mysql数据库部署

```bash
#安装mysql数据库
[root@ubuntu2404 ~]#apt install mysql-server -y
[root@ubuntu2404 ~]#vim /etc/mysql/mysql.conf.d/mysqld.cnf
#bind-address       = 127.0.0.1
#mysqlx-bind-address    = 127.0.0.1
[root@ubuntu2404 ~]#systemctl restart mysql.service 

#数据库文件
[root@ubuntu2404 nacos]#ls conf/mysql*.sql
conf/mysql-schema.sql
#准备mysql数据库
[root@ubuntu2404 ~]#mysql
mysql> create database nacos;
#需要指定验证插件，不支持默认插件CachingSha2Password
mysql> create user nacos@'10.0.0.%' identified with mysql_native_password by '123456'; 
mysql> grant all on nacos.* to nacos@'10.0.0.%';
#生成表结构
[root@ubuntu2404 nacos]#mysql -unacos -p123456 -h10.0.0.200 nacos < /usr/local/nacos/conf/mysql-schema.sql 
```

```bash
#修改nacos配置文件，连接mysql
[root@ubuntu2404 ~]#vim /usr/local/nacos/conf/application.properties
spring.sql.init.platform=mysql

db.num=1

db.url.0=jdbc:mysql://10.0.0.200:3306/nacos?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useUnicode=true&useSSL=false&serverTimezone=Asia/Shanghai
db.user.0=nacos
db.password.0=123456
```

```bash
#如果启动,先关闭服务，重新启动服务
[root@ubuntu2404 ~]#bash /usr/local/nacos/bin/shutdown.sh 
#启动服务
[root@ubuntu2404 ~]#bash /usr/local/nacos/bin/startup.sh -m standalone
```



### **服务注册&发现和配置管理**

#### 服务注册

```
curl -X POST 'http://127.0.0.1:8848/nacos/v1/ns/instance?serviceName=nacos.naming.serviceName&ip=20.18.7.10&port=8080'
```

#### 服务发现

```
curl -X GET 'http://127.0.0.1:8848/nacos/v1/ns/instance/list?serviceName=nacos.naming.serviceName'
```

#### 发布配置

```
curl -X POST "http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test&content=HelloWorld"
```

#### 获取配置

```
curl -X GET "http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test"
```



### **开启鉴权**

#### 开启鉴权

```
https://nacos.io/docs/latest/manual/admin/auth/
```

```bash
#生成toke的值，至少32位，否则无法启动
[root@ubuntu2404 ~]#openssl rand -base64 33
pTjnlks5DQKF9oG16nPNxulcHw3OKNkj3ffoUxh5fWKi

[root@ubuntu2404 ~]#vim /usr/local/nacos/conf/application.properties
#修改配置文件,共四行内容
### The auth system to use, currently only 'nacos' and 'ldap' is supported:
nacos.core.auth.system.type=nacos

### If turn on auth system:
nacos.core.auth.enabled=true

### Turn on/off caching of auth information. By turning on this switch, the update of auth information would have a 15 seconds delay.
nacos.core.auth.caching.enabled=true

### Since 1.4.1, Turn on/off white auth for user-agent: nacos-server, only for upgrade from old version.
nacos.core.auth.enable.userAgentAuthWhite=false

### Since 1.4.1, worked when nacos.core.auth.enabled=true and nacos.core.auth.enable.userAgentAuthWhite=false.
### The two properties is the white list for auth and used by identity the request from other server.
nacos.core.auth.server.identity.key=kang
nacos.core.auth.server.identity.value=kang

### worked when nacos.core.auth.system.type=nacos
### The token expiration in seconds:
nacos.core.auth.plugin.nacos.token.cache.enable=false
nacos.core.auth.plugin.nacos.token.expire.seconds=18000
### The default token (Base64 String):
nacos.core.auth.plugin.nacos.token.secret.key=pTjnlks5DQKF9oG16nPNxulcHw3OKNkj3ffoUxh5fWKi
```

```bash
#重启服务生效
[root@ubuntu2404 ~]#bash /usr/local/nacos/bin/startup.sh -m standalone
```

测试访问第一次登录需要指定nacos的新密码，默认用户名和密码都是nacos

#### 权限控制

##### 创建角色

权限控制——》角色管理——》绑定角色

##### 权限管理

注意：先绑定角色，才能创建权限

权限控制——》权限管理——》添加权限



后续API访问需要经过鉴权后才能访问

```bash
#直接访问失败
[root@ubuntu2404 ~]#curl -X POST "http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test&content=HelloWorld"
{"timestamp":"2025-03-11T21:24:24.657+08:00","status":403,"error":"Forbidden","message":"user not found!","path":"/nacos/v1/cs/configs"}

#登录验证,获取Token
[root@ubuntu2404 ~]#curl -X POST 'http://127.0.0.1:8848/nacos/v1/auth/login' -d 'username=nacos&password=nacos'
{"accessToken":"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJuYWNvcyIsImV4cCI6MTc0MTcxNzUxM30._tsxSyJT19W771dz4uFXD2UKDTh_Lt_DNAJjMCtNl2I","tokenTtl":18000,"globalAdmin":true,"username":"nacos"}

#利用Token访问
[root@ubuntu2404 ~]#TOKEN=eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJuYWNvcyIsImV4cCI6MTc0MTcxNzUxM30._tsxSyJT19W771dz4uFXD2UKDTh_Lt_DNAJjMCtNl2I

#服务注册
[root@ubuntu2404 ~]#curl -X POST "http://127.0.0.1:8848/nacos/v1/ns/instance?accessToken=$TOKEN&serviceName=nacos.wang.serviceName&ip=1.2.3.4&port=8080"
ok


```



### **Nacos** **集群部署**

```
https://nacos.io/zh-cn/docs/cluster-mode-quick-start.html
```

### 集群部署架构图

因此开源的时候推荐用户把所有服务列表放到一个vip下面，然后挂到一个域名下面

[http://ip1](http://ip1/):port/openAPI 直连ip模式，机器挂则需要修改ip才可以使用。

[http://SLB](http://slb/):port/openAPI 挂载SLB模式(内网SLB，不可暴露到公网，以免带来安全风险)，直连SLB即可，下面挂server真实ip，可读性不好。

[http://nacos.com](http://nacos.com/):port/openAPI 域名 + SLB模式(内网SLB，不可暴露到公网，以免带来安全风险)，可读性好，而且换ip方便，推荐模

<img src="5day-png/35Nacos集群部署.png" alt="image-20250311214101246" style="zoom:50%;" />

![image-20250311214259761](5day-png/35Nacos集群部署1.png)

以上三种模式，推荐使用第三种，即用户把所有服务列表放到一个vip下面，然后挂到一个域名下面

**端口情况**

| 端口 | **与主端口的偏移量** | 描述                                                       |
| ---- | -------------------- | ---------------------------------------------------------- |
| 8848 | 0                    | 主端口，客户端、控制台及OpenAPI所使用的HTTP端口            |
| 9848 | 1000                 | 客户端gRPC请求服务端端口，用于客户端向服务端发起连接和请求 |
| 9849 | 1001                 | 服务端gRPC请求服务端端口，用于服务间同步等                 |
| 7848 | -1000                | raft请求服务端端口，用于处理服务端间的Raft相关请求         |

注意：

- 使用VIP/nginx请求时，需要配置成TCP转发，不能配置http2转发，否则连接会被nginx断开
- 9849和7848端口为服务端之间的通信端口，请勿暴露到外部网络环境和客户端测

### 部署nacos集群

<img src="5day-png/35Nacos集群部署2.png" alt="image-20250311214521613" style="zoom:50%;" />

| 服务               | ip         |
| ------------------ | ---------- |
| mysql              | 10.0.0.200 |
| nacos1             | 10.0.0.201 |
| nacos2             | 10.0.0.202 |
| nacos3             | 10.0.0.203 |
| haproxy1 keeplived | 10.0.0.204 |
| haproxy2 keeplived | 10.0.0.205 |
| vip                | 10.0.0.100 |



```bash
#数据库准备
#安装mysql数据库
[root@ubuntu2404 ~]#apt install mysql-server -y
[root@ubuntu2404 ~]#sed -i '/127.0.0.1/s/^/#/' /etc/mysql/mysql.conf.d/mysqld.cnf
[root@ubuntu2404 ~]#systemctl restart mysql.service 

#数据库文件
[root@ubuntu2404 nacos]#ls conf/mysql*.sql
conf/mysql-schema.sql
#准备mysql数据库
[root@ubuntu2404 ~]#mysql
mysql> create database nacos;
#需要指定验证插件，不支持默认插件CachingSha2Password
mysql> create user nacos@'10.0.0.%' identified with mysql_native_password by '123456'; 
mysql> grant all on nacos.* to nacos@'10.0.0.%';
#生成表结构
[root@ubuntu2404 ~]#unzip 
nacos-server-2.5.0.zip
[root@ubuntu2404 ~]#unzip nacos-server-2.5.0.zip 
[root@ubuntu2404 ~]#ls nacos/conf/mysql-schema.sql 
nacos/conf/mysql-schema.sql
[root@ubuntu2404 ~]#mysql -unacos -p123456 -h10.0.0.200 nacos < nacos/conf/mysql-schema.sql
```

```
https://github.com/alibaba/nacos/releases/download/2.5.0/nacos-server-2.5.0.tar.gz
https://download.nacos.io/nacos-server/nacos-server-2.5.0.zip?spm=5238cd80.2ef5001f.0.0.3f613b7cmDsk8X&file=nacos-server-2.5.0.zip
```

```bash
#修改hosts文件
[root@ubuntu2404 ~]#hostnamectl hostname node1
[root@ubuntu2404 ~]#hostnamectl hostname node2
[root@ubuntu2404 ~]#hostnamectl hostname node3
[root@node1 ~]#vim /etc/hosts
10.0.0.201 node1
10.0.0.202 node2
10.0.0.203 node3
[root@node1 ~]#scp /etc/hosts node2:/etc/hosts
[root@node1 ~]#scp /etc/hosts node3:/etc/hosts
```

```bash
#在nacos节点上安装java和nacos
[root@node1 ~]#apt update && apt -y install openjdk-21-jdk
[root@node1 ~]#ls
install_nacos_single_node.sh  nacos-server-2.5.0.zip
[root@node1 ~]scp install_nacos_single_node.sh node2:
[root@node1 ~]scp install_nacos_single_node.sh node3:
[root@node1 ~]#vim install_nacos_single_node.sh 
#!/bin/bash

#支持在线和离线安装,建议离线安装,在线可能下载很慢

NACOS_VERSION=2.5.0
#NACOS_VERSION=2.4.3
#NACOS_VERSION=2.4.0.1
#NACOS_VERSION=2.3.2
#NACOS_VERSION=2.3.0
#NACOS_VERSION=2.2.3

#NACOS_FILE=nacos-server-${NACOS_VERSION}.tar.gz
NACOS_FILE=nacos-server-${NACOS_VERSION}.zip

GITHUB_PROXY=https://mirror.ghproxy.com/

NACOS_URL=https://github.com/alibaba/nacos/releases/download/${NACOS_VERSION}/${NACOS_FILE}

INSTALL_DIR=/usr/local/nacos


HOST=`hostname -I|awk '{print $1}'`

.  /etc/os-release

color () {
    RES_COL=60
    MOVE_TO_COL="echo -en \\033[${RES_COL}G"
    SETCOLOR_SUCCESS="echo -en \\033[1;32m"
    SETCOLOR_FAILURE="echo -en \\033[1;31m"
    SETCOLOR_WARNING="echo -en \\033[1;33m"
    SETCOLOR_NORMAL="echo -en \E[0m"
    echo -n "$1" && $MOVE_TO_COL
    echo -n "["
    if [ $2 = "success" -o $2 = "0" ] ;then
        ${SETCOLOR_SUCCESS}
        echo -n $"  OK  "    
    elif [ $2 = "failure" -o $2 = "1"  ] ;then 
        ${SETCOLOR_FAILURE}
        echo -n $"FAILED"
    else
        ${SETCOLOR_WARNING}
        echo -n $"WARNING"
    fi
    ${SETCOLOR_NORMAL}
    echo -n "]"
    echo 
}


install_jdk() {
    java -version &>/dev/null && { color "JDK 已安装!" 1 ; return;  }
    if command -v yum &>/dev/null ; then
        yum -y install java-1.8.0-openjdk-devel || { color "安装JDK失败!" 1; exit 1; }
    elif command -v apt &>/dev/null ; then
        apt update
            apt install openjdk-21-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
            #apt install openjdk-17-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        #apt install openjdk-11-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        #apt install openjdk-8-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
    else
       color "不支持当前操作系统!" 1
       exit 1
    fi
    java -version && { color "安装 JDK 完成!" 0 ; } || { color "安装JDK失败!" 1; exit 1; } 
}


install_nacos() {
    if [ -f ${NACOS_FILE} ] ;then
        cp ${NACOS_FILE} /usr/local/src/
    else
        wget -P /usr/local/src/ --no-check-certificate ${GITHUB_PROXY}$NACOS_URL || { color  "下载失败!" 1 ;exit ; }
    fi
    #tar xf /usr/local/src/${NACOS_FILE} -C /usr/local
    unzip ${NACOS_FILE} -d /usr/local
    id nacos &> /dev/null || useradd -r -s /sbin/nologin nacos
        chown -R nacos:nacos ${INSTALL_DIR}
    echo "PATH=${INSTALL_DIR}/bin:$PATH" >>  /etc/profile
    .  /etc/profile
   
start_nacos () {   
    cat > /lib/systemd/system/nacos.service <<EOF
[Unit]
Description=nacos.service
After=network.target

[Service]
Type=forking
#Environment=${INSTALL_DIR}
ExecStart=${INSTALL_DIR}/bin/startup.sh -m standalone
ExecStop=${INSTALL_DIR}/bin/shutdown.sh
User=nacos
Group=nacos

[Install]
WantedBy=multi-user.target
EOF
    systemctl daemon-reload
    systemctl enable --now  nacos.service
    systemctl is-active nacos.service
    if [ $? -eq 0 ] ;then 
        color "nacos 安装成功!" 0  
        echo "-------------------------------------------------------------------"
        echo -e "请访问链接: \E[32;1mhttp://$HOST:8848/nacos/\E[0m" 
    else 
        color "nacos 安装失败!" 1
        exit 1
    fi   
}
}



install_jdk

install_nacos

start_nacos

[root@node1 ~]#bash install_nacos_single_node.sh
[root@node2 ~]#bash install_nacos_single_node.sh
[root@node3 ~]#bash install_nacos_single_node.sh
[root@node1 ~]#systemctl stop nacos.service 
[root@node2 ~]#systemctl stop nacos.service 
[root@node3 ~]#systemctl stop nacos.service 
```

```bash
#创建集群文件
[root@node2 ~]#vim /usr/local/nacos/conf/cluster.conf
# ip:port
10.0.0.201:8848
10.0.0.202:8848
10.0.0.203:8848

#所有节点同步配置
[root@node1 ~]#scp /usr/local/nacos/conf/cluster.conf node2:/usr/local/nacos/conf/cluster.conf   
[root@node1 ~]#scp /usr/local/nacos/conf/cluster.conf node3:/usr/local/nacos/conf/cluster.conf
```

**修改** **Nacos** **配置文件** **application.properties**

```bash
#修改配置连接mysql
[root@node1 ~]#vim /usr/local/nacos/conf/application.properties
spring.sql.init.platform=mysql
### Count of DB:
db.num=1

### Connect URL of DB:
db.url.0=jdbc:mysql://10.0.0.200:3306/nacos?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useUnicode=true&useSSL=false&serverTimezone=Asia/Shanghai
db.user.0=nacos
db.password.0=123456

#所有节点同步配置
[root@node1 ~]#scp /usr/local/nacos/conf/application.properties node2:/usr/local/nacos/conf/application.properties
[root@node1 ~]#scp /usr/local/nacos/conf/application.properties node3:/usr/local/nacos/conf/application.properties
```

```bash
#修改配置文件
[root@node3 ~]#vim /lib/systemd/system/nacos.service
[Unit]
Description=nacos.service
After=network.target

[Service]
Type=forking
#Environment=/usr/local/nacos
ExecStart=/usr/local/nacos/bin/startup.sh
ExecStop=/usr/local/nacos/bin/shutdown.sh
User=nacos
Group=nacos

[Install]
WantedBy=multi-user.target

[root@node1 ~]#scp /lib/systemd/system/nacos.service node2:/lib/systemd/system/nacos.service
[root@node1 ~]#scp /lib/systemd/system/nacos.service node3:/lib/systemd/system/nacos.service

#启动nacos服务
[root@node1 ~]#systemctl daemon-reload
[root@node2 ~]#systemctl daemon-reload
[root@node3 ~]#systemctl daemon-reload
[root@node1 ~]#systemctl stop nacos
[root@node1 ~]#systemctl restart nacos
[root@node2 ~]#systemctl stop nacos
[root@node2 ~]#systemctl restart nacos
[root@node3 ~]#systemctl stop nacos
[root@node3 ~]#systemctl restart nacos
```

**配置** **haproxy** **和** **keepalived** **实现负载均衡和高可用**

```bash
#修改内核参数
[root@ubuntu2204 ~]#echo net.ipv4.ip_nonlocal_bind = 1 >> /etc/sysctl.conf
[root@ubuntu2204 ~]#sysctl -p

[root@ubuntu2204 ~]#hostnamectl hostname master
[root@ubuntu2204 ~]#hostnamectl hostname slave

#安装haproxy和keeplived
[root@master ~]#apt update && apt install -y haproxy keeplived
[root@slave ~]#apt update && apt install -y haproxy keeplived

[root@master ~]#vim /etc/haproxy/haproxy.cfg 
[root@slave ~]#vim /etc/haproxy/haproxy.cfg 
#添加下面几行
# HAProxy 统计页面
listen stats
    mode http
    bind 0.0.0.0:9999
    stats enable
    log global
    stats uri /haproxy-status
    stats auth admin:123456

# Nacos 8848 端口代理（默认 HTTP 模式）
listen nacos-8848
    mode http  # 如果 Nacos 使用 HTTP 协议，建议明确写上 mode http
    bind 10.0.0.100:8848
    server nacos01 10.0.0.201:8848 check
    server nacos02 10.0.0.202:8848 check
    server nacos03 10.0.0.203:8848 check

# Nacos 9848 端口代理（TCP 模式）
listen nacos-9848
    mode tcp  # Nacos 9848 端口一般用于 gRPC 或 Raft 通信，需要 TCP 模式
    bind 10.0.0.100:9848
    server nacos01 10.0.0.201:9848 check
    server nacos02 10.0.0.202:9848 check
    server nacos03 10.0.0.203:9848 check


[root@master ~]#systemctl reload haproxy
[root@slave ~]#systemctl reload haproxy


#浏览器访问haproxy的管理页,用户名/密码:admin/123456
http://10.0.0.204:9999/haproxy-status
http://10.0.0.205:9999/haproxy-status


#在两台服务器上安装配置keepalived实现高可用
#/usr/share/doc/keepalived/samples/keepalived.conf.sample
#/usr/share/doc/keepalived/samples/keepalived.conf.vrrp.localcheck
#参考上面文件修改配置文件

#在master上配置keepalived
[root@master ~]#vim /etc/keepalived/keepalived.conf
! Configuration File for Keepalived

global_defs {
    router_id ka1  # 另一台主机应配置为 ka2
}

# 检查 HAProxy 是否运行
vrrp_script chk_haproxy {
    script "killall -0 haproxy"  # 更轻量的方式，比 `pidof` 更快
    interval 1
    weight -30
}

vrrp_instance VI_1 {
    interface eth0
    virtual_router_id 66  # 确保两台主机的 virtual_router_id 一致
    state MASTER  # 另一台主机应配置为 BACKUP
    priority 100   # 另一台主机应配置为 80
    advert_int 1

    authentication {
        auth_type PASS
        auth_pass 123456  # 需确保两台主机的认证信息一致
    }

    virtual_ipaddress {
        10.0.0.100/24 dev eth0 label eth0:1
    }

    track_script {
        chk_haproxy
    }
}

#在slave上配置keepalived
[root@slave ~]#vim /etc/keepalived/keepalived.conf
! Configuration File for Keepalived

global_defs {
    router_id ka2  # 另一台主机应配置为 ka1
}

vrrp_instance VI_1 {
    interface eth0
    virtual_router_id 66  # 确保两台主机的 virtual_router_id 一致
    state BACKUP  # 另一台主机应配置为 MASTER
    priority 80   # 另一台主机应配置为 100
    advert_int 1

    authentication {
        auth_type PASS
        auth_pass 123456  # 需确保两台主机的认证信息一致
    }

    virtual_ipaddress {
        10.0.0.100/24 dev eth0 label eth0:1
    }

    track_script {
        chk_haproxy
    }
}

[root@master ~]#systemctl restart keepalived.service
[root@slave ~]#systemctl restart keepalived.service

[root@master ~]#hostname -I
10.0.0.204 10.0.0.100 
[root@slave ~]#hostname -I
10.0.0.205 

#浏览器访问haproxy的管理页,用户名/密码:admin/123456
http://10.0.0.100:9999/haproxy-status
```



**访问集群创建配置**

```bash
[root@ubuntu2404 ~]#curl -X POST "http://10.0.0.100:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test&content=HelloWorld"
true
[root@ubuntu2404 ~]#curl -X GET "http://10.0.0.100:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test"
HelloWorld
```

**Web** **页面访问**

```
http://10.0.0.100:8848/nacos
```

### **Nacos** **案例**

#### **Nacos** **项目服务注册和配置管理**

利用 Nacos 实现 JAVA 应用消费者和提供者调用

<img src="5day-png/35nacos案例.png" alt="image-20250312190056864" style="zoom:50%;" />

| ip              | 备注          |
| --------------- | ------------- |
| 10.0.0.200:8848 | Nacos 服务器  |
| 10.0.0.201:8001 | Provider 应用 |
| 10.0.0.202:8002 | Consumer 应用 |

**安装** **Nacos**

```bash
#docker 启动 nacos
#安装docker
[root@ubuntu2404 ~]#apt update && apt install docker.io
[root@ubuntu2404 ~]#docker run --name nacos -e MODE=standalone -p 8848:8848 -p 9848:9848 -d --restart always registry.cn-beijing.aliyuncs.com/wangxiaochun/nacos-server:v2.4.3-slim
[root@ubuntu2404 ~]#docker run --name nacos -e MODE=standalone -p 8848:8848 -p 9848:9848 -d --restart always registry.cn-beijing.aliyuncs.com/wangxiaochun/nacos-server:v2.2.3-slim


```

**运行服务提供者的JAVA应用**

10.0.0.201

```bash
#Nacos服务主机的名称解析
[root@ubuntu2404 ~]#vim /etc/hosts
10.0.0.200 nacos.wang.org

#安装JDK
[root@ubuntu2404 ~]#apt update && apt install -y openjdk-21-jdk
[root@ubuntu2404 ~]#ls
nacos-provider-1.0-SNAPSHOT.jar
[root@ubuntu2404 ~]#java -jar nacos-provider-1.0-SNAPSHOT.jar 
```



**运行服务消费者的JAVA应用**

10.0.0.202

```bash
#Nacos服务主机的名称解析
[root@ubuntu2404 ~]#vim /etc/hosts
10.0.0.200 nacos.wang.org

#安装JDK
[root@ubuntu2404 ~]#apt update && apt install -y openjdk-21-jdk
[root@ubuntu2404 ~]#ls 
nacos-provider-1.0-SNAPSHOT.jar
[root@ubuntu2404 ~]#java -jar nacos-consumer-1.0-SNAPSHOT.jar 
```

**在nacos上可以看到服务已经注册成功**



#### **Nacos** **项目服务注册和配置管理**

```bash
#docker 启动 nacos redis
#安装docker
[root@ubuntu2404 ~]#apt update && apt install docker.io
[root@ubuntu2404 ~]#docker run --name nacos -e MODE=standalone -p 8848:8848 -p 9848:9848 -d --restart always registry.cn-beijing.aliyuncs.com/wangxiaochun/nacos-server:v2.4.3-slim
[root@ubuntu2404 ~]#docker run --name nacos -e MODE=standalone -p 8848:8848 -p 9848:9848 -d --restart always registry.cn-beijing.aliyuncs.com/wangxiaochun/nacos-server:v2.2.3-slim
[root@ubuntu2404 ~]#docker run -d --name redis -p 6379:6379 registry.cn-beijing.aliyuncs.com/wangxiaochun/redis:7.2.5 --requirepass 123456
```





## Sentinel

#### Sentinel介绍

随着微服务的流行，服务和服务之间的调用导致服务的稳定性问题变得越来越重要

雪崩问题: 微服务调用链路中的某个服务故障，引起整个链路中的所有微服务都不可用，即雪崩。

解决雪崩问题的常见方式有四种:

- 超时处理:设定超时时间，请求超过一定时间没有响应就返回错误信息，不会无休止等待
- 流量控制:限制业务访问的QPS,避免服务因流量的突增而故障。
- 熔断降级:由断路器统计业务执行的异常比例，如果超出阈值则会熔断该业务,拦截访问该业务的一切请求。
- 舱壁模式:限定每个业务能使用的线程数，避免耗尽整个tomcat的资源， 因此也叫线程隔离。

```
https://github.com/alibaba/Sentinel/wiki/%E5%9C%A8%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%AD%E4%BD%BF%E7%94%A8-Sentinel
```



##### 同类组件功能对比

|                   | Sentinel                                               | Hystrix                 | resilience4j                     |
| ----------------- | ------------------------------------------------------ | ----------------------- | -------------------------------- |
| 隔离策略          | 信号量隔离（并发控制）                                 | 线程池隔离/信号量隔离   | 信号量隔离                       |
| 熔断降级策略      | 基于慢调用比例、异常比例、异常数                       | 基于异常比例            | 基于异常比例、响应时间           |
| 实时统计实现      | 滑动窗口（LeapArray）                                  | 滑动窗口（基于 RxJava） | Ring Bit Buffer                  |
| 动态规则配置      | 支持近十种动态数据源                                   | 支持多种数据源          | 有限支持                         |
| 扩展性            | 多个扩展点                                             | 插件的形式              | 接口的形式                       |
| 基于注解的支持    | 支持                                                   | 支持                    | 支持                             |
| 单机限流          | 基于 QPS，支持基于调用关系的限流                       | 有限的支持              | Rate Limiter                     |
| 集群流控          | 支持                                                   | 不支持                  | 不支持                           |
| 流量整形          | 支持预热模式与匀速排队控制效果                         | 不支持                  | 简单的 Rate Limiter 模式         |
| 系统自适应保护    | 支持                                                   | 不支持                  | 不支持                           |
| 热点识别/防护     | 支持                                                   | 不支持                  | 不支持                           |
| 多语言支持        | Java/Go/C++                                            | Java                    | Java                             |
| Service Mesh 支持 | 支持 Envoy/Istio                                       | 不支持                  | 不支持                           |
| 控制台            | 提供开箱即用的控制台，可配置规则、实时监控、机器发现等 | 简单的监控查看          | 不提供控制台，可对接其它监控系统 |



```
https://sentinelguard.io/zh-cn/
https://github.com/alibaba/spring-cloud-alibaba/wiki/Sentinel
https://github.com/alibaba/Sentinel/
https://github.com/sentinel-group
https://github.com/alibaba/sentinel-golang
```



#### Sentinel 特征

- **丰富的应用场景**：Sentinel 在阿里巴巴内部被广泛应用，几乎覆盖了近 10 年双十一的所有核心场景，比如需要限制突发流量以达到系统容量要求的秒杀、消息削峰填谷、针对下游不可靠服务的熔断、集群流控等。
- **实时监控**：Sentinel 也提供了实时监控能力，可以实时看到单台机器的运行时信息，以及 500 个节点以下集群的聚合运行时信息。
- **广泛的开源生态系统**：Sentinel 提供与常用框架和库（如 Spring Cloud、gRPC、Apache Dubbo 和 Quarkus）的开箱即用集成。您只需将适配器依赖项添加到您的服务中即可轻松使用 Sentinel。
- **多语言支持：Sentinel 为 Java、** [Go](https://github.com/alibaba/sentinel-golang)、[C++](https://github.com/alibaba/sentinel-cpp)和[Rust](https://github.com/sentinel-group/sentinel-rust)提供了原生支持。
- **多种SPI扩展**：Sentinel提供了简单易用的SPI扩展接口，可以让你快速定制自己的逻辑，例如自定义规则管理，适配数据源等等。

[![生态系统景观](https://github.com/alibaba/Sentinel/raw/1.8/doc/image/sentinel-opensource-eco-landscape-en.png)](https://github.com/alibaba/Sentinel/blob/1.8/doc/image/sentinel-opensource-eco-landscape-en.png)



#### Sentinel 基本概念

##### 资源

资源是 Sentinel 的关键概念。它可以是 Java 应用程序中的任何内容，例如，由应用程序提供的服务，或由应用程序调用的其它应用提供的服务，甚至可以是一段代码。在接下来的文档中，我们都会用资源来描述代码块。

只要通过 Sentinel API 定义的代码，就是资源，能够被 Sentinel 保护起来。大部分情况下，可以使用方法签名，URL，甚至服务名称作为资源名来标示资源。

##### 规则

围绕资源的实时状态设定的规则，可以包括流量控制规则、熔断降级规则以及系统保护规则。所有规则可以动态实时调整。

随着分布式系统的普及，服务间的可靠性变得比以往更加重要。Sentinel 以“流”为切入点，从**流量控制**、**流量整形**、 **并发限制**、**熔断**、**系统自适应过载保护**等多个方面进行研究，为微服务提供可靠性和弹性保障。



### Sentinel功能

[![哨兵特征](https://github.com/alibaba/Sentinel/raw/1.8/doc/image/sentinel-features-overview-en.png)](https://github.com/alibaba/Sentinel/blob/1.8/doc/image/sentinel-features-overview-en.png)

![image-20250312195915490](5day-png/35sentinel功能.png)

#### 流量控制

流量控制在网络传输中是一个常用的概念，它用于调整网络包的发送数据。然而，从系统稳定性角度考虑，在处理请求的速度上，也有非常多的讲究。任意时间到来的请求往往是随机不可控的，而系统的处理能力是有限的。我们需要根据系统的处理能力对流量进行控制。Sentinel 作为一个调配器，可以根据需要把随机的请求调整成合适的形状，如下图所示：

![arch](https://sentinelguard.io/docs/zh-cn/img/sentinel-flow-overview.jpg)

流量控制有以下几个角度:

- 资源的调用关系，例如资源的调用链路，资源和资源之间的关系；
- 运行指标，例如 QPS、线程池、系统负载等；
- 控制的效果，例如直接限流、冷启动、排队等。

Sentinel 的设计理念是让您自由选择控制的角度，并进行灵活组合，从而达到想要的效果。

#### 熔断降级

##### 什么是熔断降级

除了流量控制以外，降低调用链路中的不稳定资源也是 Sentinel 的使命之一。由于调用关系的复杂性，如果调用链路中的某个资源出现了不稳定，最终会导致请求发生堆积。这个问题和 [Hystrix](https://github.com/Netflix/Hystrix/wiki#what-problem-does-hystrix-solve) 里面描述的问题是一样的。

![image](https://user-images.githubusercontent.com/9434884/62410811-cd871680-b61d-11e9-9df7-3ee41c618644.png)

Sentinel 和 Hystrix 的原则是一致的: 当调用链路中某个资源出现不稳定，例如，表现为 timeout，异常比例升高的时候，则对这个资源的调用进行限制，并让请求快速失败，避免影响到其它的资源，最终产生雪崩的效果。

##### 熔断降级设计理念

在限制的手段上，Sentinel 和 Hystrix 采取了完全不一样的方法。

Hystrix 通过[线程池](https://github.com/Netflix/Hystrix/wiki/How-it-Works#benefits-of-thread-pools)的方式，来对依赖(在我们的概念中对应资源)进行了隔离。这样做的好处是资源和资源之间做到了最彻底的隔离。缺点是除了增加了线程切换的成本，还需要预先给各个资源做线程池大小的分配。

Sentinel 对这个问题采取了两种手段:

- 通过并发线程数进行限制

和资源池隔离的方法不同，Sentinel 通过限制资源并发线程的数量，来减少不稳定资源对其它资源的影响。这样不但没有线程切换的损耗，也不需要您预先分配线程池的大小。当某个资源出现不稳定的情况下，例如响应时间变长，对资源的直接影响就是会造成线程数的逐步堆积。当线程数在特定资源上堆积到一定的数量之后，对该资源的新请求就会被拒绝。堆积的线程完成任务后才开始继续接收请求。

- 通过响应时间对资源进行降级

除了对并发线程数进行控制以外，Sentinel 还可以通过响应时间来快速降级不稳定的资源。当依赖的资源出现响应时间过长后，所有对该资源的访问都会被直接拒绝，直到过了指定的时间窗口之后才重新恢复。

#### 系统负载保护

Sentinel 同时提供[系统维度的自适应保护能力](https://sentinelguard.io/zh-cn/docs/system-adaptive-protection.html)。防止雪崩，是系统防护中重要的一环。当系统负载较高的时候，如果还持续让请求进入，可能会导致系统崩溃，无法响应。在集群环境下，网络负载均衡会把本应这台机器承载的流量转发到其它的机器上去。如果这个时候其它的机器也处在一个边缘状态的时候，这个增加的流量就会导致这台机器也崩溃，最后导致整个集群不可用。

针对这个情况，Sentinel 提供了对应的保护机制，让系统的入口流量和系统的负载达到一个平衡，保证系统在能力范围之内处理最多的请求。



### Sentinel 工作主流程

在 Sentinel 里面，所有的资源都对应一个资源名称以及一个 Entry。Entry 可以通过对主流框架的适配自动创建，也可以通过注解的方式或调用 API 显式创建；每一个 Entry 创建的时候，同时也会创建一系列功能插槽（slot chain）。这些插槽有不同的职责，例如:

- `NodeSelectorSlot` 负责收集资源的路径，并将这些资源的调用路径，以树状结构存储起来，用于根据调用路径来限流降级；
- `ClusterBuilderSlot` 则用于存储资源的统计信息以及调用者信息，例如该资源的 RT, QPS, thread count 等等，这些信息将用作为多维度限流，降级的依据；
- `StatisticSlot` 则用于记录、统计不同纬度的 runtime 指标监控信息；
- `FlowSlot` 则用于根据预设的限流规则以及前面 slot 统计的状态，来进行流量控制；
- `AuthoritySlot` 则根据配置的黑白名单和调用来源信息，来做黑白名单控制；
- `DegradeSlot` 则通过统计信息以及预设的规则，来做熔断降级；
- `SystemSlot` 则通过系统的状态，例如 load1 等，来控制总的入口流量；

总体的框架如下:

![arch overview](https://sentinelguard.io/docs/zh-cn/img/sentinel-slot-chain-architecture.png)

Sentinel 将 `ProcessorSlot` 作为 SPI 接口进行扩展（1.7.2 版本以前 `SlotChainBuilder` 作为 SPI），使得 Slot Chain 具备了扩展的能力。您可以自行加入自定义的 slot 并编排 slot 间的顺序，从而可以给 Sentinel 添加自定义的功能。

![Slot Chain SPI](https://user-images.githubusercontent.com/9434884/46783631-93324d00-cd5d-11e8-8ad1-a802bcc8f9c9.png)

**Sentinel** **的构成**

Sentinel 的构成可以分为两个部分:

- 核心库（Java 客户端）：

  开发人员通过JAVA调用，不依赖任何框架/库，能够运行于 Java 8 及以上的版本的运行时环境，同时对 Dubbo / Spring Cloud 等框架也有较好的支持（见 主流框架适配）。

- 控制台（Dashboard）：

  可以由运维部署，Dashboard 主要负责管理推送规则、监控、管理机器信息等。

### Sentinel 控制台

**概述**

Sentinel 提供一个轻量级的开源控制台，它提供机器发现以及健康情况管理、监控（单机和集群），规则管理和推送的功能。这里，我们将会详细讲述如何通过简单的步骤就可以使用这些功能。

接下来，我们将会逐一介绍如何整合 Sentinel 核心库和 Dashboard，让它发挥最大的作用。同时我们也在阿里云上提供企业级的 Sentinel 服务：[AHAS Sentinel 控制台](https://github.com/alibaba/Sentinel/wiki/AHAS-Sentinel-控制台)，您只需要几个简单的步骤，就能最直观地看到控制台如何实现这些功能，并体验多样化的监控及全自动托管的集群流控能力。

Sentinel 控制台包含如下功能:

- **查看机器列表以及健康情况**：收集 Sentinel 客户端发送的心跳包，用于判断机器是否在线。
- **监控 (单机和集群聚合)**：通过 Sentinel 客户端暴露的监控 API，定期拉取并且聚合应用监控信息，最终可以实现秒级的实时监控。
- **规则管理和推送**：统一管理推送规则。
- **鉴权**：生产环境中鉴权非常重要。这里每个开发者需要根据自己的实际情况进行定制。



### **部署和启动控制台(Dashboard)**

```
https://github.com/alibaba/Sentinel/releases
```

**注意**：启动 Sentinel 控制台需要 JDK 版本为 1.8 及以上版本。

Sentinel 控制台是一个标准的 Spring Boot 应用，以 Spring Boot 的方式运行 jar 包即可。

使用如下命令启动控制台：

```bash
#自己也监控自己
java -Dserver.port=8080 -Dcsp.sentinel.dashboard.server=localhost:8080 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard.jar
#只监控别人
java -Dserver.port=8080  -jar sentinel-dashboard.jar
```

其中 `-Dserver.port=8080` 用于指定 Sentinel 控制台端口为 `8080`。

从 Sentinel 1.6.0 起，Sentinel 控制台引入基本的**登录**功能，默认用户名和密码都是 `sentinel`

启动时加入 JVM 参数 `-Dcsp.sentinel.dashboard.server=consoleIp:port` 指定控制台地址和端口。若启动多个应用，则需要通过 `-Dcsp.sentinel.api.port=xxxx` 指定客户端监控 API 的端口（默认是 8719）。

```bash
#安装java
[root@ubuntu2404 ~]#apt update && apt install openjdk-21-jdk -y

[root@ubuntu2404 ~]#ls
sentinel-dashboard-1.8.8.jar
[root@ubuntu2404 ~]#java -Dserver.port=8080 -Dcsp.sentinel.dashboard.server=localhost:8080 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard-1.8.8.jar

```

```bash
#docker 启动
https://hub.docker.com/r/bladex/sentinel-dashboard
#安装docker
[root@ubuntu2404 ~]#apt install -y docker.io
#镜像启动sentinel
[root@ubuntu2404 ~]#docker pull bladex/sentinel-dashboard:1.8.8
[root@ubuntu2404 ~]#docker run --name sentinel -d -p 8080:8858 -p 8719:8719 bladex/sentinel-dashboard:1.8.8
#镜像启动nacos
[root@ubuntu2404 ~]#docker pull nacos/nacos-server:v2.4.3-slim
[root@ubuntu2404 ~]#docker run --name nacos -e MODE=standalone --network host -d --restart always nacos/nacos-server:v2.4.3-slim 


java -Dspring.cloud.nacos.discovery.server-addr=10.0.0.200:8848 -Dspring.cloud.nacos.config.server-addr=10.0.0.200:8848 -Dspring.cloud.sentinel.transport.dashboard=10.0.0.200:8080 -jar sentinel-nacos-demo-0.0.1-SNAPSHOT.jar --server.port=8888
```



## Seata

```
https://github.com/alibaba/spring-cloud-alibaba
https://github.com/apache/incubator-seata
```

### 什么是 Seata？



**适用于微服务**架构的高性能、易用的**分布式事务解决方案**。

### 微服务中的分布式事务问题



想象一下一个传统的单体应用。其业务由 3 个模块组成。它们使用单个本地数据源。

当然，数据的一致性会由本地事务来保证。

[<img src="https://camo.githubusercontent.com/cb83671e90caf2d2ee9370906864c32afe7bb7538ef02eeb4f67c664f854fa3a/68747470733a2f2f696d672e616c6963646e2e636f6d2f696d6765787472612f69332f4f31434e30314654746a79473148347676566831734e595f2121363030303030303030303730352d302d7470732d313130362d3637382e6a7067" alt="单体应用" style="zoom:50%;" />](https://camo.githubusercontent.com/cb83671e90caf2d2ee9370906864c32afe7bb7538ef02eeb4f67c664f854fa3a/68747470733a2f2f696d672e616c6963646e2e636f6d2f696d6765787472612f69332f4f31434e30314654746a79473148347676566831734e595f2121363030303030303030303730352d302d7470732d313130362d3637382e6a7067)

在微服务架构中，情况发生了变化。上面提到的 3 个模块被设计为基于 3 个不同数据源的 3 个服务（[模式：每个服务一个数据库](http://microservices.io/patterns/data/database-per-service.html)）。每个服务内的数据一致性自然由本地事务保证。

**但是整个业务逻辑范围怎么样？**

[<img src="https://camo.githubusercontent.com/de5ff2c5da03b2bf6eaae9ab189dc6eea6b8b924cf6e2a5b8e4b65eb3c850775/68747470733a2f2f696d672e616c6963646e2e636f6d2f696d6765787472612f69312f4f31434e303144586b63336f317465396d6e4a63484f725f2121363030303030303030353932362d302d7470732d313236382d3830342e6a7067" alt="微服务问题" style="zoom:50%;" />](https://camo.githubusercontent.com/de5ff2c5da03b2bf6eaae9ab189dc6eea6b8b924cf6e2a5b8e4b65eb3c850775/68747470733a2f2f696d672e616c6963646e2e636f6d2f696d6765787472612f69312f4f31434e303144586b63336f317465396d6e4a63484f725f2121363030303030303030353932362d302d7470732d313236382d3830342e6a7067)

### Seata 怎么做？



Seata 正是针对上述问题的一个解决方案。

[<img src="https://camo.githubusercontent.com/a61d64e3ac02079397fa56012a421bb86993b377a07615f77d417a45041ece93/68747470733a2f2f696d672e616c6963646e2e636f6d2f696d6765787472612f69312f4f31434e30314668656c6948316b35564849526f6233705f2121363030303030303030343633322d302d7470732d313533342d3930382e6a7067" alt="Seata 解决方案" style="zoom:50%;" />](https://camo.githubusercontent.com/a61d64e3ac02079397fa56012a421bb86993b377a07615f77d417a45041ece93/68747470733a2f2f696d672e616c6963646e2e636f6d2f696d6765787472612f69312f4f31434e30314668656c6948316b35564849526f6233705f2121363030303030303030343633322d302d7470732d313533342d3930382e6a7067)

首先，如何定义**分布式事务**？

我们说一个**分布式事务**是一个**全局事务**，它由一批**分支事务**组成，而一般来说**分支事务**就是**本地事务**。

[<img src="https://camo.githubusercontent.com/94a9a4fdf17a11676303028865859118420a9603089dcf1bf47fed4fb387a103/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f6c61726b2f302f323031382f706e672f31383836322f313534353031353435343937392d61313865313666362d656434312d343466312d396337612d6264383263346435666639392e706e67" alt="全球及分公司" style="zoom:50%;" />](https://camo.githubusercontent.com/94a9a4fdf17a11676303028865859118420a9603089dcf1bf47fed4fb387a103/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f6c61726b2f302f323031382f706e672f31383836322f313534353031353435343937392d61313865313666362d656434312d343466312d396337612d6264383263346435666639392e706e67)

Seata Framework中有三个角色：

- **事务协调器（TC）：**维护全局和分支事务的状态，驱动全局的提交或回滚。
- **事务管理器（TM）：**定义全局事务的范围：开始全局事务、提交或回滚全局事务。
- **资源管理器(RM)：**管理分支事务处理的资源，与TC通信以注册分支事务并报告分支事务的状态，并驱动分支事务的提交或回滚。

[![模型](https://camo.githubusercontent.com/a302427a15e1ed0cd8da485e8bb38aadc673657d8574aad4023d6e71dadd65c3/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f6c61726b2f302f323031382f706e672f31383836322f313534353031333931353238362d34613930663064662d356664612d343165312d393165302d3261613364333331633033352e706e67)](https://camo.githubusercontent.com/a302427a15e1ed0cd8da485e8bb38aadc673657d8574aad4023d6e71dadd65c3/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f6c61726b2f302f323031382f706e672f31383836322f313534353031333931353238362d34613930663064662d356664612d343165312d393165302d3261613364333331633033352e706e67)

![image-20250312215612680](5day-png/35Seata.png)

Seata管理的分布式事务的典型生命周期：

1. TM 请求 TC 开始新的全局事务。TC 生成一个代表该全局事务的 XID。
2. XID 通过微服务的调用链传播。
3. RM将本地事务作为XID对应全局事务的一个分支注册到TC。
4. TM请求TC提交或者回滚XID对应的全局事务。
5. TC驱动XID对应全局事务下的所有分支事务完成分支的提交或者回滚。

[<img src="https://camo.githubusercontent.com/837b5a990d776ae3eb3f96126a251390e25cdb27a563157b45ea436f3a0dc541/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f6c61726b2f302f323031382f706e672f31383836322f313534353239363931373838312d32366661626562392d373166612d346633652d386137612d6663333137643333383966342e706e67" alt="典型工艺" style="zoom: 80%;" />](https://camo.githubusercontent.com/837b5a990d776ae3eb3f96126a251390e25cdb27a563157b45ea436f3a0dc541/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f6c61726b2f302f323031382f706e672f31383836322f313534353239363931373838312d32366661626562392d373166612d346633652d386137612d6663333137643333383966342e706e67)

 **分布务事务的工作模型**

一个分布式的全局事务，整体是 **两阶段提交** 的模型。全局事务是由若干分支事务组成的，分支事务要满足 **两阶段提交** 的模型要求，即需要每个分支事务都具备自己的：

- 一阶段 prepare 行为
- 二阶段 commit 或 rollback 行为

在两阶段提交协议中，资源管理器（RM, Resource Manager）需要提供“准备”、“提交”和“回滚” 3 个操作；

而事务管理器（TM, Transaction Manager）分 2 阶段协调所有资源管理器，

在第一阶段询问所有资源管理器“准备”是否成功

如果所有资源均“准备”成功则在第二阶段执行所有资源的“提交”操作

否则在第二阶段执行所有资源的“回滚”操作

保证所有资源的最终状态是一致的，要么全部提交要么全部回滚。

### **Seata** **事务模式**

Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。

Seata提供了四种不同的分布式事务解决方案:

- AT模式：最终一致的分阶段事务模式，无业务侵入，也是Seata的默认模式.
- XA模式：强一致性分阶段事务模式，牺牲了一定的可用性，无业务侵入
- TCC模式：最终一致的分阶段事务模式，有业务侵入
- SAGA模式：长事务模式，有业务侵入

#### XA模式

XA 模式是从 1.2 版本支持的事务模式。XA 规范 是 X/Open 组织定义的分布式事务处理（DTP，Distributed Transaction Processing）标准。Seata XA 模式是利用事务资源（数据库、消息服务等）对 XA 协议的支持，以 XA 协议的机制来管理分支事务的一种事务模式。

![image-20250312220805706](5day-png/35事务.png)

RM第一阶段的工作:

- 注册分支事务到TC
- 执行分支业务SQL，但不提交
- 发送执行状态到TC

TC第二阶段的工作:

- TC检测各分支事务执行状态
- 如果都成功，通知所有RM提交事务
- 如果有失败，通知所有RM回滚事务

RM第二阶段的工作:

- 接收TC指令，提交或回滚事务

**XA** **特点**

- 基于数据库的XA协议来实现2PC，也称为XA方案
- CP 模式工作，强一致性
- 难易程度: 简单，基于数据库自带特性实现，无需改表
- 使用要求: 使用支持XA方案的关系型数据库（主流都支持)

**优势**

与 Seata 支持的其它事务模式不同，XA 协议要求事务资源本身提供对规范和协议的支持，所以事务资源（如数据库）可以保障从任意视角对数据的访问有效隔离，满足全局数据一致性。此外的一些优势还包括：

1. 业务无侵入：和 AT 一样，XA 模式将是业务无侵入的，不给应用设计和开发带来额外负担。
2. 数据库的支持广泛：XA 协议被主流关系型数据库广泛支持，不需要额外的适配即可使用。

**缺点**

XA prepare 后，分支事务进入阻塞阶段，收到 XA commit 或 XA rollback 前必须阻塞等待。事务资源长时间得不到释放，锁定周期长，而且在应用层上面无法干预，性能差。

**整体机制**

- 执行阶段：
  - 可回滚：业务 SQL 操作放在 XA 分支中进行，由资源对 XA 协议的支持来保证 *可回滚*
  - 持久化：XA 分支完成后，执行 XA prepare，同样，由资源对 XA 协议的支持来保证 *持久化*（即，之后任何意外都不会造成无法回滚的情况）
- 完成阶段：
  - 分支提交：执行 XA 分支的 commit
  - 分支回滚：执行 XA 分支的 rollback

#### AT模式

```
https://seata.apache.org/zh-cn/docs/user/mode/at
```

AT 模式是 Seata 创新的一种非侵入式的分布式事务解决方案，Seata 在内部做了对数据库操作的代理层，我们使用 Seata AT 模式时，实际上用的是 Seata 自带的数据源代理 DataSourceProxy，Seata 在这层代理中加入了很多逻辑，比如插入回滚 undo_log 日志，检查全局锁等。

**整体机制**

两阶段提交协议的演变：

- 一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。
- 二阶段：
  - 提交异步化，非常快速地完成。
  - 回滚通过一阶段的回滚日志进行反向补偿。

阿里SEATA独有模式，通过生成反向SQL实现数据回滚，需要在数据库额外附加UNDO_LOG表

AT模式同样是分阶段提交的事务模型，不过缺弥补了XA模型中资源锁定周期过长的缺陷。

AT模式建议使用，也应用最多

AT模式支持的数据库有：MySQL、Oracle、PostgreSQL、 TiDB、MariaDB。

AT 模式前提

- 基于支持本地 ACID 事务的关系型数据库。
- Java 应用，通过 JDBC 访问数据库。

**AT** **模式工作原理**

第一阶段RM的工作:

- 注册分支事务
- 记录undo-log（数据快照)
- 执行业务SQL并提交
- 报告事务状态

第二阶段提交时RM的工作:

- 删除undo-log即可

第二阶段回滚时RM的工作:

- 根据undo-log恢复数据到更新前

![image-20250312221240155](5day-png/35事务2.png)

**AT** **模式特点**

- 性能高
- AP 模式工作，存在数据不一致的中间状态
- 难易程度相对简单，靠SEATA自己解析反向SQL并回滚
- 使用要求: 所有服务与数据库必须要自己拥有管理权，因为要创建UNDO_LOG表，支持的最好MySQL，也支持PgSQL
- 应用场景: 高并发互联网应用，允许数据出现短时不一致，可通过对账程序或补录来保证最终一致性。

**AT模式的优点**

- 一阶段完成直接提交事务，释放数据库资源，性能比较好
- 利用全局锁实现读写隔离
- 没有代码侵入，框架自动完成回滚和提交

**AT模式的缺点**

- 两阶段之间属于软状态，属于最终一致
- 框架的快照功能会影响性能，但比XA模式要好很多

**AT模式与XA模式主要区别**

- XA模式第一阶段不提交事务，锁定资源

  AT模式第一阶段直接提交，不锁定资源。

- XA模式依赖数据库机制实现回滚

  AT模式利用数据快照实现数据回滚

- XA模式强一致CP

  AT模式最终一致，AP

```sql
CREATE TABLE `undo_log` (
    `id` BIGINT(20) NOT NULL AUTO_INCREMENT COMMENT '主键ID',
    `branch_id` BIGINT(20) NOT NULL COMMENT '分支事务ID',
    `xid` VARCHAR(100) NOT NULL COMMENT '全局事务ID',
    `context` VARCHAR(128) NOT NULL COMMENT '事务上下文信息（0.7.0+ 新增）',
    `rollback_info` LONGBLOB NOT NULL COMMENT '回滚数据',
    `log_status` INT(11) NOT NULL COMMENT '日志状态（0-正常，1-已清理）',
    `log_created` DATETIME NOT NULL COMMENT '日志创建时间',
    `log_modified` DATETIME NOT NULL COMMENT '日志修改时间',
    PRIMARY KEY (`id`),
    UNIQUE KEY `ux_undo_log` (`xid`, `branch_id`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT='Seata 分布式事务回滚日志表';
```

**字段说明**

| 字段            | 类型                                 | 说明                             |
| --------------- | ------------------------------------ | -------------------------------- |
| `id`            | `BIGINT(20) NOT NULL AUTO_INCREMENT` | 主键ID，自增                     |
| `branch_id`     | `BIGINT(20) NOT NULL`                | 分支事务ID                       |
| `xid`           | `VARCHAR(100) NOT NULL`              | 全局事务ID                       |
| `context`       | `VARCHAR(128) NOT NULL`              | 事务上下文信息（0.7.0+ 新增）    |
| `rollback_info` | `LONGBLOB NOT NULL`                  | 回滚数据，存储二进制信息         |
| `log_status`    | `INT(11) NOT NULL`                   | 事务日志状态（0-正常，1-已清理） |
| `log_created`   | `DATETIME NOT NULL`                  | 日志创建时间                     |
| `log_modified`  | `DATETIME NOT NULL`                  | 日志修改时间                     |



#### TCC模式

TCC 模式是 Seata 支持的一种由业务方细粒度控制的侵入式分布式事务解决方案，是继 AT 模式后第二种支持的事务模式，最早由蚂蚁金服贡献。其分布式事务模型直接作用于服务层，不依赖底层数据库，可以灵活选择业务资源的锁定粒度，减少资源锁持有时间，可扩展性好，可以说是为独立部署的 SOA 服务而设计的。

![Overview of a global transaction](https://seata.apache.org/zh-cn/assets/images/seata_tcc-1-1f7a834639aa755d73fa2af435c4f042.png)

![image-20250312221829192](5day-png/35事务3.png)

![image-20250312221855842](5day-png/35事务4.png)

**TCC的优点**

- 性能好，一阶段完成直接提交事务，释放数据库资源
- 相比AT模型，无需生成快照，无需使用全局锁，性能最强
- 不依赖数据库事务，而是依赖补偿操作，可以用于非事务型数据库
- TCC 完全不依赖底层数据库，能够实现跨数据库、跨应用资源管理，可以提供给业务方更细粒度的控制。

**TCC的缺点**

- 有代码侵入，每个相关微服务都可能需要人为编写try、Confirm和Cancel接口及额外相关的数据库表的设计，太麻烦
- 需要考虑Confirm和Cancel的失败情况，做好幂等处理
- 实现复杂，SEATATC只负责全局事务的提交与回滚指令，具体的回滚处理全靠程序员自己实现，实际使用不多
- 所有服务与数据库必须要自己拥有管理权·支持异构数据库，可以使用不同选型实现
- AP 模式工作，存在数据不一致的中间状态, 软状态，事务是最终一致



#### **Saga模式**

Saga模式分为两个阶段

- 一阶段：直接提交本地事务
- 二阶段：成功则什么都不做;失败则通过编写补偿业务来回滚

![image-20250312222012067](5day-png/35事务5.png)

**优势：**

- 一阶段提交本地事务，无锁，高性能
- 事件驱动架构，参与者可异步执行，高吞吐
- 不用编写TCC中的三个阶段，补偿服务易于实现

**缺点：**

- 软状态持续时间不确定，时效性差
- 不保证隔离性，没有锁，没有事务隔离，会有脏写

**Saga** **特点**

性能:不一定，取决于第三方服务

AP模式工作，存在数据不一致的中间状态

难易程度:复杂，提交与回滚流程全靠程序员编排

使用要求:在当前架构引入状态机机制，类似于工作流，无法保证隔离性



### **Seata Server** **安装**

```
https://seata.apache.org/zh-cn/docs/ops/deploy-guide-beginner
```

####  二进制安装

```bash
#安装java环境
[root@ubuntu2404 seata-server]#apt install -y openjdk-21-jdk

[root@ubuntu2404 ~]#ls
apache-seata-2.2.0-incubating-bin.tar.gz
[root@ubuntu2404 ~]#tar xf apache-seata-2.2.0-incubating-bin.tar.gz -C /usr/local/              
[root@ubuntu2404 ~]#cd /usr/local/seata-server/
[root@ubuntu2404 seata-server]#ls
bin  conf  Dockerfile  ext  lib  script  target
[root@ubuntu2404 seata-server]#ls bin/
seata-server.bat  seata-server.sh  seata-setup.sh

#启动seata-server
[root@ubuntu2404 seata-server]#./bin/seata-server.sh start

[root@ubuntu2404 seata-server]#ss -tnulp | grep java
tcp   LISTEN 0      3                  *:9898            *:*    users:(("java",pid=110040,fd=303))                     
tcp   LISTEN 0      1024               *:8091            *:*    users:(("java",pid=110040,fd=318))                     
tcp   LISTEN 0      100                *:7091            *:*    users:(("java",pid=110040,fd=299))   
```

```
#浏览器访问，默认密码用户名seata/seata
10.0.0.201:7091
```

**支持的启动参数**

| 参数 | 全写         | 作用                       | 备注                                                         |
| :--- | :----------- | :------------------------- | :----------------------------------------------------------- |
| -h   | --host       | 指定在注册中心注册的 IP    | 不指定时获取当前的 IP，外部访问部署在云环境和容器中的 server 建议指定 |
| -p   | --port       | 指定 server 启动的端口     | 默认为 8091                                                  |
| -m   | --storeMode  | 事务日志存储方式           | 支持`file`,`db`,`redis`，默认为 `file` 注:redis需seata-server 1.3版本及以上 |
| -n   | --serverNode | 用于指定seata-server节点ID | 如 `1`,`2`,`3`..., 默认为 `1`                                |
| -e   | --seataEnv   | 指定 seata-server 运行环境 | 如 `dev`, `test` 等, 服务启动时会使用 `registry-dev.conf` 这样的配置 |

如：

```bash
$ sh ./bin/seata-server.sh -p 8091 -h 127.0.0.1 -m file
```



#### **Docker** **部署**

```
docker run --name seata-server -p 8091:8091 -p 7091:7091 seataio/seata-server:1.5.0
```



#### 脚本安装

```bash
#!/bin/bash
#
#支持在线和离线安装，建议离线安装

SEATA_VERSION=2.2.0
#SEATA_VERSION=2.0.0
#SEATA_VERSION=1.8.0
#SEATA_VERSION=1.7.1
#SEATA_VERSION=1.6.1
#SEATA_VERSION=1.5.1


SEATA_FILE=apache-seata-${SEATA_VERSION}-incubating-bin.tar.gz
#SEATA_FILE=seata-server-${SEATA_VERSION}-incubating-bin.tar.gz
#SEATA_FILE=seata-server-${SEATA_VERSION}.tar.gz

SEATA_URL=https://dist.apache.org/repos/dist/release/incubator/seata/${SEATA_VERSION}/apache-seata-${SEATA_VERSION}-incubating-bin.tar.gz
#SEATA_URL=http://mirror.ghproxy.com/https://github.com/seata/seata/releases/download/v${SEATA_VERSION}/$SEATA_FILE

INSTALL_DIR=/usr/local/seata-server
#INSTALL_DIR=/usr/local/seata


HOST=`hostname -I|awk '{print $1}'`

.  /etc/os-release

color () {
    RES_COL=60
    MOVE_TO_COL="echo -en \\033[${RES_COL}G"
    SETCOLOR_SUCCESS="echo -en \\033[1;32m"
    SETCOLOR_FAILURE="echo -en \\033[1;31m"
    SETCOLOR_WARNING="echo -en \\033[1;33m"
    SETCOLOR_NORMAL="echo -en \E[0m"
    echo -n "$1" && $MOVE_TO_COL
    echo -n "["
    if [ $2 = "success" -o $2 = "0" ] ;then
        ${SETCOLOR_SUCCESS}
        echo -n $"  OK  "    
    elif [ $2 = "failure" -o $2 = "1"  ] ;then 
        ${SETCOLOR_FAILURE}
        echo -n $"FAILED"
    else
        ${SETCOLOR_WARNING}
        echo -n $"WARNING"
    fi
    ${SETCOLOR_NORMAL}
    echo -n "]"
    echo 
}


install_jdk() {
    java -version &>/dev/null && { color "JDK 已安装!" 1 ; return;  }
    if command -v yum &>/dev/null ; then
        yum -y install java-1.8.0-openjdk-devel || { color "安装JDK失败!" 1; exit 1; }
    elif command -v apt &>/dev/null ; then
        apt update
        #apt install openjdk-11-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        #apt install openjdk-8-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
                apt install openjdk-17-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
    else
       color "不支持当前操作系统!" 1
       exit 1
    fi
    java -version && { color "安装 JDK 完成!" 0 ; } || { color "安装JDK失败!" 1; exit 1; } 
}



install_seata() {
    if [ -f ${SEATA_FILE} ] ;then
        cp ${SEATA_FILE} /usr/local/src/
    else
        wget -P /usr/local/src/ --no-check-certificate ${SEATA_URL} || { color  "下载失败!" 1 ;exit 1; }
    fi
    tar xf /usr/local/src/${SEATA_FILE} -C /usr/local

    cat > /lib/systemd/system/seata.service <<EOF
[Unit]
Description=seata.service
After=network.target

[Service]
Type=forking
ExecStart=${INSTALL_DIR}/bin/seata-server.sh start
ExecStop=${INSTALL_DIR}/bin/seata-server.sh  stop
ExecReload=${INSTALL_DIR}/bin/seata-server.sh restart

[Install]
WantedBy=multi-user.target
EOF

    systemctl daemon-reload
    systemctl enable --now seata.service
}

start_seata () {
    systemctl is-active seata.service
    if [ $? -eq 0 ] ;then 
        color "seata 安装成功!" 0  
        echo "-------------------------------------------------------------------"
        echo -e "请访问链接: \E[32;1mhttp://$HOST:7091/\E[0m" 
        echo -e "用户/密码: \E[32;1mseata/seata\E[0m" 
    else 
        color "seata 安装失败!" 1
        exit 1
    fi
}



install_jdk

install_seata

start_seata
```



### **综合案例: Seata结合Nacos**

```
https://github.com/alibaba/spring-cloud-alibaba/blob/2022.x/spring-cloud-alibaba-examples/seata-example/readme-zh.md
```

用户购买商品的业务逻辑。整个业务逻辑由3个微服务提供支持：

仓储服务：对给定的商品扣除仓储数量。

订单服务：根据采购需求创建订单。

帐户服务：从用户帐户中扣除余额。

![image-20250313094007856](5day-png/35seats.png)





需要三台主机

- 一台主机运行 MySQL，Nacos
- 一台主机运行 Seata-Server 使用JDK-21
- 一台主机运行四个微服务，使用JDK-21

```bash
#做DNS解析
```

```bash
#安装和配置 MySQL nacos
[root@ubuntu2404 ~]#docker run --name mysql --network host -d -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=seata -e MYSQL_USER=seata -e MYSQL_PASSWORD=123456 --restart always registry.cn-beijing.aliyuncs.com/wangxiaochun/mysql:8.0.29-oracle
[root@ubuntu2404 ~]#docker run --name nacos -e MODE=standalone --network host -d --restart always nacos/nacos-server:v2.4.3-slim
```

```bash
https://github.com/alibaba/spring-cloud-alibaba/blob/2022.x/spring-cloud-alibaba-examples/seata-example/all.sql

[root@ubuntu2404 ~]#mysql -useata -p123456 -h10.0.0.200 seata
mysql> source all.sql
```

```bash
#安装seata
[root@ubuntu2404 ~]#bash install_seata.sh
[root@ubuntu2404 ~]#cat /usr/local/seata-server/conf/application.yml
#  Copyright 1999-2019 Seata.io Group.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.

server:
  port: 7091

spring:
  application:
    name: seata-server

logging:
  config: classpath:logback-spring.xml
  file:
    path: ${log.home:${user.home}/logs/seata}
  extend:
    logstash-appender:
      destination: 127.0.0.1:4560
    kafka-appender:
      bootstrap-servers: 127.0.0.1:9092
      topic: logback_to_logstash

console:
  user:
    username: seata
    password: seata

seata:
#####################################################################################
  config:
    type: nacos
    nacos:
      server-addr: nacos.wang.org:8848
      namespace:
      # group: SEATA_GROUP
      #username: nacos
      #password: nacos
      context-path:
      #data-id: seataServer.properties
      data-id: seata.properties
      ##if use MSE Nacos with auth, mutex with username/password attribute
      #access-key:
      #secret-key:
  registry:
    # nacos配置
    type: nacos
    nacos:
      application: seata-server
      server-addr: nacos.wang.org:8848
      # group: SEATA_GROUP
      namespace:
      cluster: default
      #username: nacos
      #password: nacos
#####################################################################################

  store:
    # support: file 、 db 、 redis 、 raft
    mode: file
  #  server:
  #    service-port: 8091 #If not configured, the default is '${server.port} + 1000'
  security:
    secretKey: SeataSecretKey0c382ef121d778043159209298fd40bf3850a017
    tokenValidityInMilliseconds: 1800000
    ignore:
      urls: /,/**/*.css,/**/*.js,/**/*.html,/**/*.map,/**/*.svg,/**/*.png,/**/*.jpeg,/**/*.ico,/api/v1/auth/login,/metadata/v1/**
```



**在nacos中创建配置**

```
data-id: seata.properties 
Group: SEATA_GROUP (seata 1.8.0 默认分组)
```

```properties
#For details about configuration items, see https://seata.io/zh-cn/docs/user/configurations.html
#Transport configuration, for client and server
transport.type=TCP
transport.server=NIO
transport.heartbeat=true
transport.enableTmClientBatchSendRequest=false
transport.enableRmClientBatchSendRequest=true
transport.enableTcServerBatchSendResponse=false
transport.rpcRmRequestTimeout=30000
transport.rpcTmRequestTimeout=30000
transport.rpcTcRequestTimeout=30000
transport.threadFactory.bossThreadPrefix=NettyBoss
transport.threadFactory.workerThreadPrefix=NettyServerNIOWorker
transport.threadFactory.serverExecutorThreadPrefix=NettyServerBizHandler
transport.threadFactory.shareBossWorker=false
transport.threadFactory.clientSelectorThreadPrefix=NettyClientSelector
transport.threadFactory.clientSelectorThreadSize=1
transport.threadFactory.clientWorkerThreadPrefix=NettyClientWorkerThread
transport.threadFactory.bossThreadSize=1
transport.threadFactory.workerThreadSize=default
transport.shutdown.wait=3
transport.serialization=seata
transport.compressor=none

#Transaction routing rules configuration, only for the client
service.vgroupMapping.default_tx_group=default
#If you use a registry, you can ignore it
service.default.grouplist=seata.wang.org:8091
service.enableDegrade=false
service.disableGlobalTransaction=false

#Transaction rule configuration, only for the client
client.rm.asyncCommitBufferLimit=10000
client.rm.lock.retryInterval=10
client.rm.lock.retryTimes=30
client.rm.lock.retryPolicyBranchRollbackOnConflict=true
client.rm.reportRetryCount=5
client.rm.tableMetaCheckEnable=true
client.rm.tableMetaCheckerInterval=60000
client.rm.sqlParserType=druid
client.rm.reportSuccessEnable=false
client.rm.sagaBranchRegisterEnable=false
client.rm.sagaJsonParser=fastjson
client.rm.tccActionInterceptorOrder=-2147482648
client.tm.commitRetryCount=5
client.tm.rollbackRetryCount=5
client.tm.defaultGlobalTransactionTimeout=60000
client.tm.degradeCheck=false
client.tm.degradeCheckAllowTimes=10
client.tm.degradeCheckPeriod=2000
client.tm.interceptorOrder=-2147482648
client.undo.dataValidation=true
client.undo.logSerialization=jackson
client.undo.onlyCareUpdateColumns=true
server.undo.logSaveDays=7
server.undo.logDeletePeriod=86400000
client.undo.logTable=undo_log
client.undo.compress.enable=true
client.undo.compress.type=zip
client.undo.compress.threshold=64k
#For TCC transaction mode
tcc.fence.logTableName=tcc_fence_log
tcc.fence.cleanPeriod=1h

#Log rule configuration, for client and server
log.exceptionRate=100

#Transaction storage configuration, only for the server. The file, db, and redis configuration values are optional.
store.mode=file
store.lock.mode=file
store.session.mode=file
#Used for password encryption
store.publicKey=

#If `store.mode,store.lock.mode,store.session.mode` are not equal to `file`, you can remove the configuration block.
store.file.dir=file_store/data
store.file.maxBranchSessionSize=16384
store.file.maxGlobalSessionSize=512
store.file.fileWriteBufferCacheSize=16384
store.file.flushDiskMode=async
store.file.sessionReloadReadSize=100

#These configurations are required if the `store mode` is `db`. If `store.mode,store.lock.mode,store.session.mode` are not equal to `db`, you can remove the configuration block.
store.db.datasource=druid
store.db.dbType=mysql
#############################修改内容##########################
store.db.driverClassName=com.mysql.jdbc.Driver
store.db.url=jdbc:mysql://mysql.wang.org:3306/seata?useUnicode=true&rewriteBatchedStatements=true
store.db.user=seata
store.db.password=123456
##########################################################
store.db.minConn=5
store.db.maxConn=30
store.db.globalTable=global_table
store.db.branchTable=branch_table
store.db.distributedLockTable=distributed_lock
store.db.queryLimit=100
store.db.lockTable=lock_table
store.db.maxWait=5000

#These configurations are required if the `store mode` is `redis`. If `store.mode,store.lock.mode,store.session.mode` are not equal to `redis`, you can remove the configuration block.
store.redis.mode=single
store.redis.single.host=127.0.0.1
store.redis.single.port=6379
store.redis.sentinel.masterName=
store.redis.sentinel.sentinelHosts=
store.redis.sentinel.sentinelPassword=
store.redis.maxConn=10
store.redis.minConn=1
store.redis.maxTotal=100
store.redis.database=0
store.redis.password=
store.redis.queryLimit=100

#Transaction rule configuration, only for the server
server.recovery.committingRetryPeriod=1000
server.recovery.asynCommittingRetryPeriod=1000
server.recovery.rollbackingRetryPeriod=1000
server.recovery.timeoutRetryPeriod=1000
server.maxCommitRetryTimeout=-1
server.maxRollbackRetryTimeout=-1
server.rollbackRetryTimeoutUnlockEnable=false
server.distributedLockExpireTime=10000
server.xaerNotaRetryTimeout=60000
server.session.branchAsyncQueueSize=5000
server.session.enableBranchAsyncRemove=false
server.enableParallelRequestHandle=false

#Metrics configuration, only for the server
metrics.enabled=false
metrics.registryType=compact
metrics.exporterList=prometheus
metrics.exporterPrometheusPort=9898


service.vgroupMapping.order-service-tx-group=default
service.vgroupMapping.account-service-tx-group=default
service.vgroupMapping.business-service-tx-group=default
service.vgroupMapping.storage-service-tx-group=default
```





## Spring-cloud-gateway

![image-20250313110217845](5day-png/35Spring-cloud-gateway.png)

- 路由转发

  接收外界请求，通过网关的路由转发,转发到后端的服务上,和传统的反向代理功能类似

  此功能是API网关的最核心功能

- 过滤器: 自行定制实现

  过滤器中默认提供了多种内置功能还支持额外的自定义功能。

  比较常用的功能有网关的容错、限流以及请求及相应的额外处理。

Route 称为路由，一个Gateway项目可以包含多个Route。

一个Route路由包含如下部分:

- Route ID: 是自定的，路由的唯一标识
- predicates: 路由断言，判断请求是否匹配和符合转发规则的要求，如果符合则转发到路由目的地
- URI: 路由目的地，为一个地址, 支持多种方式, 包括 LB,http,websocket 
- filters: 路由过滤器,对请求或响应进行过滤处理, 比如: 身份认证和权限校验限流限速,此为可选项

```yaml
server:
  port: 10010  # 网关端口

spring:
  application:
    name: gateway  # 服务名称
  cloud:
    nacos:
      server-addr: localhost:8848  # Nacos 地址
    gateway:
      routes:
        - id: user-service  # 路由 ID，必须唯一
          uri: lb://userservice  # 负载均衡，指向 Nacos 中的 userservice 服务
          predicates:
            - Path=/user/**  # 以 /user/ 开头的请求符合该路由
        - id: order-service
          uri: lb://orderservice  # 负载均衡，指向 Nacos 中的 orderservice 服务
          predicates:
            - Path=/order/**
          filters:
            - StripPrefix=1  # 从请求路径中删除第一个前缀 `/order`，再转发给后端服务

```

**关于** ***** **和** ***\****

```
*（单星号）：#匹配一个路径段中的任意字符，不包括斜杠。适用于匹配具体的路径层次。
**（双星号）：#匹配任意字符，包括多个路径段和斜杠。适用于匹配多层次的路径。表示零个或多个目录级别的任意字符。它可以匹配任意长度的路径
#示例
Path=/api/*
/api/：不匹配
/api/test：匹配
/api/user：匹配
/api/user/123：不匹配（因为有多个路径段）

Path=/api/**
#这个路径谓词表示匹配所有以/api/开头的请求路径。

/api/：匹配
/api/test：匹配
/api/user/123：匹配
/api/v1/products/list：匹配
```

![image-20250313110559142](5day-png/35Spring-cloud-gateway1.png)

![image-20250313110627262](5day-png/35Spring-cloud-gateway2.png)

**Datetime**

匹配日期时间之后发生的请求

```
spring: 
 application:
   name: ruoyi-gateway
 cloud:
   gateway:
     routes:
       - id: myapp-system
         uri: http://localhost:9201/
         predicates:
           - After=2022-12-22T14:20:00.000+08:00[Asia/Shanghai]
```

**Cookie**

匹配指定名称且其值与正则表达式匹配的 cookie

```
spring: 
 application:
   name: ruoyi-gateway
 cloud:
   gateway:
     routes:
       - id: myapp-system
         uri: http://localhost:9201/
         predicates:
           - Cookie=loginname, myapp
```

**Header**

匹配具有指定名称的请求头， \d+ 值匹配正则表达式

```
spring: 
 application:
   name: myapp-gateway
 cloud:
   gateway:
     routes:
       - id: myapp-system
         uri: http://localhost:9201/
         predicates:
           - Header=X-Request-Id, \d+
```

**Host**

匹配主机名的列表

```
spring: 
 application:
   name: myapp-gateway
 cloud:
   gateway:
     routes:
       - id: myapp-system
         uri: http://localhost:9201/
         predicates:
           - Host=**.somehost.org,**.anotherhost.org
```

**Method**

匹配请求methods的参数，它是一个或多个参数

```
spring: 
 application:
   name: myapp-gateway
 cloud:
   gateway:
     routes:
       - id: myapp-system
         uri: http://localhost:9201/
         predicates:
           - Method=GET,POST
```

**Path**

匹配请求路径

```
spring: 
 application:
   name: myapp-gateway
 cloud:
   gateway:
     routes:
       - id: myapp-system
         uri: http://localhost:9201/
         predicates:
           - Path=/system/**
```

**Query**

匹配查询参数

```
spring: 
 application:
   name: myapp-gateway
 cloud:
   gateway:
     routes:
       - id: myapp-system
         uri: http://localhost:9201/
         predicates:
           - Query=username, abc.
```

**RemoteAddr**

匹配IP地址和子网掩码

```
spring: 
 application:
   name: myapp-gateway
 cloud:
   gateway:
     routes:
       - id: myapp-system
         uri: http://localhost:9201/
         predicates:
           - RemoteAddr=192.168.10.0/24
```

**Weight**

匹配权重

```
spring: 
 application:
   name: myapp-gateway
 cloud:
   gateway:
     routes:
       - id: myapp-system-a
         uri: http://localhost:9201/
         predicates:
           - Weight=group1, 8
       - id: myapp-system-b
         uri: http://localhost:9201/
         predicates:
           - Weight=group1, 2
```

**路由URI三种实现**

在 spring cloud gateway 中配置 uri 有三种方式，包括

**websocket配置方式**

```
spring:
 cloud:
   gateway:
     routes:
       - id: myapp-api
         uri: ws://localhost:9090/
         predicates:
           - Path=/api/**
```

**http地址配置方式**

```
spring:
 cloud:
   gateway:
     routes:
       - id: myapp-api
         uri: http://localhost:9090/
         predicates:
           - Path=/api/**
```

**注册中心配置方式**

```
spring:
 cloud:
   gateway:
     routes:
       - id: myapp-api
         uri: lb://myapp-api   #myapp-api 服务名称，需要结合nacos发现实现解析IP：Port
         predicates:
           - Path=/api/**
```

## **APISIX**

### 介绍

```
https://apisix.apache.org/zh/docs/apisix/getting-started/configure-routes/
```

```
https://apisix.apache.org/
https://apisix.apache.org/zh/
https://github.com/apache/apisix
https://www.apiseven.com/
```

主要特性

- Apache APISIX 使用户只需专注在具体业务中，而无需考虑 API 处理基础组件。
- Apache APISIX 是首个提供低代码能力的开源 API 网关，作为 Apache 软件基金会顶级项目，避免出现Licences的商业纠纷
- Apache APISIX 采用了基于OpenResty的架构，以及高效的Nginx引擎，能够处理更高的并发请求，响应更快的API。单核心QPS可达1.5万，延迟低于0.7ms，基于ETCD的事件驱动的配置生效时间小于1ms，相对Kong轮询拉取需要5s
- APISIX 与其它API网关不同,不使用关系型数据库,而使用 etcd 作为存储，让 APISIX 在底层上更加贴合云原生，也让它在系统高可用和性能带来了更多优势
- 多平台支持: APISIX提供了多平台解决方案，它不但支持裸机运行，也支持在Kubernetes中使用，还支持与AWS Lambda、Azure Function、Lua 函数和Apache OpenWhisk等云服务集成。
- 全动态能力: APISIX支持热加载，这意味着你不需要重启服务就可以更新APISIX的配置。
- 精细化路由: APISIX支持使用NGINX内置变量做为路由的匹配条件,你可以自定义匹配函数来过滤请求,匹配路由。
- 运维友好: APISIX支持与以下工具和平台集成: Apache SkyWalking、 Consul、 Nacos、 Eureka、 HashiCorp Vault、 Zipkin, 通过APISIX Dashboard,运维人员可以通过友好且直观的UI配置APISIX。
- 部署更简单：APISIX提供了Docker镜像、Kubernetes Helm Chart等多种部署方式，可以快速部署APISIX。
- 多语言插件支持: APISIX支持多种开发语言进行插件开发,开发人员可以选择擅长语言的SDK开发自定义插件。当前支持80多种插件，覆盖各种功能包括;身份认证、安全、日志、可观测性...
- 开源社区更活跃：APISIX由Apache APISIX社区维护，有着广泛的用户和贡献者社区，提供了更加完善的文档和支持。

**APISIX** **架构**

![image-20250313142854098](5day-png/35APISIX架构.png)

- 数据平面 Data Plane

  它是真正去处理来自客户端请求的一个组件，去处理用户的真实流量，包括像身份验证、证书卸载、日志分析和可观测性等功能。

  数据面本身并不会存储任何数据，所以它是一个无状态结构。

- 控制平面 Control Plane

  增加独立的控制平面（CP)，将来可以支持调度所有数据平面（DP)，不局限于APISIX

  APISIX在底层架构上和其它API网关的一个很大不同就在于控制面。

  APISIX在控制面上选择使用etcd。

- AI 平面 AP 

  未来增加AI 平面，因为仅是接入层本身没有价值，分析流动的数据并反哺业务，才有价值

**主要组件**

- Route

  Route 也称为路由，是 APISIX 中最基础和最核心的资源对象。

  APISIX 可以通过路由定义规则来匹配客户端请求，根据匹配结果加载并执行相应的插件，最后把请求转发给到指定的上游服务。

  路由中主要包含三部分内容：匹配规则、插件配置和上游信息。

- Upstream

  Upstream 也称为上游，上游是对虚拟主机的抽象，即应用层服务或节点的抽象。

  上游的作用是按照配置规则对服务节点进行负载均衡，它的地址信息可以直接配置到路由或服务上。

  当多个路由或服务引用同一个上游时，可以通过创建上游对象，在路由或服务中使用上游 ID 的方式引用上游，减轻维护压力。

- 服务

  Service 也称为服务，是某类 API 的抽象（也可以理解为一组 Route 的抽象）。

  它通常与上游服务抽象是一一对应的，Route 与 Service 之间，通常是 N:1 的关系。

  Service 用于定义 API 服务的通用属性，并将其与上游（ upstream ）关联。它的主要功能包括：

  1. **路由配置**： service 可以通过路由规则（ route ）关联，指定哪些请求会被转发到该服务。
  2. **插件配置**：可以在 service 层面上配置插件，如身份认证、限流、监控等，这些插件将应用于所有通过该服务的请求。
  3. **上游关联**： service 会引用一个 upstream ，定义具体的后端服务集群。

- Admin API

  用户可以通过 Admin API 控制 APISIX 实例。



### APISIX安装

APISIX 包括以下三个核心组件

- APISIX 
- ETCD
- APISIX Dashboard

建议安装LTS版本

```
https://apisix.apache.org/docs/apisix/installation-guide/#installing-etcd
```



#### **安装etcd**

```
https://github.com/etcd-io/etcd/releases/download/v3.5.19/etcd-v3.5.19-linux-amd64.tar.gz
```

```bash
[root@ubuntu2404 ~]#ls
etcd-v3.5.19-linux-amd64.tar.gz
[root@ubuntu2404 ~]#tar xf etcd-v3.5.19-linux-amd64.tar.gz -C /usr/local/
[root@ubuntu2404 ~]#cd /usr/local/
[root@ubuntu2404 local]#ln -s etcd-v3.5.19-linux-amd64/ etcd
[root@ubuntu2404 local]#cd etcd
[root@ubuntu2404 etcd]#ls
Documentation  etcd  etcdctl  etcdutl  README-etcdctl.md  README-etcdutl.md  README.md  READMEv2-etcdctl.md
[root@ubuntu2404 etcd]#ln -s /usr/local/etcd/etcd /usr/local/bin/

#编写service文件
[root@ubuntu2404 etcd]#vim /lib/systemd/system/etcd.service
[Unit]
Description=etcd
After=network-online.target
Wants=network-online.target

[Service]
#Type=forking
#Restart=on-failure
ExecStart=/usr/local/bin/etcd
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

#启动etcd
[root@ubuntu2404 etcd]#systemctl daemon-reload 
[root@ubuntu2404 etcd]#systemctl enable --now  etcd

[root@ubuntu2404 etcd]#ss -tnulp | grep etcd
tcp   LISTEN 0      4096                           127.0.0.1:2380       0.0.0.0:*    users:(("etcd",pid=7351,fd=6))                         
tcp   LISTEN 0      4096                           127.0.0.1:2379       0.0.0.0:*    users:(("etcd",pid=7351,fd=7))  
```

脚本安装

```bash
[root@ubuntu2404 ~]#cat install_etcd.sh 
#!/bin/bash

ETCD_VERSION='3.5.19'
#ETCD_VERSION='3.5.4'
#ETCD_VERSION='3.5.15'
GITHUB_PROXY=http://mirror.ghproxy.com/
[ -e etcd-v${ETCD_VERSION}-linux-amd64.tar.gz  ] || wget ${GITHUB_PROXY}https://github.com/etcd-io/etcd/releases/download/v${ETCD_VERSION}/etcd-v${ETCD_VERSION}-linux-amd64.tar.gz
tar -xvf etcd-v${ETCD_VERSION}-linux-amd64.tar.gz -C /usr/local/
cd /usr/local/etcd-v${ETCD_VERSION}-linux-amd64 && cp -a etcd etcdctl /usr/local/bin/

etcd --version

#nohup etcd >/tmp/etcd.log 2>&1 &

cat  > /lib/systemd/system/etcd.service <<EOF
[Unit]
Description=etcd
After=network-online.target
Wants=network-online.target

[Service]
#Type=forking
#Restart=on-failure
ExecStart=/usr/local/bin/etcd
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

EOF

systemctl daemon-reload 
systemctl enable --now  etcd

```

#### **安装apisix**

```bash
# amd64
wget -O - http://repos.apiseven.com/pubkey.gpg | sudo apt-key add -
echo "deb http://repos.apiseven.com/packages/debian bullseye main" | sudo tee /etc/apt/sources.list.d/apisix.list
```

```bash
apt update
apt list apisix -a
apt install -y apisix=3.10.0-0
```

安装 APISIX 后，您可以通过运行以下命令初始化配置文件和 etcd：

```bash
apisix init
```

```bash
[root@ubuntu2404 ~]#apisix init
/usr/local/openresty//luajit/bin/luajit /usr/local/apisix/apisix/cli/apisix.lua init
#报警文件描述符打开的不多，默认1024
Warning! Current maximum number of open file descriptors [1024] is not greater than 1024, please increase user limits by execute 'ulimit -n <new user limits>' , otherwise the performance is low.

#修改资源限制，消除告警
[root@ubuntu2404 ~]#vim /etc/security/limits.conf 
#在最后加一行
root  -  nofile 88888

#重启生效
[root@ubuntu2404 ~]#reboot

[root@ubuntu2404 ~]#ulimit -n
88888

[root@ubuntu2404 ~]#apisix init
/usr/local/openresty//luajit/bin/luajit /usr/local/apisix/apisix/cli/apisix.lua init
```

要启动 APISIX 服务器，请运行：

```bash
apisix start

systemctl start apisix.service
```

```bash
[root@ubuntu2404 ~]#ss -tnlp | grep openresty
LISTEN 0      511                            0.0.0.0:9180       0.0.0.0:*    users:(("openresty",pid=2251,fd=8),("openresty",pid=2250,fd=8),("openresty",pid=2249,fd=8))
LISTEN 0      511                            0.0.0.0:9080       0.0.0.0:*    users:(("openresty",pid=2251,fd=13),("openresty",pid=2249,fd=13))                          
LISTEN 0      511                            0.0.0.0:9080       0.0.0.0:*    users:(("openresty",pid=2250,fd=9),("openresty",pid=2249,fd=9))                            
LISTEN 0      511                          127.0.0.1:9090       0.0.0.0:*    users:(("openresty",pid=2251,fd=7),("openresty",pid=2250,fd=7),("openresty",pid=2249,fd=7))
LISTEN 0      511                          127.0.0.1:9091       0.0.0.0:*    users:(("openresty",pid=2254,fd=26))                                                       
LISTEN 0      511                            0.0.0.0:9443       0.0.0.0:*    users:(("openresty",pid=2251,fd=15),("openresty",pid=2249,fd=15))                          
LISTEN 0      511                            0.0.0.0:9443       0.0.0.0:*    users:(("openresty",pid=2250,fd=11),("openresty",pid=2249,fd=11))                          
LISTEN 0      511                               [::]:9080          [::]:*    users:(("openresty",pid=2250,fd=10),("openresty",pid=2249,fd=10))                          
LISTEN 0      511                               [::]:9080          [::]:*    users:(("openresty",pid=2251,fd=14),("openresty",pid=2249,fd=14))                          
LISTEN 0      511                               [::]:9443          [::]:*    users:(("openresty",pid=2250,fd=12),("openresty",pid=2249,fd=12))                          
LISTEN 0      511                               [::]:9443          [::]:*    users:(("openresty",pid=2251,fd=16),("openresty",pid=2249,fd=16)) 
```

配置文件格式检查

```
apisix test
```

配置文件

```bash
[root@ubuntu2404 ~]#cat /usr/local/apisix/conf/config.yaml 
---
...
etcd:
  prefix: /apisix
  tls:
    verify: true
  host:	
  - http://127.0.0.1:2379		#修改etcd地址
  startup_retry: 2
  watch_timeout: 50
  timeout: 30
...
  admin:
    admin_api_version: v3
    admin_key_required: true
    admin_key:
    - name: admin
      role: admin
      key: edd1c9f034335f136f87ad84b625c8f1		#这个生产环境建议修改
    allow_admin:
    - 127.0.0.0/24
    enable_admin_cors: true
    admin_listen:
      port: 9180
      ip: 0.0.0.0
  role: traditional
  config_provider: etcd
graphql:
  max_size: 1048576
...
apisix:
  enable_admin: true
  node_listen:
  - 9080		#	修改监听端口
  
...
```

```
[root@ubuntu2404 ~]#curl -i http://127.0.0.1:9180/apisix/admin/routes -H"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1"
HTTP/1.1 200 OK
Date: Thu, 13 Mar 2025 07:43:51 GMT
Content-Type: application/json
Transfer-Encoding: chunked
Connection: keep-alive
Server: APISIX/3.11.0
Access-Control-Allow-Origin: *
Access-Control-Allow-Credentials: true
Access-Control-Expose-Headers: *
Access-Control-Max-Age: 3600
X-API-VERSION: v3

{"list":[],"total":0}
```

**Docker 脚本安装**

```
curl -sL https://run.api7.ai/apisix/quickstart
```

```bash
[root@ubuntu2404 ~]#cat quickstart 
#!/bin/bash

#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

DEFAULT_ETCD_IMAGE_NAME="bitnami/etcd"
DEFAULT_ETCD_IMAGE_TAG="3.5.7"

DEFAULT_APISIX_IMAGE_NAME="apache/apisix"
DEFAULT_APISIX_IMAGE_TAG="3.11.0-debian"

DEFAULT_ETCD_LISTEN_PORT=2379
DEFAULT_APISIX_PORT=9180

DEFAULT_ETCD_NAME="etcd-quickstart"
DEFAULT_APP_NAME="apisix-quickstart"
DEFAULT_NET_NAME="apisix-quickstart-net"

usage() {
  echo "Runs a Docker based Apache APISIX."
  echo
  echo "See the document for more information:"
  echo "  https://docs.api7.ai/apisix/getting-started"
  exit 0
}

echo_fail() {
  printf "\e[31m✘ \e[0m$@\n"
}

echo_pass() {
  printf "\e[32m✔ \e[0m$@\n"
}

echo_warning() {
  printf "\e[33m⚠ $@\e[0m\n"
}

ensure_docker() {
  {
    docker ps -q >/dev/null 2>&1
  } || {
    return 1
  }
}

ensure_curl() {
  {
    curl -h >/dev/null 2>&1
  } || {
    return 1
  }
}

install_apisix() {

  echo "Installing APISIX with the quickstart options."
  echo ""

  echo "Creating bridge network ${DEFAULT_NET_NAME}."

  docker network create -d bridge $DEFAULT_NET_NAME && echo_pass "network ${DEFAULT_NET_NAME} created" || {
    echo_fail "Create network failed!"
    return 1
  }

  echo ""

  echo "Starting the container ${DEFAULT_ETCD_NAME}."
  docker run -d \
    --name ${DEFAULT_ETCD_NAME} \
    --network=$DEFAULT_NET_NAME \
    -e ALLOW_NONE_AUTHENTICATION=yes \
    -e ETCD_ADVERTISE_CLIENT_URLS=http://${DEFAULT_ETCD_NAME}:${DEFAULT_ETCD_LISTEN_PORT} \
    ${DEFAULT_ETCD_IMAGE_NAME}:${DEFAULT_ETCD_IMAGE_TAG} && echo_pass "etcd is listening on ${DEFAULT_ETCD_NAME}:${DEFAULT_ETCD_LISTEN_PORT}" || {
    echo_fail "Start etcd failed!"
    return 1
  }

  echo ""

  APISIX_DEPLOYMENT_ETCD_HOST="[\"http://${DEFAULT_ETCD_NAME}:${DEFAULT_ETCD_LISTEN_PORT}\"]"

  echo "Starting the container ${DEFAULT_APP_NAME}."
  docker run -d \
    --name ${DEFAULT_APP_NAME} \
    --network=$DEFAULT_NET_NAME \
    -p9080:9080 -p9180:9180 -p9443:9443/tcp -p9443:9443/udp -p9090:9092 -p9100:9100 -p9091:9091 \
    -e APISIX_DEPLOYMENT_ETCD_HOST=${APISIX_DEPLOYMENT_ETCD_HOST} \
    ${DEFAULT_APISIX_IMAGE_NAME}:${DEFAULT_APISIX_IMAGE_TAG} && validate_apisix && sleep 2 || {
    echo_fail "Start APISIX failed!"
    return 1
  }

  docker exec ${DEFAULT_APP_NAME} /bin/bash -c "echo '
apisix:
  enable_control: true
  control:
    ip: "0.0.0.0"
    port: 9092
deployment:
  role: traditional
  role_traditional:
    config_provider: etcd
  admin:
    admin_key_required: false
    allow_admin:
      - 0.0.0.0/0
plugin_attr:
  prometheus:
    export_addr:
      ip: 0.0.0.0
      port: 9091
  ' > /usr/local/apisix/conf/config.yaml"
  docker exec ${DEFAULT_APP_NAME} apisix reload >>/dev/null 2>&1

  echo_warning "WARNING: The Admin API key is currently disabled. You should turn on admin_key_required and set a strong Admin API key in production for security."

  echo ""
}

destroy_apisix() {
  echo "Destroying existing ${DEFAULT_APP_NAME} container, if any."
  echo ""
  docker rm -f $DEFAULT_APP_NAME >>/dev/null 2>&1
  docker rm -f $DEFAULT_ETCD_NAME >>/dev/null 2>&1
  docker network rm $DEFAULT_NET_NAME >>/dev/null 2>&1
  sleep 2
}

validate_apisix() {
  local rv=0
  retry 30 curl "http://localhost:${DEFAULT_APISIX_PORT}/apisix/admin/services" >>/dev/null 2>&1 && echo_pass "APISIX is up" || rv=$?
}

main() {
  ensure_docker || {
    echo_fail "Docker is not available, please install it first"
    exit 1
  }

  ensure_curl || {
    echo_fail "curl is not available, please install it first"
    exit 1
  }

  destroy_apisix

  install_apisix || {
    exit 1
  }

  echo_pass "APISIX is ready!"
}

main "$@"
```

**Docker Compose 安装**

```
https://apisix.apache.org/docs/apisix/installation-guide/#installing-apisix
```





#### **安装APISIX Dashboard**

通过Dashboard，我们能够:

- 创建并管理服务、上游、路由、应用、证书
- 全局插件等模块
- 配合内置的80多种插件，可精细化控制流量
- 支持配置自定义插件
- 围绕API全生命周期管理提供解决方案，通过持续地更新、升级，更好地为用户赋能
- 兼容OpenAPI 3.0，支持路由的导入、导出

![image-20250313154600565](5day-png/35APISIX Dashboar.png)

```
https://apisix.apache.org/zh/docs/dashboard/install/
```

```bash
docker pull apache/apisix-dashboard
docker run -d --name dashboard \
           -p 9000:9000        \
           -v <CONFIG_FILE>:/usr/local/apisix-dashboard/conf/conf.yaml \
           apache/apisix-dashboard
```

**Docker** **安装**

```
https://apisix.apache.org/zh/docs/dashboard/install/#docker
https://hub.docker.com/r/apache/apisix-dashboard
```

```bash
[root@ubuntu2404 ~]#docker pull apache/apisix-dashboard:3.0.1-alpine

#使用宿主机网络启动，连接宿主机的ETCD
[root@ubuntu2404 ~]#docker run -d --network host --name apisix-dashboard apache/apisix-dashboard:3.0.1-alpine
```

```bash
[root@ubuntu2404 ~]#ss -tnulp
tcp    LISTEN  0       4096                                    *:9000                 *:*      users:(("manager-api",pid=2606,fd=10))   
```

```bash
#默认不允许远程访问apix dashboard，只允许本地127.0.0.1访问
#生成dashboard配置文件
#从容器出提取conf.yaml文件
[root@ubuntu2404 ~]#docker cp apisix-dashboard:/usr/local/apisix-dashboard/conf/conf.yaml .


#修改dashboard配置文件
[root@ubuntu2204 ~]#vim conf.yaml
......
conf:
 listen:
   host: 0.0.0.0       #`manager api` listening ip or host name
   port: 9000          #`manager api` listening port
 allow_list:           #If we don't set any IP list, then any IP access is allowed by default.
    - 0.0.0.0/0        #开放远程访问
 etcd:
   endpoints:          #supports defining multiple etcd host addresses for an etcd cluster
      #- "http://etcd:2379"     #原内容
      - "http://127.0.0.1:2379" #修改此行
      
[root@ubuntu2404 ~]#docker rm -f apisix-dashboard      
[root@ubuntu2404 ~]#docker run -d --network host -v ./conf.yaml:/usr/local/apisix-dashboard/conf/conf.yaml --name apisix-dashboard apache/apisix-dashboard:3.0.1-alpine
```

```
#用户名密码admin/admin
http://10.0.0.200:9000
```



**基于APISIX Dashboard管理路由**

```
https://httpbin.org
```

```bash
[root@ubuntu2404 ~]#docker pull kennethreitz/httpbin
[root@ubuntu2404 ~]#docker run -d --name httpbin -p 80:80 kennethreitz/httpbin
```



### **APISIX API** **访问**

```
https://apisix.apache.org/zh/docs/apisix/admin-api
```

请求 URL 由以下参数构成：

- Protocol：即网络传输协议，在示例中，我们使用的是 HTTP 协议。
- Port：即端口，示例中使用的 80 端口。
- Host：即主机地址，示例中使用的是 httpbin.org 。
- Path：即路径，示例中的路径是 /get 。
- Query Parameters：即查询字符串，这里有两个字符串，分别是 foo1 和 foo2 。

#### **管理路由** **Route**

```
https://apisix.apache.org/zh/docs/apisix/admin-api/#route
```

#### **管理上游** **Upstream**

```
https://apisix.apache.org/zh/docs/apisix/admin-api/#upstream
```



### APISIX连接nacos

在apisix的配置文件conf/config.yaml 中添加以下配置:

```yaml

discovery:
  nacos:
    host:
      - "http://${username}:${password}@${host1}:${port1}"
    prefix: "/nacos/v1/"
    fetch_interval: 30  # 默认 30 秒
    weight: 100         # 默认 100
    timeout:
      connect: 2000  # 默认 2000ms
      send: 2000     # 默认 2000ms
      read: 5000     # 默认 5000ms

# 简化配置 (使用默认配置)
discovery:
  nacos:
    host:
      - "http://10.0.0.200:8848"

```



## 高性能分布式对象存储 MINIO

### 以前存储方法

| 存储类型                           | 解释                                                     | 适用场景                             |
| ---------------------------------- | -------------------------------------------------------- | ------------------------------------ |
| **DAS (Direct Attached Storage)**  | 直接连接到服务器的存储设备，比如硬盘、SSD、RAID 磁盘阵列 | 适用于单台服务器，高性能但不易扩展   |
| **NAS (Network Attached Storage)** | 通过网络（如 NFS、SMB）提供文件存储，多个客户端可以访问  | 适用于文件共享，如企业文档存储       |
| **SAN (Storage Area Network)**     | 通过光纤通道（FC）或 iSCSI 连接服务器，提供块级存储      | 适用于数据库、高性能计算、虚拟化环境 |

------

#### **详细解析**

##### **1. DAS（直接附加存储）**

- 特点：
  - 直接连接到服务器，像本地硬盘一样使用
  - 高速、低延迟
  - 适用于单个服务器，扩展性较差
- 示例：
  - 本地 HDD/SSD
  - RAID 存储阵列
  - USB 移动硬盘

##### **2. NAS（网络附加存储）**

- 特点：
  - 通过 TCP/IP 网络提供文件共享
  - 适用于多用户访问
  - 使用 **NFS、SMB、CIFS** 等协议
  - 扩展性较好，但性能受网络带宽限制
- 示例：
  - 共享文件服务器（Synology、QNAP）
  - 企业级 NAS 设备（NetApp、Dell EMC）

##### **3. SAN（存储区域网络）**

- 特点：
  - 提供 **块级存储**，主机视其为本地磁盘
  - 高吞吐、低延迟，适合高性能应用
  - 通过 **FC（光纤通道）或 iSCSI** 连接
  - 适用于数据中心、大型企业
- 示例：
  - IBM、HPE、Dell EMC 的企业级 SAN 解决方案
  - 采用 **FC 交换机 + 存储阵列**

------

#### **DAS vs. NAS vs. SAN 对比**

| 特性         | **DAS**          | **NAS**            | **SAN**              |
| ------------ | ---------------- | ------------------ | -------------------- |
| **存储方式** | 直接连接         | 通过网络访问       | 通过专用存储网络访问 |
| **协议**     | SATA, SAS        | NFS, SMB, CIFS     | FC, iSCSI            |
| **性能**     | 高               | 中等，受网络影响   | 最高                 |
| **可扩展性** | 差               | 好                 | 非常好               |
| **适用场景** | 单服务器，低成本 | 文件共享，企业办公 | 高性能数据库，虚拟化 |

------

#### **如何选择？**

- **个人或小型服务器** ➝ **DAS**
- **企业文件共享** ➝ **NAS**
- **数据中心、数据库、虚拟化（VMware、KVM）** ➝ **SAN**

### **OS Object storage** **对象存储**

**1** **什么是** **Object storage**

对象存储（Object Storage）是一种用于存储和管理海量非结构化数据的存储架构，常用于存放图片、视频、备份文件、日志等大文件。与传统的块存储和文件存储不同，对象存储将数据作为**对象（Object）**来管理，而不是文件系统中的文件或块设备中的块。每个对象包含：

1. **数据**：需要存储的内容（例如一张图片或一个视频）。
2. **元数据**：关于这个数据的描述性信息，比如创建时间、文件类型等。
3. **唯一的标识符**：每个对象都有一个唯一的ID，通过它可以直接访问该对象。

**OS** **对象存储应用场景**

当前互联网上有海量的非结构化数据的需要存储,如下数据

- 电商网站: 海量商品图片
- 视频网站: 海量视频文件
- 音乐网站: 海量音频文件
- 社交网站: 海量图片
- 云平台: 大量的虚拟机镜像文件
- 网盘: 海量文件

**对象存储服务** **OSS (Object Storage Service )** **特点**

- 对象存储是一种分布式、基于Restful方式操作的存储服务
- 数据作为单独的对象进行存储
- 水平扩展性：对象存储的架构能够轻松扩展，支持存储海量数据，在上传大文件时，OSS会采用分片上传, 通常用于云存储服务（如Amazon S3、Google Cloud Storage等）
- 高可用性和持久性：对象存储的数据被自动复制并分布在多个物理位置，减少数据丢失的可能。
- 无层级结构：与文件存储的层次目录结构不同，对象存储是扁平结构，不直接使用目录树，所有对象存放在同一个命名空间中，可以通过其唯一ID直接访问。可以在互联网任何位置通过URL方式对每个对象进行存储和访问,但不支持挂载
- 适合大文件存储：适合存储非结构化数据﹔比如︰视频、图片、日志、文本文件等，通常用于云计算环境中
- 适合读多写少的场景

**OSS** **分类**

分为公有云的OSS服务和私有云的OSS服务

- 公有云: 阿里云的OSS，腾讯云的COS，天翼云的OSS，亚马逊的S3(2006年3月)，Microsoft Azure Blob存储等公有云提供OSS服务
- 私有云: MinIO, Ceph RGW

```
https://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/file-manage-files/zh-CN/20220727/ttob/OSS%E6%96%B0%E7%89%88%E4%BA%A7%E5%93%81%E4%BB%8B%E7%BB%8D%EF%BC%88%E4%B8%AD%E6%96%87%EF%BC%89.mp4
```

### **MinIO介绍**

```
https://min.io/
http://minio.org.cn/
https://github.com/minio/minio
```

**MinIO** **特点**

- MinIO 提供高性能、与S3 兼容的对象存储系统，让你自己能够构建自己的云储存服务。
- MinIO 使用和部署非常简单，可以让您在最快的时间内实现下载到生产环境的部署。
- MinIO 读写性能优异,高性能MinIO 是世界上最快的对象存储，没有之一。在 32 个 NVMe 驱动器节点和 100Gbe 网络上发布的 GET/PUT 结果超过 325 GiB/秒和 165 GiB/秒
- MinIO 支持的对象文件小到几kb到最大5T,并实现了数据的高可用
- MinIO 原生支持 Kubernetes，它可用于每个独立的公共云、每个 Kubernetes 发行版、私有云和边缘的对象存储套件。
- MinIO 是软件定义的，不需要购买其他任何硬件，在 GNU AGPL v3 下是 100% 开源的

### minio性能

```
https://blog.min.io/scaling-minio-more-hardware-for-higher-scale/
```

### **MinIO** **工作机制**

#### **MinIO** **相关术语**

- 0bject 对象

  存储到Minio的基本对象,如文件、字节流等任意数据

  一个文件就是一个对象

- Bucket 桶

  用来存储Object的逻辑空间。

  每个Bucket之间的数据是相互隔离的。

  对于客户端而言，就相当于一个存放文件的顶层文件夹，用于实现不同资源的分类存储

  通常一个项目或同一类资源可以对应于一个Bucket

- Drive 驱动器

  即存储数据的磁盘,一个Drive通常对应一块物理磁盘或者一个独立目录

  在MinIO启动时，以参数的方式传入。

  Minio 中所有的对象数据都会存储在Drive里

- Set 存储集

  set 是一组Drive的集合,即一组相关的磁盘的集合

  分布式部署时,MinIO会根据集群规模自动划分一个或多个Set，每个Set中的Drive分布在不同位置。

  一个对象存储在一个Set上

  一个集群划分为多个Set

  一个Set包含的Drive数量是固定的， 默认由系统根据集群规模自动计算得出

  一个Set中的Drive尽可能分布在不同的节点上

**纠删码EC (Erasure Code)**

```
https://blog.min.io/erasure-coding/
https://min.io/docs/minio/linux/operations/concepts/erasure-coding.html
https://www.minio.org.cn/docs/minio/kubernetes/upstream/operations/concepts.html#id11
```

分布式存储，很关键的点在于数据的可靠性，即保证数据的完整，不丢失，不损坏。只有在可靠性实现的前提下，才有了追求一-致性、高可用、高性能的基础。而对于在存储领域，一 般对于保证数据可靠性的方法主要有两类，一类是冗余法，一 类是校验法。

分布式存储可靠性常用方法

- 冗余法

  冗余法即对存储的数据进行副本备份，当数据出现丢失，损坏，即可使用备份内容进行恢复，而副本备份的多少，决定了数据可靠性的高低。这其中会有成本的考量，副本数据越多，数据越可靠，但需要的设备就越多，成本就越高。可靠性是允许丢失其中一份数据。

  当前有很多分布式系统采用此种方式实现，如：ELasticsearch的索引副本，Kafka的副本，Redis的集群，MySQL的主从模式，Hadoop的多副本的文件系统

- 校验法

  校验法即通过校验码的数学计算的方式，对出现丢失、损坏的数据可以实现校验和还原两个功能

  通过对数据进行校验和( checksum )进行计算，可以检查数据是否完整，有无损坏或更改，在数据传输和保存时经常用到，如TCP协议

  恢复还原，通过对数据结合校验码进行数学计算，还原丢失或损坏的数据，可以在保证数据可靠的前提下，降低冗余，如单机硬盘存储中的RAID技术，纠删码(Erasure Code)技术等。

  MinlO采用的就是纠删码技术。

MinIO 纠删码EC (Erasure Code) 是一种提供数据高可用性功能，允许具有多个驱动器的 MinIO 部署即时自动重建对象，即使群集中丢失了多个驱动器或节点。 纠删码提供了对象级修复

Minio使用纠删码erasure code 与校验和checksum来保护数据免受硬件故障和无声数据损坏。即便您丢失一 半数量(N/2) 的硬盘,您仍然可以恢复数据。

纠删码是一种恢复铁和损坏数据的数学算法，Minio采用Reed-Solomon code将对象拆分成N/2数据和N/2奇偶校验块。

**当损坏总磁盘数的一半磁盘时,只能读取而不能上传新的文件,只要保证正常磁盘数大于等n/2+1时,就可以支持写入新数据**

这就意味着如果是12块盘，一个对象会被分成6个数据块、6个奇偶校验块，你可以失任意6块盘(不管其是存放的数据块还是奇偶校验块)，你仍可以从剩下的盘中的数据进行恢复。

**实现纠删码EC至少需要4块Drive磁盘以上**

```
https://min.io/docs/minio/linux/operations/concepts/erasure-coding.html
```

纠删码计算器

```
https://min.io/product/erasure-code-calculator
```

#### **MinIO** **工作流程**

![image-20250313175736072](5day-png/35MINIO工作流程.png)

#### **MinIO** **启动模式**

![image-20250313175826263](5day-png/35Minio启动模式.png)





### 单机部署

```
https://min.io/docs/minio/linux/operations/install-deploy-manage/deploy-minio-single-node-single-drive.html
https://min.io/docs/minio/linux/operations/install-deploy-manage/deploy-minio-single-node-multi-drive.html
https://min.io/docs/minio/linux/index.html
https://www.minio.org.cn/docs/minio/linux/index.html
```

在standalone模式下，还可以分为non-erasure code mode和erasure code mode(纠删码)。

- non-erasure code mode

  当minio server 运行时只传入一个本地磁盘参数。即为 non-erasure code mode在此启动模式下，对于每一份对象数据， minio直接存储这份数据，不会建立副本，也不会启用纠删码机制。

  因此,这种模式无论是服务实例还是磁盘都是”单点"，无任何高可用保障，磁盘损坏就意味着数据丢失。

- erasure code mode

  此模式需要为minio server实例传入多个本地磁盘参数。

  一旦遇到多于一 个磁盘的参数， minio server会自动启用erasure code mode.

  erasure code对磁盘的个数是有要求的，至少4个磁盘, 如不满足要求，实例启动将失败。

  erasure code启用后，要求传给minio server的endpoint(standalone模式下，即本地磁盘上的目录)至少为4个。

##### **包安装**

```
https://min.io/open-source/download?platform=linux
```

```bash
wget https://dl.min.io/server/minio/release/linux-amd64/minio_20250312180418.0.0_amd64.deb
dpkg -i minio_20250312180418.0.0_amd64.deb
MINIO_ROOT_USER=admin MINIO_ROOT_PASSWORD=password minio server /mnt/data --console-address ":9001"
```

```bash
[root@ubuntu2404 ~]#wget https://dl.min.io/server/minio/release/linux-amd64/minio_20250312180418.0.0_amd64.deb
[root@ubuntu2404 ~]#dpkg -i minio_20250312180418.0.0_amd64.deb 

#service文件
[root@ubuntu2404 ~]#cat /lib/systemd/system/minio.service
[Unit]
Description=MinIO
Documentation=https://docs.min.io
Wants=network-online.target
After=network-online.target
AssertFileIsExecutable=/usr/local/bin/minio

[Service]
Type=notify

WorkingDirectory=/usr/local

User=minio-user
Group=minio-user
ProtectProc=invisible

EnvironmentFile=-/etc/default/minio
ExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES

# Let systemd restart this service always
Restart=always

# Specifies the maximum file descriptor number that can be opened by this process
LimitNOFILE=1048576

# Turn-off memory accounting by systemd, which is buggy.
MemoryAccounting=no

# Specifies the maximum number of threads this process can create
TasksMax=infinity

# Disable timeout logic and wait until process is stopped
TimeoutSec=infinity

# Disable killing of MinIO by the kernel's OOM killer
OOMScoreAdjust=-1000

SendSIGKILL=no

[Install]
WantedBy=multi-user.target

# Built for ${project.name}-${project.version} (${project.name})


#默认没有创建启动用户
[root@ubuntu2404 ~]#useradd -r -s /sbin/nologin minio-user

[root@ubuntu2404 ~]#vim /etc/default/minio
MINIO_VOLUMES=/data/minio
MINIO_CONSOLE_ADDRESS=0.0.0.0:9001		#默认使用随机端口，可以访问9000进行跳转到此随机端口
#MINIO_OPTS='--console-address :9001'	#默认使用随机端口，可以访问9000进行跳转到此随机端口
MINIO_ROOT_USER=admin					#默认minioadmin
MINIO_ROOT_PASSWORD=12345678			#默认minioadmin，密码不能低于8位
[root@ubuntu2404 ~]#mkdir /data/minio -p
[root@ubuntu2404 ~]#chown -R minio-user: /data/minio/

[root@ubuntu2404 ~]#systemctl enable --now minio.service 
[root@ubuntu2404 ~]#ss -tnulp | grep minio
tcp   LISTEN 0      4096       127.0.0.1:9000      0.0.0.0:*    users:(("minio",pid=1473,fd=8))                       
tcp   LISTEN 0      4096         0.0.0.0:9001      0.0.0.0:*    users:(("minio",pid=1473,fd=10))                      
tcp   LISTEN 0      4096               *:9000            *:*    users:(("minio",pid=1473,fd=7))                       
tcp   LISTEN 0      4096           [::1]:9000         [::]:*    users:(("minio",pid=1473,fd=9))     
```

##### **二进制安装**

```
https://dl.min.io/enterprise/minio/release/
```

```bash
wget https://dl.min.io/server/minio/release/linux-amd64/minio
chmod +x minio
MINIO_ROOT_USER=admin MINIO_ROOT_PASSWORD=password ./minio server /mnt/data --console-address ":9001"
```

```bash
[root@ubuntu2404 ~]#wget https://dl.min.io/server/minio/release/linux-amd64/minio
[root@ubuntu2404 ~]#mv minio /usr/local/bin/
[root@ubuntu2404 ~]#chmod +x /usr/local/bin/minio 

#编写service文件
[root@ubuntu2404 ~]#vim /lib/systemd/system/minio.service
[Unit]
Description=MinIO
Documentation=https://docs.min.io
Wants=network-online.target
After=network-online.target
AssertFileIsExecutable=/usr/local/bin/minio

[Service]
Type=notify

WorkingDirectory=/usr/local

User=minio
Group=minio
ProtectProc=invisible

EnvironmentFile=-/etc/default/minio
ExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES

# Let systemd restart this service always
Restart=always

# Specifies the maximum file descriptor number that can be opened by this process
LimitNOFILE=1048576

# Turn-off memory accounting by systemd, which is buggy.
MemoryAccounting=no

# Specifies the maximum number of threads this process can create
TasksMax=infinity

# Disable timeout logic and wait until process is stopped
TimeoutSec=infinity

# Disable killing of MinIO by the kernel's OOM killer
OOMScoreAdjust=-1000

SendSIGKILL=no

[Install]
WantedBy=multi-user.target

# Built for ${project.name}-${project.version} (${project.name})

[root@ubuntu2404 ~]#vim /etc/default/minio
MINIO_VOLUMES=/data/minio
MINIO_CONSOLE_ADDRESS=0.0.0.0:9001
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=12345678

[root@ubuntu2404 ~]#useradd -r -s /sbin/nologin minio
[root@ubuntu2404 ~]#mkdir /data/minio -p
[root@ubuntu2404 ~]#chown -R minio: /data/minio/

[root@ubuntu2404 ~]#systemctl daemon-reload 
[root@ubuntu2404 ~]#systemctl restart minio.service
```



##### **脚本二进制安装**

```bash
[root@ubuntu2404 ~]#cat install_minio_single_node.sh 
#!/bin/bash
#
#支持在线和离线基于二进制安装MinIO


#管理后台用户和密码,注意:用户名至少3个字符,密码至少8个字符,如果不指定下面环境变量，默认用户名/密码为minioadmin/minioadmin
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=12345678

#管理后台监听端口
MINIO_PORT=9999

#数据目录
MINIO_DATA=/data/minio

#文件名和下载URL
MINIO_FILE=minio
#MINIO_FILE=minio.RELEASE.2025-03-12T18-04-18Z
#MINIO_FILE=minio.RELEASE.2024-05-01T01-11-10Z
#MINIO_FILE=minio_20230829230735.0.0_amd64

MINIO_URL="https://dl.minio.org.cn/server/minio/release/linux-amd64/${MINIO_FILE}"
#MINIO_URL="https://dl.min.io/server/minio/release/linux-amd64/${MINIO_FILE}"


HOST_IP=`hostname -I|awk '{print $1}'`



COLOR_SUCCESS="echo -e \\033[1;32m"
COLOR_FAILURE="echo -e \\033[1;31m"
END="\033[m"


color () {
    RES_COL=60
    MOVE_TO_COL="echo -en \\033[${RES_COL}G"
    SETCOLOR_SUCCESS="echo -en \\033[1;32m"
    SETCOLOR_FAILURE="echo -en \\033[1;31m"
    SETCOLOR_WARNING="echo -en \\033[1;33m"
    SETCOLOR_NORMAL="echo -en \E[0m"
    echo -n "$1" && $MOVE_TO_COL
    echo -n "["
    if [ $2 = "success" -o $2 = "0" ] ;then
        ${SETCOLOR_SUCCESS}
        echo -n $"  OK  "    
    elif [ $2 = "failure" -o $2 = "1"  ] ;then 
        ${SETCOLOR_FAILURE}
        echo -n $"FAILED"
    else
        ${SETCOLOR_WARNING}
        echo -n $"WARNING"
    fi
    ${SETCOLOR_NORMAL}
    echo -n "]"
    echo 
}


install_minio(){
    if  [ ! -e ${MINIO_FILE}  ] ;then
        wget ${MINIO_URL} || ${COLOR_FAILURE} "下载失败!" ${END}
    fi
    install ${MINIO_FILE} /usr/local/bin/minio
    id minio &> /dev/null ||  useradd -s /sbin/nologin  -r minio
    mkdir -p ${MINIO_DATA}
    chown -R minio:minio  ${MINIO_DATA}
    cat > /lib/systemd/system/minio.service <<EOF
[Unit]
Description=Minio
After=systemd-networkd.service systemd-resolved.service
Documentation=https://min.io

[Service]
Type=simple
Environment=MINIO_ROOT_USER=${MINIO_ROOT_USER} MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
ExecStart=/usr/local/bin/minio server ${MINIO_DATA} --console-address ":${MINIO_PORT}"
Restart=on-failure
User=minio
Group=minio
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
OOMScoreAdjust=-1000

[Install]
WantedBy=multi-user.target
EOF

    systemctl daemon-reload 
    systemctl enable --now  minio.service &>/dev/null
    systemctl is-active minio.service &>/dev/null
    if [ $?  -eq 0 ];then  
        echo 
        color "MinIO 安装完成!" 0
        echo "-------------------------------------------------------------------"
        echo -e "请访问链接: \E[32;1mhttp://${HOST_IP}:${MINIO_PORT}/\E[0m" 
        echo -e "用户和密码: \E[32;1m${MINIO_ROOT_USER}/${MINIO_ROOT_PASSWORD}\E[0m" 
    else
        color "MinIO 安装失败!" 1
        exit
    fi
}



install_minio
```



##### **docker部署**

```
https://min.io/docs/minio/container/index.html
```

```bash
[root@ubuntu2404 ~]#apt update && apt install docker.io -y

mkdir -p ~/minio/data

docker run \
   -d \
   -p 9000:9000 \
   -p 9001:9001 \
   --name minio \
   -v ~/minio/data:/data \
   -e "MINIO_ROOT_USER=admin" \
   -e "MINIO_ROOT_PASSWORD=12345678" \
   quay.io/minio/minio server /data --console-address ":9001"
```



### 集群部署

**分布式Minio优势**

- 数据保护

  分布式Minio采用纠删码来防范多个节点宕机和位衰减bit rot。

  分布式Minio至少需要4个硬盘，使用分布式Minio 会自动引入纠删码功能。

- 高可用

  单机Minio服务存在单点故障，相反，如果是一个有N块硬盘的分布式Minio,只要有N/2硬盘在线，数据就是安全的。不过你需要至少有N/2+1个硬盘来创建新的对象。

  例如，一个16节点Minio集群，每个节点16块硬盘，就算8台服务器宕机，这个集群仍然可读的，不过需要9台服务器才能写数据。

- 性能

  多个节点负载均衡，提升性能

- 一致性

  Minio在分布式和单机模式下，所有读写操作都严格遵守read-after-write一致性模型。

**分布式Minio实现条件**

- 在所有节点运行同样的命令启动一个分布式Minio实例，只需要把硬盘位置做为参数传给minio server命令即可

- 分布式Minio中的所有节点需要有同样的环境变量MINIO_ACCESS_KEY和MINIO_SECRET_KEY，才能建立分布式集群

  注意：新版本使用环境变量 MINIO_ROOT_USER和MINIO_ROOT_PASSWORD

- 分布式Minio使用的磁盘里必须是干净的，里面没有数据

- 分布式Minio里的节点时间差不能超过3秒，可以使用NTP来保证时间一致。

**分布式Minio注意事项**

- minio服务器多块数据磁盘需要使用独立的磁盘，在物理底层要相互独立避免遇到磁盘io竞争，导致minio性能直线下降,如果性能下降严重，数据量大时甚至会导致集群不可用

- minio数据磁盘最大不超过2T,如果使用lvm逻辑卷，逻辑卷大小也不要超过2T，过大的磁盘或文件系统会导致后期IO延迟较高导致minio性能降低

- minio集群共M节点每个节点N块数据磁盘，磁盘只要存活M*N/2，minio集群数据就是安全的，在节点数剩余M/2+1时节点可以正常读写

- 如果使用lvm方式扩展集群容量，请在部署阶段minio数据目录就使用lvm。

- 如果需要备份minio集群数据，请准备存放minio集群所有对象数据的空间容量

  比如: 一个2T/盘*4块/节点*8节点=64T的集群所有容量的一半存储空间即32T服务器，配置内存CPU配置不需要太高

- 如果网络环境允许请把minio集群节点配置双网卡，节点通信网络与客户端访问网络分开避免网络瓶颈

- 配置反向代理实现MinIO的负载均衡, 可以使用云服务SLB或者2台haproxy/nginx结合keepalived实现高可用

- minio系统中不要安装消耗IO较高的应用,比如:updatedb程序，如安装请排除扫描minio数据目录,否则可以会导致磁盘io延迟过高，会导致cpu负载过高，从而降低minio性能



#### 二进制部署分布式集群

![image-20250315170905205](5day-png/35MINIO集群部署.png)

| IP         |                   |
| ---------- | ----------------- |
| 10.0.0.201 | minio1            |
| 10.0.0.202 | minio2            |
| 10.0.0.203 | minio3            |
| 10.0.0.100 | haproxy+keepalive |
| 10.0.0.101 | haproxy+keepalive |



```bash
[root@minio1 ~]#cat /etc/hosts
10.0.0.201 minio1.kang.org minio1
10.0.0.202 minio2.kang.org minio2
10.0.0.203 minio3.kang.org minio3

[root@minio1 ~]#scp /etc/hosts minio2:/etc/hosts   
[root@minio1 ~]#scp /etc/hosts minio3:/etc/hosts

#创建逻辑卷
[root@minio1 ~]#lvcreate -n minio1 -L 10G ubuntu-vg 
  Logical volume "minio1" created.
[root@minio1 ~]#lvcreate -n minio2 -L 10G ubuntu-vg 
  Logical volume "minio2" created.
[root@minio1 ~]#lvcreate -n minio3 -L 10G ubuntu-vg 
  Logical volume "minio3" created.
[root@minio1 ~]#lvcreate -n minio4 -L 10G ubuntu-vg 
  Logical volume "minio4" created.
[root@minio1 ~]#lvs
  LV        VG        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  minio1    ubuntu-vg -wi-a-----  10.00g                                                    
  minio2    ubuntu-vg -wi-a-----  10.00g                                                    
  minio3    ubuntu-vg -wi-a-----  10.00g                                                    
  minio4    ubuntu-vg -wi-a-----  10.00g                                                    
  ubuntu-lv ubuntu-vg -wi-ao---- <99.00g 
  
[root@minio2 ~]#lvcreate -n minio1 -L 10G ubuntu-vg
  Logical volume "minio1" created.
[root@minio2 ~]#lvcreate -n minio2 -L 10G ubuntu-vg
  Logical volume "minio2" created.
[root@minio2 ~]#lvcreate -n minio3 -L 10G ubuntu-vg
  Logical volume "minio3" created.
[root@minio2 ~]#lvcreate -n minio4 -L 10G ubuntu-vg
  Logical volume "minio4" created.
[root@minio2 ~]#lvs
  LV        VG        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  minio1    ubuntu-vg -wi-a-----  10.00g                                                    
  minio2    ubuntu-vg -wi-a-----  10.00g                                                    
  minio3    ubuntu-vg -wi-a-----  10.00g                                                    
  minio4    ubuntu-vg -wi-a-----  10.00g                                                    
  ubuntu-lv ubuntu-vg -wi-ao---- <99.00g   

[root@minio3 ~]#lvcreate -n minio1 -L 10G ubuntu-vg
  Logical volume "minio1" created.
[root@minio3 ~]#lvcreate -n minio2 -L 10G ubuntu-vg
  Logical volume "minio2" created.
[root@minio3 ~]#lvcreate -n minio3 -L 10G ubuntu-vg
  Logical volume "minio3" created.
[root@minio3 ~]#lvcreate -n minio4 -L 10G ubuntu-vg
  Logical volume "minio4" created.
[root@minio3 ~]#lvs
  LV        VG        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  minio1    ubuntu-vg -wi-a-----  10.00g                                                    
  minio2    ubuntu-vg -wi-a-----  10.00g                                                    
  minio3    ubuntu-vg -wi-a-----  10.00g                                                    
  minio4    ubuntu-vg -wi-a-----  10.00g                                                    
  ubuntu-lv ubuntu-vg -wi-ao---- <99.00g   

#格式化逻辑卷
[root@minio1 ~]#mkfs.xfs /dev/ubuntu-vg/minio1
[root@minio1 ~]#mkfs.xfs /dev/ubuntu-vg/minio2
[root@minio1 ~]#mkfs.xfs /dev/ubuntu-vg/minio3
[root@minio1 ~]#mkfs.xfs /dev/ubuntu-vg/minio4

[root@minio2 ~]#mkfs.xfs /dev/ubuntu-vg/minio1
[root@minio2 ~]#mkfs.xfs /dev/ubuntu-vg/minio2
[root@minio2 ~]#mkfs.xfs /dev/ubuntu-vg/minio3
[root@minio2 ~]#mkfs.xfs /dev/ubuntu-vg/minio4

[root@minio3 ~]#mkfs.xfs /dev/ubuntu-vg/minio1
[root@minio3 ~]#mkfs.xfs /dev/ubuntu-vg/minio2
[root@minio3 ~]#mkfs.xfs /dev/ubuntu-vg/minio3
[root@minio3 ~]#mkfs.xfs /dev/ubuntu-vg/minio4

[root@minio1 ~]#mkdir /data/minio{1..4} -p
[root@minio1 ~]#scp -r /data minio2:/
[root@minio1 ~]#scp -r /data minio3:/

[root@minio1 ~]#vim /etc/fstab
# /etc/fstab: static file system information.
#
# Use 'blkid' to print the universally unique identifier for a
# device; this may be used with UUID= as a more robust way to name devices
# that works even if disks are added and removed. See fstab(5).
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
# / was on /dev/ubuntu-vg/ubuntu-lv during curtin installation
/dev/disk/by-id/dm-uuid-LVM-6ECnNgAqzYKXoqs01uF514wncWM0qTgNBzMHdCon20QExYZ4tNIAeZdBeIvDgbyJ / ext4 defaults 0 1
# /boot was on /dev/sda2 during curtin installation
/dev/disk/by-uuid/bc633fdc-5174-445b-b484-e756d9140d56 /boot ext4 defaults 0 1
/swap.img       none    swap    sw      0       0
/dev/ubuntu-vg/minio1 /data/minio1 xfs defaults 0 0
/dev/ubuntu-vg/minio2 /data/minio2 xfs defaults 0 0
/dev/ubuntu-vg/minio3 /data/minio3 xfs defaults 0 0
/dev/ubuntu-vg/minio4 /data/minio4 xfs defaults 0 0

[root@minio1 ~]#scp /etc/fstab minio2:/etc/fstab   
[root@minio1 ~]#scp /etc/fstab minio3:/etc/fstab

[root@minio1 ~]#chown -R minio: /data/
[root@minio2 ~]#chown -R minio: /data/
[root@minio3 ~]#chown -R minio: /data/

[root@minio1 ~]#mount -a
[root@minio2 ~]#mount -a
[root@minio3 ~]#mount -a 
```

```bash
#使用脚本安装minio
[root@minio1 ~]#bash install_minio_single_node.sh 
[root@minio2 ~]#bash install_minio_single_node.sh 
[root@minio3 ~]#bash install_minio_single_node.sh 

#修改安装的内容
[root@minio1 ~]#vim /etc/default/minio
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=12345678
MINIO_VOLUMES='http://minio{1...3}.kang.org:9000/data/minio{1...4}'
MINIO_OPTS='--console-address :9001'
MINIO_PROMETHEUS_AUTH_TYPE="public"

[root@minio1 ~]#vim /lib/systemd/system/minio.service 
[Unit]
Description=MinIO
Documentation=https://docs.min.io
Wants=network-online.target
After=network-online.target

[Service]
WorkingDirectory=/usr/local
User=minio
Group=minio
EnvironmentFile=-/etc/default/minio

# 确保 MINIO_VOLUMES 变量已设置
ExecStartPre=/bin/bash -c 'if [ -z "$MINIO_VOLUMES" ]; then echo "Variable MINIO_VOLUMES not set in /etc/default/minio"; exit 1; fi'

# 启动 MinIO
ExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES

Restart=always
LimitNOFILE=1048576
TasksMax=infinity
OOMScoreAdjust=-1000

[Install]
WantedBy=multi-user.target

#在所有节点上同步
[root@minio1 ~]#scp /etc/default/minio minio2:/etc/default/minio
[root@minio1 ~]#scp /etc/default/minio minio3:/etc/default/minio
[root@minio1 ~]#scp /lib/systemd/system/minio.service minio2:/lib/systemd/system/minio.service
[root@minio1 ~]#scp /lib/systemd/system/minio.service minio3:/lib/systemd/system/minio.service

[root@minio1 ~]#systemctl daemon-reload && systemctl enable --now minio
[root@minio1 ~]#systemctl status minio.service 
[root@minio2 ~]#systemctl daemon-reload && systemctl enable --now minio
[root@minio2 ~]#systemctl status minio.service 
[root@minio3 ~]#systemctl daemon-reload && systemctl enable --now minio
[root@minio3 ~]#systemctl status minio.service 
```

**配置代理haproxy+keepalived**

```bash
[root@master ~]#apt update && apt install haproxy keepalived -y
[root@slave ~]#apt update && apt install haproxy keepalived -y
[root@master ~]#echo net.ipv4.ip_nonlocal_bind = 1 >> /etc/sysctl.conf
[root@master ~]#sysctl -p
net.ipv4.ip_nonlocal_bind = 1
[root@slave ~]#echo net.ipv4.ip_nonlocal_bind = 1 >> /etc/sysctl.conf
[root@slave ~]#sysctl -p
net.ipv4.ip_nonlocal_bind = 1
```

```bash
[root@master ~]#vim /etc/haproxy/haproxy.cfg
[root@slave ~]#vim /etc/haproxy/haproxy.cfg
#添加下面几行
#方法一
global
    log /dev/log local0
    maxconn 4096
    daemon

defaults
    log global
    mode http
    option httplog
    timeout connect 5s
    timeout client 50s
    timeout server 50s
    retries 3

listen stats
    mode http
    bind 0.0.0.0:9999
    stats enable
    stats uri /haproxy-status
    stats auth admin:123456

listen minio
    bind 10.0.0.10:9000
    mode http
    balance roundrobin
    option httpchk HEAD /minio/health/live HTTP/1.1\r\nHost:\ 10.0.0.10
    http-check expect status 200
    server 10.0.0.201 10.0.0.201:9000 check inter 3000 fall 2 rise 5
    server 10.0.0.202 10.0.0.202:9000 check inter 3000 fall 2 rise 5
    server 10.0.0.203 10.0.0.203:9000 check inter 3000 fall 2 rise 5

listen minio_console
    bind 10.0.0.10:9001
    mode http 
    balance roundrobin
    option httpchk GET /
    server 10.0.0.201 10.0.0.201:9001 check inter 3000 fall 2 rise 5
    server 10.0.0.202 10.0.0.202:9001 check inter 3000 fall 2 rise 5
    server 10.0.0.203 10.0.0.203:9001 check inter 3000 fall 2 rise 5
 
#方法二
listen stats
    mode http
    bind 0.0.0.0:9999
    stats enable
    stats uri /haproxy-status
    stats auth admin:123456

listen minio
    bind 10.0.0.10:9000
    mode http
    log global
    server 10.0.0.201 10.0.0.201:9000 check inter 3000 fall 2 rise 5
    server 10.0.0.202 10.0.0.202:9000 check inter 3000 fall 2 rise 5
    server 10.0.0.203 10.0.0.203:9000 check inter 3000 fall 2 rise 5

listen minio_console
    bind 10.0.0.10:9001
    mode http
    log global
    server 10.0.0.201 10.0.0.201:9001 check inter 3000 fall 2 rise 5
    server 10.0.0.202 10.0.0.202:9001 check inter 3000 fall 2 rise 5
    server 10.0.0.203 10.0.0.203:9001 check inter 3000 fall 2 rise 5
    
[root@master ~]#systemctl reload haproxy
[root@slave ~]#systemctl reload haproxy
```

```bash
#部署keepalived高可用
[root@master ~]#vim /etc/keepalived/keepalived.conf
! Configuration File for Keepalived

global_defs {
    router_id ka1  # 另一台主机应配置为 ka2
}

# 检查 HAProxy 是否运行
vrrp_script chk_haproxy {
    script "killall -0 haproxy"  # 更轻量的方式，比 `pidof` 更快
    interval 1
    weight -30
}

vrrp_instance VI_1 {
    interface eth0
    virtual_router_id 66  # 确保两台主机的 virtual_router_id 一致
    state MASTER  # 另一台主机应配置为 BACKUP
    priority 100   # 另一台主机应配置为 80
    advert_int 1

    authentication {
        auth_type PASS
        auth_pass 123456  # 需确保两台主机的认证信息一致
    }

    virtual_ipaddress {
        10.0.0.10/24 dev eth0 label eth0:1
    }

    track_script {
        chk_haproxy
    }
}

[root@slave ~]#vim /etc/keepalived/keepalived.conf
! Configuration File for Keepalived

global_defs {
    router_id ka2  # 另一台主机应配置为 ka1
}

vrrp_instance VI_1 {
    interface eth0
    virtual_router_id 66  # 确保两台主机的 virtual_router_id 一致
    state BACKUP  # 另一台主机应配置为 MASTER
    priority 80   # 另一台主机应配置为 100
    advert_int 1

    authentication {
        auth_type PASS
        auth_pass 123456  # 需确保两台主机的认证信息一致
    }

    virtual_ipaddress {
        10.0.0.10/24 dev eth0 label eth0:1
    }

    track_script {
        chk_haproxy
    }
}

[root@master ~]#systemctl restart keepalived.service
[root@slave ~]#systemctl restart keepalived.service

#浏览器访问haproxy的管理页,用户名/密码:admin/123456
http://10.0.0.10:9999/haproxy-status

#浏览器访问minio的管理页,用户名/密码:admin/12345678
http://10.0.0.10:9001
```

**客户端连接**

```
access key
sWcwfBpHKuz2l9wdFWDB
secret key
1wNjghGM3eYSP85hfjvEgZBePlhsiCjoE7K4FRlo
```

```bash
[root@ubuntu2404 ~]#mc alias set mycluster http://10.0.0.10:9000 sWcwfBpHKuz2l9wdFWDB 1wNjghGM3eYSP85hfjvEgZBePlhsiCjoE7K4FRlo
Added `mycluster` successfully.
[root@ubuntu2404 ~]#dd if=/dev/zero of=f1.img bs=1M count=1000
[root@ubuntu2404 ~]#mc cp f1.img mycluster/mybucket/ 
/root/f1.img:                     1000.00 MiB / 1000.00 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 177.86 MiB/s 5s

```



```bash
#文件分布式存放
[root@minio1 ~]#tree /data/
/data/
├── minio1
│   └── mybucket
│       └── f1.img
│           ├── 9089c558-5dcd-4fd5-b527-3a204cb7db29
│           │   ├── part.1
│           │   ├── part.10
│           │   ├── part.11
│           │   ├── part.12
│           │   ├── part.13
│           │   ├── ...
│           └── xl.meta
├── minio2
│   └── mybucket
│       └── f1.img
│           ├── 9089c558-5dcd-4fd5-b527-3a204cb7db29
│           │   ├── part.1
│           │   ├── part.10
│           │   ├── part.11
│           │   ├── part.12
│           │   ├── part.13
│           │   ├── part.
│           └── xl.meta
├── minio3
│   └── mybucket
│       └── f1.img
│           ├── 9089c558-5dcd-4fd5-b527-3a204cb7db29
│           │   ├── part.1
│           │   ├── part.10
│           │   ├── part.11
│           │   ├── part.12
│           │   ├── part.13
│           │   ├── part.
│           └── xl.meta
└── minio4
    └── mybucket
        └── f1.img
            ├── 9089c558-5dcd-4fd5-b527-3a204cb7db29
            │   ├── part.1
            │   ├── part.10
            │   ├── part.11
            │   ├── part.12
            │   ├── part.13
            │   ├── part.
            └── xl.meta
            
            
[root@minio1 ~]#du -sh /data/
503M    /data/
[root@minio2 ~]#du -sh /data/
503M    /data/
[root@minio3 ~]#du -sh /data/
503M    /data/
```

**测试**

```bash
[root@minio3 ~]#systemctl stop minio.service 
[root@ubuntu2404 ~]#mc admin info mycluster/ 
●  minio1.kang.org:9000
   Uptime: 1 hour 
   Version: 2025-02-28T09:55:16Z
   Network: 2/3 OK 
   Drives: 4/4 OK 
   Pool: 1

●  minio2.kang.org:9000
   Uptime: 1 hour 
   Version: 2025-02-28T09:55:16Z
   Network: 2/3 OK 
   Drives: 4/4 OK 
   Pool: 1

●  minio3.kang.org:9000
   Uptime: offline
   Drives: 0/4 OK 

┌──────┬──────────────────────┬─────────────────────┬──────────────┐
│ Pool │ Drives Usage         │ Erasure stripe size │ Erasure sets │
│ 1st  │ 3.5% (total: 60 GiB) │ 12                  │ 1            │
└──────┴──────────────────────┴─────────────────────┴──────────────┘

1000 MiB Used, 1 Bucket, 1 Object
1 node offline, 8 drives online, 4 drives offline, EC:4
[root@ubuntu2404 ~]#mc cp mycluster/mybucket/f1.img /opt
...0.0.0.10:9000/mybucket/f1.img: 1000.00 MiB / 1000.00 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 138.34 MiB/s 7s
[root@ubuntu2404 ~]#ll /opt/ -h
total 1001M
drwxr-xr-x  2 root root  4.0K Mar 15 18:01 ./
drwxr-xr-x 23 root root  4.0K Mar 15 14:40 ../
-rw-r--r--  1 root root 1000M Mar 15 18:01 f1.img
```

 



### MINIO控制台使用

#### 创建 Access Key 和 Secret key 用于访问数据的凭据

![image-20250315150148419](5day-png/35MINIO创建访问数据凭证.png)

```
Access Key:
YltMOvYs0hPStO0a7Y5t
Secret Key:
ex9XFzvTRmqx6CzPKmD6f74J60yM1Cuir4B36Tzr
```

### python访问

```bash
[root@ubuntu2404 ~]#apt update && apt install python3-pip -y
[root@ubuntu2404 ~]#python3 -m pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/
Writing to /root/.config/pip/pip.conf
[root@ubuntu2404 ~]#pip3 install minio --break-system-packages


[root@ubuntu2404 ~]#cat access_minio.py 
#!/usr/bin/python3
import os
from minio import Minio
from minio.error import S3Error
#from minio.error import (ResponseError, BucketAlreadyOwnedByYou, BucketAlreadyExists)

# 初始化MinIO客户端
minio_client = Minio(
    endpoint='10.0.0.200:9000',
    access_key="YltMOvYs0hPStO0a7Y5t",
    secret_key="ex9XFzvTRmqx6CzPKmD6f74J60yM1Cuir4B36Tzr",
    secure=False  # 如果使用SSL，请将此项设置为True
)

bucket_name = "mybucket"
object_name = "example-object.txt"
local_file_path = "/var/log/syslog"

try:
    # 检查存储桶是否存在，如果不存在则创建
    if not minio_client.bucket_exists(bucket_name):
        minio_client.make_bucket(bucket_name)
        print(f"Bucket '{bucket_name}' created successfully.")
    else:
        print(f"Bucket '{bucket_name}' already exists.")

    # 上传文件到存储桶
    minio_client.fput_object(bucket_name, object_name, local_file_path)
    print(f"Successfully uploaded '{local_file_path}' to '{object_name}'.")
except ResponseError as e:
    print(f"An error occurred: {e}")

# 下载存储桶中的对象
downloaded_file_path = "example-object2.txt"
try:
    minio_client.fget_object(bucket_name, object_name, downloaded_file_path)
    print(f"Successfully downloaded '{object_name}' to '{downloaded_file_path}'.")
except ResponseError as e:
    print(f"An error occurred while downloading '{object_name}': {e}")


[root@ubuntu2404 ~]#python3 access_minio.py 
Bucket 'mybucket' created successfully.
Successfully uploaded '/var/log/syslog' to 'example-object.txt'.
Successfully downloaded 'example-object.txt' to 'example-object2.txt'.
```



### MINIO 客户端部署

#### 包安装

```
wget https://dl.min.io/client/mc/release/linux-amd64/mcli_20250312172924.0.0_amd64.deb
dpkg -i mcli_20250312172924.0.0_amd64.deb
mcli alias set myminio/ http://MINIO-SERVER MYUSER MYPASSWORD
```

#### 二进制

```
wget https://dl.min.io/client/mc/release/linux-amd64/mc
chmod +x mc
mc alias set myminio/ http://MINIO-SERVER MYUSER MYPASSWORD
```

```bash
[root@ubuntu2404 ~]#wget https://dl.min.io/client/mc/release/linux-amd64/mc
[root@ubuntu2404 ~]#mv mc /usr/local/bin/
[root@ubuntu2404 ~]#chmod +x /usr/local/bin/mc 
```

#### 脚本部署

```bash
[root@ubuntu2404 ~]#cat install_minio_mc.sh 
#!/bin/bash
#支持在线和离线安装

#文件名和下载URL
MINIO_MC_FILE=mc
#MINIO_MC_FILE=mc.RELEASE.2025-03-12T17-29-24Z
#MINIO_MC_FILE=mc.RELEASE.2024-05-09T17-04-24Z
#MINIO_MC_FILE=mc-2023-09-02T21-28-03Z

MINIO_MC_URL="https://dl.minio.org.cn/client/mc/release/linux-amd64/${MINIO_MC_FILE}"
#MINIO_MC_URL="https://dl.min.io/client/mc/release/linux-amd64/${MINIO_MC_FILE}"

COLOR_SUCCESS="echo -e \\033[1;32m"
COLOR_FAILURE="echo -e \\033[1;31m"
END="\033[m"


color () {
    RES_COL=60
    MOVE_TO_COL="echo -en \\033[${RES_COL}G"
    SETCOLOR_SUCCESS="echo -en \\033[1;32m"
    SETCOLOR_FAILURE="echo -en \\033[1;31m"
    SETCOLOR_WARNING="echo -en \\033[1;33m"
    SETCOLOR_NORMAL="echo -en \E[0m"
    echo -n "$1" && $MOVE_TO_COL
    echo -n "["
    if [ $2 = "success" -o $2 = "0" ] ;then
        ${SETCOLOR_SUCCESS}
        echo -n $"  OK  "    
    elif [ $2 = "failure" -o $2 = "1"  ] ;then 
        ${SETCOLOR_FAILURE}
        echo -n $"FAILED"
    else
        ${SETCOLOR_WARNING}
        echo -n $"WARNING"
    fi
    ${SETCOLOR_NORMAL}
    echo -n "]"
    echo 
}


install_minio_mc(){
    if  [ ! -e ${MINIO_MC_FILE}  ] ;then
        wget ${MINIO_MC_URL} || ${COLOR_FAILURE} "下载失败!" ${END}
    fi
    install ${MINIO_MC_FILE} /usr/local/bin/mc
    mc -v 
    if [ $?  -eq 0 ];then  
        echo 
        color "MC 安装完成!" 0
    else
        color "MC 安装失败!" 1
        exit
    fi
}



install_minio_mc
```



### MINIO 客户端使用

```bash
#打开自动补全
[root@ubuntu2404 ~]#mc --autocompletion
mc: Configuration written to `/root/.mc/config.json`. Please update your access credentials.
mc: Successfully created `/root/.mc/share`.
mc: Initialized share uploads `/root/.mc/share/uploads.json` file.
mc: Initialized share downloads `/root/.mc/share/downloads.json` file.

mc: Your shell is set to 'bash', by env var 'SHELL'.
mc: enabled autocompletion in your 'bash' rc file. Please restart your shell.

#自动生成的配置文件
[root@ubuntu2404 ~]# cat /root/.mc/config.json
{
        "version": "10",
        "aliases": {
                "gcs": {
                        "url": "https://storage.googleapis.com",
                        "accessKey": "YOUR-ACCESS-KEY-HERE",
                        "secretKey": "YOUR-SECRET-KEY-HERE",
                        "api": "S3v2",
                        "path": "dns"
                },
                "local": {
                        "url": "http://localhost:9000",
                        "accessKey": "",
                        "secretKey": "",
                        "api": "S3v4",
                        "path": "auto"
                },
                "play": {
                        "url": "https://play.min.io",
                        "accessKey": "Q3AM3UQ867SPQQA43P2F",
                        "secretKey": "zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG",
                        "api": "S3v4",
                        "path": "auto"
                },
                "s3": {
                        "url": "https://s3.amazonaws.com",
                        "accessKey": "YOUR-ACCESS-KEY-HERE",
                        "secretKey": "YOUR-SECRET-KEY-HERE",
                        "api": "S3v4",
                        "path": "dns"
                }
        }
}
```

**常见命令**

```bash
ls 		#列出文件和文件夹
mb  	#创建一个存储桶或一一个文件夹
cat  	#显示文件和对象内容
pipe 	#一个STDIN重定向到一一个对象或者文件或者STDOUT
share 	#生成用于共享的URL
cp  	#拷贝文件和对象
mirror 	#给存储桶和文件夹做镜像
find  	#基于参数查找文件
diff   	#对两个文件夹或者存储桶比较差异
rm   	#删除文件和对象
events 	#管理对象通知
watch 	#监视文件和对象的事件
policy	#管理访问策略
config 	#管理mc配置文件
update 	#检查软件更新
version	#输出版本信息
```

 

```bash
#查看默认连接信息
[root@ubuntu2404 ~]#mc alias ls
gcs  
  URL       : https://storage.googleapis.com
  AccessKey : YOUR-ACCESS-KEY-HERE
  SecretKey : YOUR-SECRET-KEY-HERE
  API       : S3v2
  Path      : dns
  Src       : /root/.mc/config.json

local
  URL       : http://localhost:9000
  AccessKey : 
  SecretKey : 
  API       : 
  Path      : auto
  Src       : /root/.mc/config.json

play 
  URL       : https://play.min.io
  AccessKey : Q3AM3UQ867SPQQA43P2F
  SecretKey : zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG
  API       : S3v4
  Path      : auto
  Src       : /root/.mc/config.json

s3   
  URL       : https://s3.amazonaws.com
  AccessKey : YOUR-ACCESS-KEY-HERE
  SecretKey : YOUR-SECRET-KEY-HERE
  API       : S3v4
  Path      : dns
  Src       : /root/.mc/config.json

```

```bash
#使用key进行连接
[root@ubuntu2404 ~]#mc alias set minio-server http://10.0.0.200:9000 YltMOvYs0hPStO0a7Y5t ex9XFzvTRmqx6CzPKmD6f74J60yM1Cuir4B36Tzr
Added `minio-server` successfully.
[root@ubuntu2404 ~]#mc alias ls
...
minio-server
  URL       : http://10.0.0.200:9000
  AccessKey : YltMOvYs0hPStO0a7Y5t
  SecretKey : ex9XFzvTRmqx6CzPKmD6f74J60yM1Cuir4B36Tzr
  API       : s3v4
  Path      : auto
  Src       : /root/.mc/config.json
...
```

```bash

[root@ubuntu2404 ~]#mc admin info minio-server/ 
●  10.0.0.200:9000
   Uptime: 40 minutes 
   Version: 2025-03-12T18:04:18Z
   Network: 1/1 OK 
   Drives: 1/1 OK 
   Pool: 1

┌──────┬──────────────────────┬─────────────────────┬──────────────┐
│ Pool │ Drives Usage         │ Erasure stripe size │ Erasure sets │
│ 1st  │ 6.4% (total: 92 GiB) │ 1                   │ 1            │
└──────┴──────────────────────┴─────────────────────┴──────────────┘

455 KiB Used, 1 Bucket, 1 Object
1 drive online, 0 drives offline, EC:0
```

```bash
#使用用户名和密码连接
[root@ubuntu2404 ~]#mc alias set minio-server http://10.0.0.200:9000 admin 12345678
Added `minio-server` successfully.
[root@ubuntu2404 ~]#mc admin info minio-server/ 
●  10.0.0.200:9000
   Uptime: 42 minutes 
   Version: 2025-03-12T18:04:18Z
   Network: 1/1 OK 
   Drives: 1/1 OK 
   Pool: 1

┌──────┬──────────────────────┬─────────────────────┬──────────────┐
│ Pool │ Drives Usage         │ Erasure stripe size │ Erasure sets │
│ 1st  │ 6.4% (total: 92 GiB) │ 1                   │ 1            │
└──────┴──────────────────────┴─────────────────────┴──────────────┘

455 KiB Used, 1 Bucket, 1 Object
1 drive online, 0 drives offline, EC:0
```

```bash
#上传文件
[root@ubuntu2404 ~]#dd if=/dev/zero of=f1.img bs=1M count=1
1+0 records in
1+0 records out
1048576 bytes (1.0 MB, 1.0 MiB) copied, 0.000753909 s, 1.4 GB/s
[root@ubuntu2404 ~]#mc cp f1.img play/m61
/root/f1.img:                     1.00 MiB / 1.00 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 321.35 KiB/s 3s[root@ubuntu2404 ~]#
[root@ubuntu2404 ~]#mc ls play/m61
[2025-03-15 15:36:19 CST] 1.0MiB STANDARD f1.img
```

```bash
#创建桶backet
[root@ubuntu2404 ~]#mc mb play/hzk
Bucket created successfully `play/hzk`.
[root@ubuntu2404 ~]#mc ls play/hzk
[root@ubuntu2404 ~]#mc cp f1.img play/hzk
/root/f1.img:                     1.00 MiB / 1.00 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 315.43 KiB/s 3s[root@ubuntu2404 ~]#mc cp f2.img play/hzk
/root/f2.img:                     10.00 MiB / 10.00 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 852.21 KiB/s 12s[root@ubuntu2404 ~]#mc ls play/hzk
[2025-03-15 15:38:52 CST] 1.0MiB STANDARD f1.img
[2025-03-15 15:39:09 CST]  10MiB STANDARD f2.img
```



### **MINIO** **故障恢复**

minio集群有纠删码机制，即使在集群数据盘挂掉一半的情况下，集群中数据也是安全的。

但是如果集群想要正常读写,就需要有N/2+1的节点数才可以正常读写

如果现有minio集群有节点出现故障，就需要更换节点

注意事项

- 如果更换节点旧节点数据量较大，在节点更换时可以正常使用请先备份原有节点数据到新节点，避免同步的数据过多导致网络带宽被占用
- 如果数据量小，可以不进行备份数据，直接进行更换，节点启动完毕会自动同步数据
- 如果节点挂掉时集群还在读写数据，会导致集群挂掉的节点与其他minio节点数据不同，这里在恢复节点后需修复数据（自动修复，无需人为干预）
- 最好部署minio集群时使用hosts文件做地址解析，避免更换节点时修改minio配置文件参数
- 更换节点时需要停止minio集群客户端的读写
- 更换的新节点所有配置信息要和旧节点保持一致，包括minio版本，配置文件，hosts解析文件，数据目录位置以及大小

**停止minio3,其他节点还可以正常上传下载**

```bash
[root@minio3 ~]#systemctl stop minio.service 
[root@ubuntu2404 ~]#mc admin info mycluster/ 
●  minio1.kang.org:9000
   Uptime: 2 hours 
   Version: 2025-02-28T09:55:16Z
   Network: 2/3 OK 
   Drives: 4/4 OK 
   Pool: 1

●  minio2.kang.org:9000
   Uptime: 2 hours 
   Version: 2025-02-28T09:55:16Z
   Network: 2/3 OK 
   Drives: 4/4 OK 
   Pool: 1

●  minio3.kang.org:9000
   Uptime: offline
   Drives: 0/4 OK 

┌──────┬──────────────────────┬─────────────────────┬──────────────┐
│ Pool │ Drives Usage         │ Erasure stripe size │ Erasure sets │
│ 1st  │ 3.5% (total: 60 GiB) │ 12                  │ 1            │
└──────┴──────────────────────┴─────────────────────┴──────────────┘

1000 MiB Used, 1 Bucket, 1 Object
1 node offline, 8 drives online, 4 drives offline, EC:4

[root@ubuntu2404 ~]#mc cp f3.img mycluster/mybucket/
/root/f3.img:                     1000.00 MiB / 1000.00 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 207.21 MiB/s 4s
[root@ubuntu2404 ~]#mc cp mycluster/mybucket/f2.img /srv/ 
...0.0.0.10:9000/mybucket/f2.img: 1000.00 MiB / 1000.00 MiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 182.24 MiB/s 5s
```

**把minio2替换为新的主机10.0.0.204**

```powershell
注意：minio的service文件以域名启动
[root@minio2 ~]#systemctl stop  minio.service 
1 新打开一个主机和上面集群创建操作一样
2 把host文件修改，同步到minio1和minio2
3 修改haproxy，替换minio2地址为10.0.0.204
4 重启haproxy、minio1和minio3，启动10.0.0.204
[root@ubuntu2404 ~]#systemctl daemon-reload && systemctl enable --now minio
```

**把minio2恢复**

```powershell
注意：minio的service文件以域名启动
1 先停止10.0.0.204 （注意：先停止10.0.0.204）
[root@ubuntu2404 ~]#systemctl stop minio.service 
2 修改minio1 minio3 的hosts文件，修改10.0.0.204 的地址为minio2的地址
3 修改haproxy 把10.0.0.204 修改为minio2的地址
4 重启haproxy、minio1、minio2和minio3
```



### **MINIO** **扩容和缩容**

#### **扩容**

通常当MinIO容量使用到70%时,建议考虑进行扩容。

扩容有两种方案

- 通过LVM逻辑卷扩容,前提安装时事先使用LVM
- 通过一个相同规格的集群扩容

使用lvm逻辑卷进行扩容minio集群，通过在原有节点增加硬盘来进行扩容minio集群

如果通过lvm逻辑卷扩容，部署原有集群时,minio数据目录需要使用lvm逻辑卷

lvm逻辑卷单个的大小不要超过2T，文件系统过大会导致minio集群IO降低

例如：部署原有MinIO集群时使用4块1T的磁盘使用逻辑卷的方式部署四个数据目录，之后扩容时再增加

4块1T新的磁盘

把四个数据目录扩容到2T，相应的minio集群容量也会扩容1倍

##### **LVM 逻辑卷扩容**

```bash
#添加新磁盘到pv
#pvcreate /dev/sdc

#把pv添加到卷组
#vgextend minio /dev/sdc

#扩容逻辑卷
lvresize -r -l +100%FREE /dev/ubuntu-vg/minio
```

```bash
[root@ubuntu2404 ~]#mc admin info mycluster/ 
●  minio1.kang.org:9000
   Uptime: 20 minutes 
   Version: 2025-02-28T09:55:16Z
   Network: 3/3 OK 
   Drives: 4/4 OK 
   Pool: 1

●  minio2.kang.org:9000
   Uptime: 20 minutes 
   Version: 2025-02-28T09:55:16Z
   Network: 3/3 OK 
   Drives: 4/4 OK 
   Pool: 1

●  minio3.kang.org:9000
   Uptime: 17 minutes 
   Version: 2025-02-28T09:55:16Z
   Network: 3/3 OK 
   Drives: 4/4 OK 
   Pool: 1

┌──────┬──────────────────────┬─────────────────────┬──────────────┐
│ Pool │ Drives Usage         │ Erasure stripe size │ Erasure sets │
│ 1st  │ 6.1% (total: 80 GiB) │ 12                  │ 1            │
└──────┴──────────────────────┴─────────────────────┴──────────────┘

2.9 GiB Used, 1 Bucket, 3 Objects
12 drives online, 0 drives offline, EC:4

[root@minio1 ~]#lvresize -r -L +10G /dev/ubuntu-vg/minio1
[root@minio1 ~]#lvresize -r -L +10G /dev/ubuntu-vg/minio2
[root@minio1 ~]#lvresize -r -L +10G /dev/ubuntu-vg/minio3
[root@minio1 ~]#lvresize -r -L +10G /dev/ubuntu-vg/minio4
[root@minio1 ~]#df -h
Filesystem                         Size  Used Avail Use% Mounted on
tmpfs                              192M 1004K  192M   1% /run
/dev/mapper/ubuntu--vg-ubuntu--lv   97G  6.4G   86G   7% /
tmpfs                              960M     0  960M   0% /dev/shm
tmpfs                              5.0M     0  5.0M   0% /run/lock
/dev/sda2                          2.0G   95M  1.7G   6% /boot
tmpfs                              192M  8.0K  192M   1% /run/user/0
/dev/mapper/ubuntu--vg-minio1       20G  800M   20G   4% /data/minio1
/dev/mapper/ubuntu--vg-minio2       20G  800M   20G   4% /data/minio2
/dev/mapper/ubuntu--vg-minio3       20G  800M   20G   4% /data/minio3
/dev/mapper/ubuntu--vg-minio4       20G  800M   20G   4% /data/minio4

[root@minio2 ~]#lvresize -r -L +10G /dev/ubuntu-vg/minio1
[root@minio2 ~]#lvresize -r -L +10G /dev/ubuntu-vg/minio2
[root@minio2 ~]#lvresize -r -L +10G /dev/ubuntu-vg/minio3
[root@minio2 ~]#lvresize -r -L +10G /dev/ubuntu-vg/minio4
[root@minio2 ~]#df -h
Filesystem                         Size  Used Avail Use% Mounted on
tmpfs                              193M  996K  192M   1% /run
/dev/mapper/ubuntu--vg-ubuntu--lv   97G  5.5G   87G   6% /
tmpfs                              960M     0  960M   0% /dev/shm
tmpfs                              5.0M     0  5.0M   0% /run/lock
/dev/sda2                          2.0G   95M  1.7G   6% /boot
tmpfs                              192M  8.0K  192M   1% /run/user/0
/dev/mapper/ubuntu--vg-minio1       20G  800M   20G   4% /data/minio1
/dev/mapper/ubuntu--vg-minio2       20G  800M   20G   4% /data/minio2
/dev/mapper/ubuntu--vg-minio3       20G  800M   20G   4% /data/minio3
/dev/mapper/ubuntu--vg-minio4       20G  800M   20G   4% /data/minio4

[root@minio3 ~]#lvresize -r -L +10G /dev/ubuntu-vg/minio1
[root@minio3 ~]#lvresize -r -L +10G /dev/ubuntu-vg/minio2
[root@minio3 ~]#lvresize -r -L +10G /dev/ubuntu-vg/minio3
[root@minio3 ~]#lvresize -r -L +10G /dev/ubuntu-vg/minio4
[root@minio3 ~]#df -h
Filesystem                         Size  Used Avail Use% Mounted on
tmpfs                              192M 1004K  192M   1% /run
/dev/mapper/ubuntu--vg-ubuntu--lv   97G  6.4G   86G   7% /
tmpfs                              960M     0  960M   0% /dev/shm
tmpfs                              5.0M     0  5.0M   0% /run/lock
/dev/sda2                          2.0G   95M  1.7G   6% /boot
tmpfs                              192M  8.0K  192M   1% /run/user/0
/dev/mapper/ubuntu--vg-minio1       20G  857M   20G   5% /data/minio1
/dev/mapper/ubuntu--vg-minio2       20G  857M   20G   5% /data/minio2
/dev/mapper/ubuntu--vg-minio3       20G  856M   20G   5% /data/minio3
/dev/mapper/ubuntu--vg-minio4       20G  856M   20G   5% /data/minio4

[root@ubuntu2404 ~]#mc admin info mycluster/ 
●  minio1.kang.org:9000
   Uptime: 30 minutes 
   Version: 2025-02-28T09:55:16Z
   Network: 3/3 OK 
   Drives: 4/4 OK 
   Pool: 1

●  minio2.kang.org:9000
   Uptime: 30 minutes 
   Version: 2025-02-28T09:55:16Z
   Network: 3/3 OK 
   Drives: 4/4 OK 
   Pool: 1

●  minio3.kang.org:9000
   Uptime: 27 minutes 
   Version: 2025-02-28T09:55:16Z
   Network: 3/3 OK 
   Drives: 4/4 OK 
   Pool: 1

┌──────┬───────────────────────┬─────────────────────┬──────────────┐
│ Pool │ Drives Usage          │ Erasure stripe size │ Erasure sets │
│ 1st  │ 4.0% (total: 160 GiB) │ 12                  │ 1            │
└──────┴───────────────────────┴─────────────────────┴──────────────┘

2.9 GiB Used, 1 Bucket, 3 Objects
12 drives online, 0 drives offline, EC:4
```



##### **通过增加相同规格的集群节点扩容**

**扩容流程**

- 扩容需要准备一个跟原有minio服务器相同规格的集群，进行服务器初始化以及安装minio
- 需要所有minio服务器的配置文件进行修改，并重启所有节点的服务
- 最后负载均衡服务中添加新节点服务器

**注意**

- 集群扩容时，需要新准备一个和原有集群相同或整数倍的节点和磁盘资源。

  比如原集群为3节点12个数据盘,则可以扩展为6节点24数据盘,或者9个节点36块磁盘

  比如原集群为8节点16个数据盘,则可以扩展为16节点32数据盘,或者24个节点48块磁盘

- 扩容节点的数据盘大小配置要跟原有集群一致

- 新加的所有节点必须是没旧MinIO数据新系统，如果有需要清除干净

```bash
#基于 上面的3个节点12个数据盘 基于LVM实现二进制部署MinIO 实现3节点的分布式高可用集群的环境扩容
#停止所有服务
[root@minio1 ~]#systemctl stop minio.service
[root@minio2 ~]#systemctl stop minio.service
[root@minio3 ~]#systemctl stop minio.service
#在新加的节点上修改主机名
[root@ubuntu2404 ~]#hostnamectl hostname minio4
[root@ubuntu2404 ~]#hostnamectl hostname minio5
[root@ubuntu2404 ~]#hostnamectl hostname minio6
#在所有节点修改hosts文件
[root@minio1 ~]#vim /etc/hosts
10.0.0.201 minio1.kang.org minio1
10.0.0.202 minio2.kang.org minio2
10.0.0.203 minio3.kang.org minio3
10.0.0.204 minio4.kang.org minio4
10.0.0.205 minio5.kang.org minio5
10.0.0.206 minio6.kang.org minio6

[root@minio1 ~]#scp /etc/hosts minio2:/etc/hosts 
[root@minio1 ~]#scp /etc/hosts minio3:/etc/hosts  
[root@minio1 ~]#scp /etc/hosts minio4:/etc/hosts
[root@minio1 ~]#scp /etc/hosts minio5:/etc/hosts  
[root@minio1 ~]#scp /etc/hosts minio6:/etc/hosts

#基于脚本在minio4、minio5、minio6上安装minio
[root@minio4 ~]#bash install_minio_single_node.sh 
[root@minio5 ~]#bash install_minio_single_node.sh 
[root@minio6 ~]#bash install_minio_single_node.sh 
[root@minio4 ~]#systemctl stop minio.service 
[root@minio5 ~]#systemctl stop minio.service 
[root@minio6 ~]#systemctl stop minio.service 
```

```bash
#在新添加的节点上执行
lvcreate -n minio1 -L +20G ubuntu-vg
lvcreate -n minio2 -L +20G ubuntu-vg
lvcreate -n minio3 -L +20G ubuntu-vg
lvcreate -n minio4 -L +20G ubuntu-vg

#初始化逻辑卷
mkfs.xfs /dev/ubuntu-vg/minio1
mkfs.xfs /dev/ubuntu-vg/minio2
mkfs.xfs /dev/ubuntu-vg/minio3
mkfs.xfs /dev/ubuntu-vg/minio4

#创建文件
[root@minio4 ~]#mkdir /data/minio{1..4} -p
[root@minio5 ~]#mkdir /data/minio{1..4} -p
[root@minio6 ~]#mkdir /data/minio{1..4} -p

[root@minio1 ~]#scp /etc/fstab minio4:/etc/fstab
[root@minio1 ~]#scp /etc/fstab minio5:/etc/fstab
[root@minio1 ~]#scp /etc/fstab minio6:/etc/fstab

[root@minio4 ~]#mount -a
[root@minio5 ~]#mount -a
[root@minio6 ~]#mount -a

[root@minio4 ~]#chown -R minio: /data/
[root@minio5 ~]#chown -R minio: /data/
[root@minio6 ~]#chown -R minio: /data/

#在所有节点上修改配置文件
[root@minio1 ~]#vim /etc/default/minio
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=12345678
MINIO_VOLUMES='http://minio{1...3}.kang.org:9000/data/minio{1...4} http://minio{4...6}.kang.org:9000/data/minio{1...4}'
MINIO_OPTS='--console-address :9001'
MINIO_PROMETHEUS_AUTH_TYPE="public"
#同步到所有节点
[root@minio1 ~]#for i in {2..6}; do
    scp /etc/default/minio 10.0.0.20$i:/etc/default/minio
done
#同步service到新节点
[root@minio1 ~]#for i in {4..6}; do
	scp /lib/systemd/system/minio.service 10.0.0.20$i:/lib/systemd/system/minio.service; 
done

#新节点启动minio
[root@minio4 ~]#systemctl daemon-reload && systemctl enable --now minio
[root@minio5 ~]#systemctl daemon-reload && systemctl enable --now minio
[root@minio6 ~]#systemctl daemon-reload && systemctl enable --now minio
#老节点重启minio
[root@minio1 ~]#systemctl restart minio.service 
[root@minio2 ~]#systemctl restart minio.service 
[root@minio3 ~]#systemctl restart minio.service 
```

```bash
[root@ubuntu2404 ~]#mc admin info mycluster/
●  minio1.kang.org:9000
   Uptime: 1 minute 
   Version: 2025-02-28T09:55:16Z
   Network: 6/6 OK 
   Drives: 4/4 OK 
   Pool: 1

●  minio2.kang.org:9000
   Uptime: 1 minute 
   Version: 2025-02-28T09:55:16Z
   Network: 6/6 OK 
   Drives: 4/4 OK 
   Pool: 1

●  minio3.kang.org:9000
   Uptime: 1 minute 
   Version: 2025-02-28T09:55:16Z
   Network: 6/6 OK 
   Drives: 4/4 OK 
   Pool: 1

●  minio4.kang.org:9000
   Uptime: 1 minute 
   Version: 2025-02-28T09:55:16Z
   Network: 6/6 OK 
   Drives: 4/4 OK 
   Pool: 2

●  minio5.kang.org:9000
   Uptime: 1 minute 
   Version: 2025-02-28T09:55:16Z
   Network: 6/6 OK 
   Drives: 4/4 OK 
   Pool: 2

●  minio6.kang.org:9000
   Uptime: 1 minute 
   Version: 2025-02-28T09:55:16Z
   Network: 6/6 OK 
   Drives: 4/4 OK 
   Pool: 2

┌──────┬───────────────────────┬─────────────────────┬──────────────┐
│ Pool │ Drives Usage          │ Erasure stripe size │ Erasure sets │
│ 1st  │ 4.0% (total: 160 GiB) │ 12                  │ 1            │
│ 2nd  │ 2.1% (total: 160 GiB) │ 12                  │ 1            │
└──────┴───────────────────────┴─────────────────────┴──────────────┘

2.9 GiB Used, 1 Bucket, 3 Objects
24 drives online, 0 drives offline, EC:4
```

#### **缩容**

如果不是LVM，缩容相当于重新部署集群

缩容前所有节点的数据备份,再在配置中删除上面的添加的节点,只保留部分节点,重启服务后,再恢复数据

### **MINIO** **备份还原**

**rclone** **工具介绍**

配置可以直接添加配置文件的方式或者通过进入交互式配置会话命令一步步的完成配置。

默认配置完成的后配置文件都保存在： ~/.config/rclone/rclone.conf 目录下。

```bash
[tencent-cos] # 自定义的配置名称
type = s3  # 存储类型，参考官方文档所有支持的类型
provider = TencentCOS  # 提供商，参考官方文档或者全部
env_auth = false   # 不通过环境变量配置认证
access_key_id = AKxxxxxxxx  # 腾讯云后台生成的密钥key
secret_access_key = Secretxxxxxxx # 腾讯云后台生成的密钥secret
endpoint = cos.ap-chengdu.myqcloud.com # 腾讯云cos所在的地区，看你所在存储桶的公网地址
```

**rclone语法**

```bash
# 本地同步到网盘
rclone [功能选项] <本地路径> <配置名称:路径> [参数] [参数]
# 网盘同步到本地
rclone [功能选项] <配置名称:路径> <本地路径> [参数] [参数]
# 网盘同步到网盘
rclone [功能选项] <配置名称:路径> <配置名称:路径> [参数] [参数]
# [参数]为可选项
#示例：
#同步本地/data/file的文件夹内容到tencent-cos存储下的/backup文件夹中,并且排
除/root/excludes.txt中指定的文件内容
rclone sync /data/file tencent-cos:/backup --exclude-from '/root/backup_file.txt'
# 两个网盘文件同步
rclone copy 配置网盘名称1:网盘路径 配置网盘名称2:网盘路径
```

| 命令          | 说明                                                         |
| ------------- | ------------------------------------------------------------ |
| rclone copy   | 复制                                                         |
| rclone move   | 移动，如果要在移动后删除空源目录，加上 --delete-empty-src-dirs 参数,会删除源文件类似于MV命令 |
| rclone mount  | 挂载                                                         |
| rclone sync   | 同步：将源目录同步到目标目录，只更改目标目录                 |
| rclone size   | 查看网盘文件占用大小                                         |
| rclone delete | 删除路径下的文件内容                                         |
| rclone mkdir  | 创建目录                                                     |
| rclone rmdir  | 删除目录                                                     |
| rclone rmdirs | 删除指定环境下的空目录。如果加上 --leave-root 参数，则不会删除根目录 |
| rclone check  | 检查源和目的地址数据是否匹配                                 |
| rclone ls     | 列出指定路径下的所有的文件以及文件大小和路径                 |
| rclone lsl    | 比上面多一个显示上传时间                                     |
| rclone lsd    | 列出指定路径下的目录                                         |
| rclone lsf    | 列出指定路径下的目录和文件                                   |

**安装**

```bash
[root@ubuntu2404 ~]#apt list rclone
Listing... Done
rclone/noble-security,noble-updates,noble-updates,noble-security 1.60.1+dfsg-3ubuntu0.24.04.2 amd64
N: There is 1 additional version. Please use the '-a' switch to see it
[root@ubuntu2404 ~]#apt update && apt install rclone -y
```



**手动准备配置文件**

```
B7jFlClirm2NjlBXRtA2
ynx7ngVbugvRJhpTmX3LGiOkNbVMEtVDVkUakVnh
```

具体备份还原流程

```bash
#手动准备配置文件
[root@ubuntu2404 ~]#mkdir -p .config/rclone
[root@ubuntu2404 ~]#vim .config/rclone/rclone.conf
[minio]
type = s3
provider = Minio
evn_auth = false
#access_key_id = admin
#secret_access_key = 12345678
access_key_id = B7jFlClirm2NjlBXRtA2
secret_access_key = ynx7ngVbugvRJhpTmX3LGiOkNbVMEtVDVkUakVnh
region = beijing-1
endpoint = http://10.0.0.10:9000

#验证配置命令
rclone lsd <minio集群名称>:
#备份命令
#全量备份
rclone sync <minio集群名称>:/<bucket名称> <备份目录> -vP
#增量备份,使用md5判断文件是否需要重新同步
rclone sync <minio集群名称>:/<bucket名称> <备份目录>  -vP --checksum
#备份选项
--transfers=N - 并行文件数，默认为4，根据内存，以及带宽调整
--buffer-size=SIZE 加速 sync命令，使用内存缓存大小
--bwlimit UP:DOWN 上传下载限速b|k|M|G,在业务需要时可根据需求设置
--checksum 通过md5判断文件是否有需要同步，消耗cpu比较高
--update --use-server-modtime 通过mtime判断文件是否需要同步,Skip files that are newer on the destination
#推荐同步备份数据命令
rclone sync <minio集群名称>:/<bucket名称> <备份目录> --transfers=8 --update -v -P
#推荐恢复备份数据命令
rclone sync <备份目录> <minio集群名称>:/<bucket名称> --transfers=8 --update -v --logfile=rclone.log
```

#### **数据备份**

```bash
#验证
[root@ubuntu2404 ~]#rclone lsd minio:
          -1 2025-03-15 17:55:19        -1 mybucket

#第一次执行完全备份
[root@ubuntu2404 ~]#mc ls mycluster/mybucket/
[2025-03-15 20:45:19 CST] 2.0GiB STANDARD f4.img
[root@ubuntu2404 ~]#rclone sync minio:/mybucket /data/minio-bak/ -vP
2025-03-15 21:14:09 INFO  : f4.img: Multi-thread Copied (new)
Transferred:        1.953 GiB / 1.953 GiB, 100%, 243.091 MiB/s, ETA 0s
Transferred:            1 / 1, 100%
Elapsed time:         8.4s
2025/03/15 21:14:09 INFO  : 
Transferred:        1.953 GiB / 1.953 GiB, 100%, 243.091 MiB/s, ETA 0s
Transferred:            1 / 1, 100%
Elapsed time:         8.4s

[root@ubuntu2404 ~]#ll /data/minio-bak/ -h
total 2.0G
drwxr-xr-x 2 root root 4.0K Mar 15 21:14 ./
drwxr-xr-x 4 root root 4.0K Mar 15 21:14 ../
-rw-r--r-- 1 root root 2.0G Mar 15 20:45 f4.img

[root@ubuntu2404 ~]#mc cp /var/log/syslog mycluster/mybucket/
/var/log/syslog:                  462.91 KiB / 462.91 KiB ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 8.87 MiB/s 0s[root@ubuntu2404 ~]#
#第二次增量备份
[root@ubuntu2404 ~]#rclone sync minio:/mybucket /data/minio-bak/ -vP
2025-03-15 21:15:26 INFO  : syslog: Copied (new)
Transferred:      462.909 KiB / 462.909 KiB, 100%, 0 B/s, ETA -
Checks:                 1 / 1, 100%
Transferred:            1 / 1, 100%
Elapsed time:         0.0s
2025/03/15 21:15:26 INFO  : 
Transferred:      462.909 KiB / 462.909 KiB, 100%, 0 B/s, ETA -
Checks:                 1 / 1, 100%
Transferred:            1 / 1, 100%
Elapsed time:         0.0s

[root@ubuntu2404 ~]#ll /data/minio-bak/ -h
total 2.0G
drwxr-xr-x 2 root root 4.0K Mar 15 21:15 ./
drwxr-xr-x 4 root root 4.0K Mar 15 21:14 ../
-rw-r--r-- 1 root root 2.0G Mar 15 20:45 f4.img
-rw-r--r-- 1 root root 463K Mar 15 21:15 syslog
```

#### 还原

```bash
#在网页删除数据，查看
[root@ubuntu2404 ~]#mc ls mycluster/
[2025-03-15 17:55:19 CST]     0B mybucket/
[root@ubuntu2404 ~]#mc ls mycluster/mybucket/ 

#还原
[root@ubuntu2404 ~]#rclone sync /data/minio-bak/ minio:mybucket/ --transfers=8 --update -v -P
2025-03-15 21:29:56 INFO  : syslog: Copied (new)
2025-03-15 21:30:14 INFO  : f4.img: Copied (new)
Transferred:        1.954 GiB / 1.954 GiB, 100%, 111.503 MiB/s, ETA 0s
Transferred:            2 / 2, 100%
Elapsed time:        18.1s
2025/03/15 21:30:14 INFO  : 
Transferred:        1.954 GiB / 1.954 GiB, 100%, 111.503 MiB/s, ETA 0s
Transferred:            2 / 2, 100%
Elapsed time:        18.1s

#查看
[root@ubuntu2404 ~]#rclone ls minio:mybucket/
2097152000 f4.img
   474019 syslog
[root@ubuntu2404 ~]#mc ls mycluster/mybucket/
[2025-03-15 21:30:14 CST] 2.0GiB STANDARD f4.img
[2025-03-15 21:29:56 CST] 463KiB STANDARD syslog

```





### MINIO监控

minio监控内置支持Prometheus,推荐使用Prometheus+grafana进行监控

```bash
#minio集群grafana模板
https://grafana.com/grafana/dashboards/13502-minio-dashboard/


scrape_configs:
  # 监控 MinIO 集群
  - job_name: "minio-job"
    # 如果需要身份认证，可以启用 bearer_token
    # bearer_token: "<secret>"
    metrics_path: "/minio/v2/metrics/cluster"
    scheme: "http"
    static_configs:
      - targets: ["10.0.0.10:9000"]

  
#minio节点主机grafana模板:8919,1860,11074,13978,13502模板
```

**MINIO** **通过** **Prometheus** **和** **Grafana** **监控案例**

```bash
[root@master ~]#curl -s 10.0.0.201:9000/minio/v2/metrics/cluster
#生成Promethus配置

[root@ubuntu2404 ~]#mc admin prometheus generate mycluster/ 
scrape_configs:
- job_name: minio-job
  bearer_token: eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJwcm9tZXRoZXVzIiwic3ViIjoic1djd2ZCcEhLdXoybDl3ZEZXREIiLCJleHAiOjQ4OTU2NDU5NTJ9.5DDdawwx4UuAvJ2n3BoHOIlkNKJxNvtrygAUvvROaWWGXldKDhEAwvId_6qBiDNHiVziUypFeajccZHuy3KMRw
  metrics_path: /minio/v2/metrics/cluster
  scheme: http
  static_configs:
  - targets: ['10.0.0.10:9000']
  
[root@master ~]#vim /etc/prometheus/prometheus.yml 
  - job_name: minio
    bearer_token: eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJwcm9tZXRoZXVzIiwic3ViIjoic1djd2ZCcEhLdXoybDl3ZEZXREIiLCJleHAiOjQ4OTU2NDU5NTJ9.5DDdawwx4UuAvJ2n3BoHOIlkNKJxNvtrygAUvvROaWWGXldKDhEAwvId_6qBiDNHiVziUypFeajccZHuy3KMRw
    metrics_path: /minio/v2/metrics/cluster
    scheme: http
    static_configs:
    - targets: ['10.0.0.10:9000']
[root@master ~]#systemctl reload prometheus

#安装grafana
wget https://mirrors.tuna.tsinghua.edu.cn/grafana/apt/pool/main/g/grafana/grafana_11.4.0_amd64.deb
[root@master ~]#apt install ./grafana_11.4.0_amd64.deb  -y
[root@master ~]#systemctl enable --now grafana-server.service 

```











20250316
