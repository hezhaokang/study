# 35、微服务

## 微服务架构

![image-20250309131446408](5day-png/35微服务架构图.png)

![image-20250309131518993](5day-png/35微服务架构图1.png)

![image-20250309131700484](5day-png/35微服务架构图2.png)

![image-20250309131912127](5day-png/35微服务架构图5.png)



## 微服务

![image-20250309132319892](5day-png/35微服务架构演变.png)

### 单体(巨石)架构

- 传统架构（单机系统），一个项目一个工程：比如:商品、订单、支付、库存、登录、注册等等，统一部署，一个进程
- all in one的架构方式，把所有的功能单元放在一个应用里。然后把整个应用部署到一台服务器上。如果负载能力不行，将整个应用进行水平复制，进行扩展，然后通过负载均衡实现访问。
- Java实现：JSP、Servlet，打包成一个jar、war部署
- 易于开发和测试:也十分方便部署;当需要扩展时，只需要将war复制多份，然后放到多个服务器 上，再做个负载均衡就可以了。
- 如果某个功能模块出问题，有可能全站不可访问，修改Bug后、某模块功能修改或升级后，需要停掉整个服务，重新整体重新打包、部署这个应用war包，功能模块相互之间耦合度高,相互影响,不适合当今互联网业务功能的快速迭代。
- 特别是对于一个大型应用，我们不可能吧所有内容都放在一个应用里面，我们如何维护、如何分工合作都是问题。如果项目庞大，管理难度大
- Web应用服务器：开源的tomcat、jetty、glassfish。商用的有weblogic、websphere、Jboss
- 总结: 架构简单,部署方便,打包编译部署升级效率低下,各功能耦合度高,扩展困难,扩缩容不灵活,适合小型简单项目

### **MVC**

**JAVA的MVC**

MVC是模型(Model)、视图(View)、控制器(Controller)的简写，是一种软件设计规范。是将业务逻辑、数据、显示分离的方法来组织码。

MVC主要作用是降低了视图与业务逻辑间的双向偶合。

MVC不是一种设计模式，MVC是一种架构模式。当然不同的MVC存在差异。

MVC 采用template技术实现前端展示，前端和后端最终编译和整合为一个单体应用，和前后端分离方式分为两个独立项目是不同的。

- Model（模型）：数据模型，提供要展示的数据，因此包含数据和行为，可以认为是领域模型或JavaBean组件（包含数据和行为），不过现在一般都分离开来：Value Object（数据Dao） 和 服务层（行为Service）。也就是模型提供了模型数据查询和模型数据的状态更新等功能，包括数据和业务。
- View（视图）：负责进行模型的展示，一般就是我们见到的用户界面，客户想看到的东西。可通过JSP实现
- Controller（控制器）：接收用户请求，委托给模型进行处理（状态改变），处理完毕后把返回的模型数据返回给视图，由视图负责展示。也就是说控制器做了个调度员的工作。最终表现为Servlet

**Python的MTV**

- M: Model 数据模型，负责与数据库交互，
- T：Template 模板展示，负责呈现内容到浏览器，一般为.html文件
- V: view 流程逻辑，负责接收请求，相当于处理中枢，获取数据（来自用户） ，经过处理后选择合适的结果返回给用户。



### **SOA**

SOA（Service Oriented Architecture）是由多个服务组成的分布式系统

各个子系统之间没有采用统一的通信标准,导致系统间通信与数据交互变得异常复杂

各个服务之间通过**ESB(Enterprise Service Bus)**进行通信,ESB是一个由大量规则和原则集成的软件架构，可以将一系列不同的应用程集成到单个基础架构中，由于没有好的开源方案，只能使用商业公司的产品,因此成本很高

此外ESB属于重量级产品，部署规划异常笨重

ESB的单点依赖和商业ESB的费用问题反而成为了所有服务的瓶颈

![image-20250309132642648](5day-png/35SOAESB.png)

### 微服务

**微服务特点**

- 功能单一职责: 微服务拆分粒度更小，每一个服务都对应唯一的业务能力，做到单一职责，避免重复业务开发
- 访问面向服务:微服务对外暴露业务接口,通过接口实现服务的访问,而不能直接访问服务内的数据和功能
- 独立自治: 团队独立、技术独立、数据独立、部署独立
- 安全性: 服务调用需要实现隔离,容错,降级等安全保护,防止级联故障

**SOA和微服务比较**

<img src="5day-png/35SOA和微服务比较.png" alt="image-20250309132907747" style="zoom: 80%;" />

**微服务的优缺点**

**微服务优点：**

- 每个服务足够内聚，足够小，代码容易理解。这样能聚焦一个简单唯一的业务功能或业务需求。
- 开发简单、开发效率提高，一个服务可能就是专业的只干一件事，微服务能够被小团队单独开发，这个小团队可以是2到5人的开发人员组成
- 微服务是松耦合的，是有功能意义的服务，无论是在开发阶段或部署阶段都是独立的。
- 微服务能使用不同的语言开发
- 易于和第三方集成，微服务运行容易且灵活的方式集成自动部署，通过持续集成工具，如: Jenkins、Hudson、Bamboo
- 微服务易于被一个开发人员理解、修改和维护，这样小团队能够更关注自己的工作成果，无需通过合作才能体现价值
- 微服务允许你利用融合最新技术。微服务只是业务逻辑的代码，不会和HTML/CSS或其他界面组件混合，即前后端分离
- 每个微服务都有自己的存储能力，一般都有自己的独立的数据库，也可以有统一数据库

**微服务缺点**：

- 微服务把原有的一个项目拆分成多个独立工程，增加了开发、测试、运维、监控等的复杂度
- 微服务架构需要保证不同服务之间的数据一致性，引入了分布式事务和异步补偿机制，为设计和开发带来一定挑战
- 开发人员和运维需要处理分布式系统的复杂性，需要更强的技术能力
- 微服务适用于复杂的大系统，对于小型应用使用微服务，进行盲目的拆分只会增加其维护和开发成本

![image-20250309133305506](5day-png/35微服务.png)

**常见的微服务框架**

在多种编程语言中，有许多专门设计用来开发微服务的框架。以下是一些主流的微服务开发框架：

- **Spring Boot & Spring Cloud**：对于Java来说，Spring Boot是一个用来创建独立、基于Spring的生产级应用的框架。Spring Cloud则是一套基于Spring Boot的工具，专门用于构建分布式的、云原生的系统，包括配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等。
- **Go Kit & Micro**：对于Go来说，Go Kit和Go Micro都是专门设计用来创建微服务的工具集，提供了服务发现、负载均衡、同步调用、异步消息等功能。
- **Django & Flask**：对于Python来说，Django和Flask都是非常流行的框架，虽然它们原本并不专为微服务设计，但通过扩展和一些良好的设计实践，可以在这些框架上构建微服务。
- **Express.js**：对于JavaScript和Node.js来说，Express.js是一个非常流行的web应用框架，常被用来创建RESTful API服务。
- **.NET Core**：对于.NET来说，.NET Core是一个跨平台的、开源的框架，用于构建云应用和微服务。
- **Lagom**：Lagom是Lightbend公司为Scala和Java设计的一种用于创建响应式微服务的开源框架。
- **Akka**：Akka是一个用Scala和Java编写的开源工具包和运行时，用于构建并发和分布式应用。
- **gRPC**：gRPC是一个高性能、开源的通用远程过程调用（RPC）框架，由Google开发并开源，适合构建微服务。

这些框架都有各自的特点和优点，选择哪种框架取决于你的项目需求以及团队对某种语言或框架的熟悉度。



### **Spring Cloud**

![image-20250309135359238](5day-png/35spring cloud.png)

|              | Dubbo              | SpringCloud Netflix      | SpringCloudAlibaba       |
| ------------ | ------------------ | ------------------------ | ------------------------ |
| 注册中心     | zookeeper,Redis    | Eureka、Consul           | Nacos、Eureka            |
| 服务远程调用 | Dubbo 协议         | Feign (http协议)         | Dubbo、Feign             |
| 配置中心     | 无                 | SpringCloudConfig        | Nacos、SpringCloudConfig |
| 服务网关     | 无                 | SpringCloudGateway、Zuul | SpringCloudGateway、zuul |
| 服务监控安全 | dubbo-admin 功能少 | Hystrix                  | Sentinel                 |

![image-20250309135954636](5day-png/35SpringCloudAlibaba.png)

| 组件名称                 | 作用                                                       |
| ------------------------ | ---------------------------------------------------------- |
| Nacos                    | 服务注册和配置中心                                         |
| Sentinel                 | 客户端容错保护                                             |
| Dubbo                    | 远程调用                                                   |
| Seata                    | 分布式事务                                                 |
| RocketMQ                 | 消息队列                                                   |
| Alibaba Cloud SchedulerX | 分布式任务调度产品(商用)                                   |
| Alibaba Cloud OSS        | 阿里云对象存储服务（Object Storage Service)(商用)          |
| Alibaba Cloud SMS        | 阿里云短信服务(商用)                                       |
| Alibaba Cloud ACM        | 应用配置中心产品Application Configuration Management(商用) |

Spring Cloud原生及其他组件

| 组件名称      | 作用           |
| ------------- | -------------- |
| consul        | 服务注册中心   |
| config        | 分布式配置中心 |
| Gateway       | API服务网关    |
| Sleuth/Zipkin | 分布式链路追踪 |

### **服务网格** **Mesh** 边车

服务网格 Mesh 优势

- 把原本程序员需要关注的安全策略、负载均衡。流量控制、路由选择等基础能力下沉到了底层组件中，并提供了自动恢复的能力，让开发人员只需关注业务本身即可
- 提供各类服务编排,和服务调用的一整套完整解决方案，不仅仅针对JAVA语言写的服务,可以支持任何语言
- 提供了更灵活，可观察和安全的云原生解决方案，可以适应更复杂的应用程序需求
- 结合Kubernetes的相关云原生技术是未来的发展趋势

### **无服务架构** **Serverless**

Kuberetes 结合 ISTIO和Knative 是实现 Serverless 的流行的实现方案



### 微服务还是单体

单体优先策略一：模块化单体

合乎逻辑的方法是仔细设计一个单体应用，注意软件内部的模块化，包括 API 边界和数据存储方式。

做好这一点之后，从单体应用转向微服务是一件相对简单的事情。

单体优先策略二：边缘剥离

一种更常见的方法是从单体开始，逐渐剥离边缘的微服务。

这种方法可以在微服务架构的核心留下一个实质性的单体。

大多数新的开发都发生在微服务中，而单体是相对静止的。

单体优先策略三：整体替换

很少有人把这种做法看成是一种值得骄傲的做法，然而把单体作为一种牺牲性的架构来建造是有好处的。

不要害怕建造一个你会丢弃的单体应用，特别是如果一个单体应用能让你快速进入市场。

单体优先策略四：粗粒度服务

从几个粗粒度的服务开始，这些服务比你最终得到的微服务要大。

使用这些粗粒度的服务来习惯与多个服务一起工作。

同时享受这样一个事实，即这种粗粒度减少了你必须做的服务间重构的数量。

然后，随着边界的稳定，分解为更细粒度的服务。

“单体优先”的思想，目前已逐渐开始成为是业界普遍共识。现在已经属于后微服务时代!

在一个人数不多、资金不是无限充裕，需要快速将产品推向市场的团队，建议使用“单体优先”的实现方式。

在很多团队中，使用微服务其实是一种Hype Driven Development（炒作/简历驱动开发），不是为了真正为了解决业务问题。

使用“单体优先”，是一个务实的选择！试想如果是你自己创业，你会是选择“单体”还是“微服务”，那么请为你的企业进行务实的选择。



## 有状态和无状态

### **1. 什么是有状态（Stateful）？**

有状态系统会**记住**之前的操作或数据，并依赖这些信息来处理后续请求。

#### **特点**

✅ **依赖历史数据**（状态信息存储在服务器或数据库中）
 ✅ **必须维护会话信息**（Session、用户登录、事务）
 ✅ **扩展和故障恢复更复杂**（需要共享或同步状态）

#### **示例**

- **数据库（MySQL, PostgreSQL）**
   维护连接状态，事务操作依赖之前的执行记录。
- **有状态应用（Tomcat Session 服务器）**
   用户登录后，服务器保存会话（Session），下次访问需依赖该状态。
- **Kafka 消费者组**
   记录消息的消费偏移量（offset），确保消费进度。

#### **有状态架构示意**

```
1 ---> 服务器A（存储状态）
请求2 ---> 服务器A（需要之前的状态才能正确响应）
```

如果**服务器 A 挂掉**，用户会话丢失，必须有共享存储（如 Redis）来恢复状态。

#### **适用场景**

✔ **需要保持用户会话**（如购物车、登录认证）
 ✔ **事务处理**（如银行转账，必须保证一致性）
 ✔ **流式计算**（如 Kafka 消费偏移量）

------

### **2. 什么是无状态（Stateless）？**

无状态系统**不存储**任何请求的历史信息，每次请求都是独立的。

#### **特点**

✅ **请求独立**（服务器不存储用户状态，每次请求都包含完整信息）
 ✅ **易扩展**（任何服务器都能处理请求，负载均衡更简单）
 ✅ **故障恢复简单**（无状态服务可随时重启）

#### **示例**

- **HTTP 协议**（每次请求都是独立的，不存状态）
- **REST API**（所有请求必须包含必要信息，如 Token 认证）
- **DNS 解析**（查询某个域名的 IP 地址，每次查询都是独立的）
- **无状态 Web 服务器（Nginx, Cloudflare）**
   仅代理请求，不存储用户状态。

#### **无状态架构示意**

```
请求1 ---> 服务器A（无状态处理）
请求2 ---> 服务器B（任意服务器都能处理）
```

如果 **服务器 A 挂了**，服务器 B 仍能处理请求，不受影响。

#### **适用场景**

✔ **高并发、分布式系统**（微服务架构、云原生应用）
 ✔ **Web API 设计**（RESTful API，客户端携带所有信息）
 ✔ **负载均衡系统**（如 Nginx，任意后端都能处理请求）

------

### **3. 有状态 vs 无状态 对比**

| **对比项**       | **有状态（Stateful）**             | **无状态（Stateless）**      |
| ---------------- | ---------------------------------- | ---------------------------- |
| **是否存储状态** | 是，需要存储历史数据               | 否，每次请求独立             |
| **扩展性**       | 难扩展，需同步状态                 | 易扩展，请求可由任何实例处理 |
| **故障恢复**     | 复杂，需恢复状态                   | 简单，直接替换失败实例       |
| **典型应用**     | 数据库、Kafka 消费者               | HTTP、REST API、CDN          |
| **请求方式**     | 依赖之前的交互                     | 每次都包含完整信息           |
| **负载均衡**     | 需要会话粘性（Session Stickiness） | 任何服务器都能处理           |

------

### **4. 典型应用场景**

| **应用**       | **有状态（Stateful）**        | **无状态（Stateless）**        |
| -------------- | ----------------------------- | ------------------------------ |
| **数据库**     | MySQL、PostgreSQL（连接状态） | DynamoDB（无状态 API）         |
| **缓存**       | Redis 集群（主从同步）        | Redis 单实例（无状态 GET/SET） |
| **Web 服务器** | Tomcat（Session 管理）        | Nginx（无状态代理）            |
| **消息队列**   | Kafka（消费偏移量）           | RabbitMQ（消息即发即用）       |
| **认证系统**   | 服务器存 Session（有状态）    | JWT Token 认证（无状态）       |

------

### **5. 什么时候选择有状态 vs 无状态？**

#### **选择有状态（Stateful）**

✅ 需要**保持会话**（用户购物车、登录态）
 ✅ 需要**事务一致性**（银行转账、订单支付）
 ✅ 需要**持久化存储**（数据库、文件存储）

#### **选择无状态（Stateless）**

✅ 需要**高并发、高扩展**（云计算、负载均衡）
 ✅ 需要**轻量级请求**（REST API、微服务架构）
 ✅ 需要**自动恢复**（服务器无状态，随时可重启）

------

### **6. 结合实际案例**

#### **REST API（无状态） vs 传统 Session 认证（有状态）**

- 有状态方式（Session 认证）
  - 用户登录后，服务器生成一个 **Session ID** 存在内存中。
  - 客户端请求时携带 **Session ID**，服务器检查状态是否有效。
  - **问题**：如果服务器崩溃，Session 丢失，用户需要重新登录。
- 无状态方式（JWT Token）
  - 用户登录后，服务器生成一个 **JWT Token** 并返回给客户端。
  - 客户端每次请求都携带 **JWT Token**，服务器直接解析 Token 验证身份。
  - **优势**：无需服务器存储 Session，任何服务器都能处理请求。

------

### **7. 总结**

- **有状态（Stateful）**：需要记住历史数据，适用于**数据库、事务、会话管理**等。
- **无状态（Stateless）**：不存储请求状态，每次都是全新的，适用于**REST API、云原生应用、高并发系统**等。

一般情况下： ✅ **数据库、Session、Kafka** 需要有状态。
 ✅ **Web API、CDN、负载均衡** 采用无状态架构。
 在云计算和微服务架构中，**无状态更容易扩展**，但某些业务（如支付系统）仍然需要**有状态的事务处理**。

**最佳实践：**

- **尽可能设计成无状态（Stateless）**，方便扩展和恢复。
- **必须有状态的应用（Stateful）**，需要考虑如何同步状态（如数据库集群、Redis 共享会话）。



## zookeeper

### 工作原理

ZooKeeper 是一个分布式服务框架，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：命名服务、状态同步、配置中心、集群管理等。

### 功能

#### 命名服务

命名服务是分布式系统中比较常见的一类场景。命名服务是分布式系统最基本的公共服务之一。在分布式系统中，被命名的实体通常可以是集群中的机器、提供的服务地址或远程对象等——这些我们都可以统称它们为名字（Name），其中较为常见的就是一些分布式服务框架（如RPC、RMI）中的服务地址列表，通过使用命名服务，客户端应用能够根据指定名字来获取资源的实体、服务地址和提供者的信息等。

**数据模型**

```
在 Zookeeper 中，节点分为两类
第一类是指构成Zookeeper集群的主机，称之为主机节点
第二类则是指内存中zookeeper数据模型中的数据单元，用来存储各种数据内容，称之为数据节点 ZNode。
Zookeeper内部维护了一个层次关系(树状结构)的数据模型，它的表现形式类似于Linux的文件系统，甚至操作的种类都一致。
Zookeeper数据模型中有自己的根目录(/)，根目录下有多个子目录，每个子目录后面有若干个文件,由斜杠(/)进行分割的路径，就是一个ZNode,每个 ZNode上都会保存自己的数据内容和一系列属性信息.
```

![image-20250309143031879](5day-png/35数据模型.png)

#### 状态同步

每个节点除了存储数据内容和 node 节点状态信息之外，还存储了已经注册的APP 的状态信息，当有些节点或APP 不可用，就将当前状态同步给其他服务。

#### **配置中心**

现在我们大多数应用都是采用的是分布式开发的应用，搭建到不同的服务器上，我们的配置文件，同一个应用程序的配置文件一样，还有就是多个程序存在相同的配置，当我们配置文件中有个配置属性需要改变，需要改变每个程序的配置属性，这样会很麻烦的去修改配置，那么可用使用ZooKeeper 来实现配置中心

ZooKeeper 采用的是推拉相结合的方式：客户端向服务端注册自己需要关注的节点，一旦该节点的数据发生变更，那么服务端就会向相应的客户端发送Watcher事件通知，客户端接收到这个消息通知后，需要主动到服务端获取最新的数据。

Apollo（阿波罗）是携程框架部门研发的开源配置管理中心,此应用比较流行

#### **集群管理**

所谓集群管理，包括集群监控与集群控制两大块，前者侧重对集群运行时状态的收集，后者则是对集群进行操作与控制，在日常开发和运维过程中，我们经常会有类似于如下的需求：

- 希望知道当前集群中究竟有多少机器在工作。
- 对集群中每台机器的运行时状态进行数据收集。对集群中机器进行上下线操作。

ZooKeeper 具有以下两大特性:

- 客户端如果对ZooKeeper 的一个数据节点注册 Watcher监听，那么当该数据节点的内容或是其子节点列表发生变更时，ZooKeeper 服务器就会向已注册订阅的客户端发送变更通知。
- 对在ZooKeeper上创建的临时节点，一旦客户端与服务器之间的会话失效，那么该临时节点也就被自动清除。

Watcher（事件监听器）是 Zookeeper 中的一个很重要的特性。Zookeeper 允许用户在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候， ZooKeeper 服务端会将事件通知到感兴趣的客户端上去，该机制是 Zookeeper 实现分布式协调服务的重要特性。



### zookeeper服务流程

<img src="5day-png/35zookeeper服务流程.png" alt="image-20250309143433405" style="zoom:80%;" />

1. 生产者启动
2. 生产者注册至zookeeper
3. 消费者启动并订阅频道
4. zookeeper 通知消费者事件
5. 消费者调用生产者
6. 监控中心负责统计和监控服务状态



### **ZooKeeper** **单机部署**

```powershell
官方依赖介绍
https://zookeeper.apache.org/doc/r3.9.0/zookeeperAdmin.html#sc_systemReq
官方文档
https://zookeeper.apache.org/doc/r3.6.2/zookeeperStarted.html#sc_InstallingSingleMode
```

```bash
#配置java环境
[root@ubuntu2404 ~]#apt update && apt install -y openjdk-21-jdk
```

#### 包安装

```bash
#部署 ZooKeeper
#包安装
[root@ubuntu2404 ~]#apt list zookeeper
Listing... Done
zookeeper/noble,noble 3.9.1-1build2 all
[root@ubuntu2404 ~]#apt install -y zookeeper
```

#### 二进制安装

```bash
#二进制安装
https://zookeeper.apache.org/
版本： stable 和 current ，生产建议使用stable版本
#国内镜像源
https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/apache-zookeeper-3.8.4-bin.tar.gz

[root@ubuntu2404 ~]#wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/apache-zookeeper-3.8.4-bin.tar.gz
[root@ubuntu2404 ~]#ls
apache-zookeeper-3.8.4-bin.tar.gz
[root@ubuntu2404 ~]#tar xf apache-zookeeper-3.8.4-bin.tar.gz -C /usr/local/
[root@ubuntu2404 ~]#cd /usr/local/
[root@ubuntu2404 local]#ls
apache-zookeeper-3.8.4-bin  bin  etc  games  include  lib  man  sbin  share  src
[root@ubuntu2404 local]#ln -s apache-zookeeper-3.8.4-bin/ zookeeper
[root@ubuntu2404 local]#cd zookeeper
[root@ubuntu2404 zookeeper]#cp conf/zoo_sample.cfg conf/zoo.cfg 
[root@ubuntu2404 zookeeper]#cat conf/zoo.cfg | grep -Ev '^$|^#'
tickTime=2000			#服务器与服务器之间的单次心跳检测时间间隔，单位为毫秒
initLimit=10			#集群中leader 服务器与follower服务器初始连接心跳次数，即多少个 2000 毫秒
syncLimit=5				#leader 与follower之间检测发送和应答的心跳次数，如果该follower在时间段5*2000不能与leader进行通信，此follower将不可用
dataDir=/tmp/zookeeper	#自定义的zookeeper保存数据的目录
clientPort=2181			#客户端连接 Zookeeper 服务器的端口，Zookeeper会监听这个端口，接受客户端的访问请求
[root@ubuntu2404 zookeeper]#ls bin/
README.txt    zkCli.cmd  zkEnv.cmd  zkServer.cmd            zkServer.sh             zkSnapshotComparer.sh  zkSnapShotToolkit.sh  zkTxnLogToolkit.sh
zkCleanup.sh  zkCli.sh   zkEnv.sh   zkServer-initialize.sh  zkSnapshotComparer.cmd  zkSnapShotToolkit.cmd  zkTxnLogToolkit.cmd
```

```bash
[root@ubuntu2404 zookeeper]#cat conf/zoo.cfg
# The number of milliseconds of each tick
tickTime=2000		#这是 Zookeeper 的基本时间单位，单位是毫秒。它定义了 Zookeeper 的心跳时间，也就是 Zookeeper 中节点之间通信的最小时间间隔。
# The number 
of ticks that the initial 
# synchronization phase can take
initLimit=10		#这是 Zookeeper 集群中初始化连接的最大限制。它指的是在 Zookeeper 启动时，集群成员之间可以用来完成初始化连接的时间限制。
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5			#这是 Zookeeper 集群成员之间同步的最大时间限制，单位是 tickTime。它设置了 Zookeeper 集群同步请求的最长时间。
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir=/data/zookeeper	#这是 Zookeeper 数据存储目录。Zookeeper 会将事务日志和快照文件保存在这个目录下。
dataLogDir=/usr/local/zookeeper/logs	#指定日志路径，默认与 dataDir 一致,事务日志对性能影响非常大，强烈建议事务日志目录和数据目录分开，如果后续修改路径，需要先删除中dataDir中旧的事务日志，否则可能无法启动
# the port at which the clients will connect
clientPort=2181		#这是 Zookeeper 客户端连接的端口，默认值是 2181。
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60		#控制每个客户端连接到 Zookeeper 的最大连接数。
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3	
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1
#autopurge.snapRetainCount=3 和 autopurge.purgeInterval=1: 用来自动清理过期的快照文件和事务日志文件。

## Metrics Providers
#
# https://prometheus.io Metrics Exporter
metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider		#这个配置启用了 Prometheus 监控。PrometheusMetricsProvider 是用来从 Zookeeper 导出 Prometheus 监控数据的类。
metricsProvider.httpHost=0.0.0.0	#这个配置表示 Prometheus 将通过绑定到所有可用的网络接口来提供监控数据。
metricsProvider.httpPort=7000		#Prometheus 用于监听的端口。
metricsProvider.exportJvmInfo=true	#设置为 true，表示会导出 JVM 相关的信息，供 Prometheus 监控。
```

```bash
[root@ubuntu2404 zookeeper]#cat conf/zoo.cfg | grep -Ev '^$|^#'
tickTime=2000
of ticks that the initial 
initLimit=10
syncLimit=5
dataDir=/data/zookeeper
clientPort=2181
metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider
metricsProvider.httpHost=0.0.0.0
metricsProvider.httpPort=7000
metricsProvider.exportJvmInfo=true
```

**启动Zookeeper**

```bash
[root@ubuntu2404 zookeeper]#./bin/zkServer.sh 
/usr/bin/java
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
Usage: ./bin/zkServer.sh [--config <conf-dir>] {start|start-foreground|stop|version|restart|status|print-cmd}
[root@ubuntu2404 zookeeper]#./bin/zkServer.sh start
/usr/bin/java
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[root@ubuntu2404 zookeeper]#./bin/zkServer.sh status
/usr/bin/java
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: standalone

#前台启动观察启动过程
[root@ubuntu2404 zookeeper]#./bin/zkServer.sh start-foreground
```

```bash
#先停止服务
[root@ubuntu2404 zookeeper]#./bin/zkServer.sh stop
/usr/bin/java
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
Stopping zookeeper ... STOPPED

#准备service文件
[root@ubuntu2404 zookeeper]#vim /lib/systemd/system/zookeeper.service
[Unit]
Description=zookeeper.service
After=network.target

[Service]
Type=forking
ExecStart=/usr/local/zookeeper/bin/zkServer.sh start
ExecStop=/usr/local/zookeeper/bin/zkServer.sh stop
ExecReload=/usr/local/zookeeper/bin/zkServer.sh restart

[Install]
WantedBy=multi-user.target

[root@ubuntu2404 ~]#systemctl daemon-reload 
[root@ubuntu2404 ~]#systemctl enable --now zookeeper.service 
```



#### 脚本安装

```bash
[root@ubuntu2404 ~]#cat install_zookeeper_single_node.sh 
#!/bin/bash
#

#支持在线和离线安装

ZK_VERSION=3.9.3
#ZK_VERSION=3.9.2
#ZK_VERSION=3.9.1
#ZK_VERSION=3.9.0
#ZK_VERSION=3.8.1
#ZK_VERSION=3.8.0
#ZK_VERSION=3.6.3
#ZK_VERSION=3.7.1
#ZK_URL=https://archive.apache.org/dist/zookeeper/zookeeper-${ZK_VERSION}/apache-zookeeper-${ZK_VERSION}-bin.tar.gz
ZK_URL=https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-${ZK_VERSION}/apache-zookeeper-${ZK_VERSION}-bin.tar.gz
#ZK_URL="https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/apache-zookeeper-${ZK_VERSION}-bin.tar.gz"
#ZK_URL="https://downloads.apache.org/zookeeper/stable/apache-zookeeper-${ZK_VERSION}-bin.tar.gz"

INSTALL_DIR=/usr/local/zookeeper


HOST=`hostname -I|awk '{print $1}'`

.  /etc/os-release

color () {
    RES_COL=60
    MOVE_TO_COL="echo -en \\033[${RES_COL}G"
    SETCOLOR_SUCCESS="echo -en \\033[1;32m"
    SETCOLOR_FAILURE="echo -en \\033[1;31m"
    SETCOLOR_WARNING="echo -en \\033[1;33m"
    SETCOLOR_NORMAL="echo -en \E[0m"
    echo -n "$1" && $MOVE_TO_COL
    echo -n "["
    if [ $2 = "success" -o $2 = "0" ] ;then
        ${SETCOLOR_SUCCESS}
        echo -n $"  OK  "    
    elif [ $2 = "failure" -o $2 = "1"  ] ;then 
        ${SETCOLOR_FAILURE}
        echo -n $"FAILED"
    else
        ${SETCOLOR_WARNING}
        echo -n $"WARNING"
    fi
    ${SETCOLOR_NORMAL}
    echo -n "]"
    echo 
}

install_jdk() {
    java -version &>/dev/null && { color "JDK 已安装!" 1 ; return;  }
    if command -v yum &>/dev/null ; then
        yum -y install java-1.8.0-openjdk-devel || { color "安装JDK失败!" 1; exit 1; }
    elif command -v apt &>/dev/null ; then
        apt update
        #apt install openjdk-11-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        apt install openjdk-8-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
    else
       color "不支持当前操作系统!" 1
       exit 1
    fi
    java -version && { color "安装 JDK 完成!" 0 ; } || { color "安装JDK失败!" 1; exit 1; } 
}


install_zookeeper() {
    if [ -f apache-zookeeper-${ZK_VERSION}-bin.tar.gz ] ;then
        cp apache-zookeeper-${ZK_VERSION}-bin.tar.gz /usr/local/src/
    else
        wget -P /usr/local/src/ --no-check-certificate $ZK_URL || { color  "下载失败!" 1 ;exit ; }
    fi
    tar xf /usr/local/src/${ZK_URL##*/} -C /usr/local
    ln -s /usr/local/apache-zookeeper-*-bin/ ${INSTALL_DIR}
   #echo "PATH=${INSTALL_DIR}/bin:$PATH" >  /etc/profile.d/zookeeper.sh
    echo "PATH=${INSTALL_DIR}/bin:$PATH" >>  /etc/profile
    #.  /etc/profile.d/zookeeper.sh
    mkdir -p ${INSTALL_DIR}/data 
    cat > ${INSTALL_DIR}/conf/zoo.cfg <<EOF
tickTime=2000
initLimit=10
syncLimit=5
dataDir=${INSTALL_DIR}/data
dataLogDir=${INSTALL_DIR}/logs
clientPort=2181
maxClientCnxns=128
autopurge.snapRetainCount=3
autopurge.purgeInterval=24
EOF
        cat > /lib/systemd/system/zookeeper.service <<EOF
[Unit]
Description=zookeeper.service
After=network.target

[Service]
Type=forking
#Environment=${INSTALL_DIR}
ExecStart=${INSTALL_DIR}/bin/zkServer.sh start
ExecStop=${INSTALL_DIR}/bin/zkServer.sh stop
ExecReload=${INSTALL_DIR}/bin/zkServer.sh restart

[Install]
WantedBy=multi-user.target
EOF
    systemctl daemon-reload
    systemctl enable --now  zookeeper.service
    systemctl is-active zookeeper.service
        if [ $? -eq 0 ] ;then 
        color "zookeeper 安装成功!" 0  
    else 
        color "zookeeper 安装失败!" 1
        exit 1
    fi   
}


install_jdk

install_zookeeper
```

 

### zookeeper集群

<img src="5day-png/35zookeeper集群结构.png" alt="image-20250309165328414" style="zoom:67%;" />

Zookeeper集群基于Master/Slave的模型

处于主要地位负责处理写操作)的主机称为Leader节点，处于次要地位主要负责处理读操作的主机称为 follower 节点

Zookeeper的读写机制

- **写操作**：在Zookeeper中，写操作（如创建、删除、更新节点等）通常由Leader节点处理。当客户端发送写请求时，Leader会先将其写入本地日志，并尝试将其同步到集群中的其他Follower节点。一旦大多数在总数过半的节点成功应用了这个写操作，Leader就会认为这次写操作是成功的，并返回结果给客户端。
- **读操作**：读操作可以由Leader或Follower节点处理。为了提高读取性能，Zookeeper通常会采用异步复制的方式，即Follower节点会异步地从Leader节点复制最新的数据。这样，即使Follower节点的数据不是最新的，它仍然可以处理读请求，并在后续从Leader节点获取更新的数据。生产中读取的方式在提高性能和容错性方面具有显著优势，同时也在一定程度上牺牲了数据的一致性。然而，对于大多数应用场景来说，这种折衷是合理的。

对于n台server,每个server都知道彼此的存在。只要有>n/2台server节点可用，整个zookeeper系统保持可用。因此zookeeper集群通常由奇数台Server节点组成

**集群中节点数越多，写性能越差，读性能越好**

<img src="5day-png/35zookeeper集群1.png" alt="image-20250309165601384" style="zoom:50%;" />

<img src="5day-png/35zookeeper集群角色.png" alt="image-20250309165644841" style="zoom:67%;" />

| 序号 | 角色             | 职责描述                                                     |
| ---- | ---------------- | ------------------------------------------------------------ |
| 1    | 领导者(Leader)   | 负责处理写入请求的，事务请求的唯一调度和处理者,负责进行投票发起和决议，更新系统状态 |
| 2    | 跟随者(Follower) | 接收客户请求并向客户端返回结果，在选Leader过程中参与投票     |
| 3    | 观察者(Observer) | 转交客户端写请求给leader节点，和同步leader状态和Follower唯一区别就是不参与Leader投票,也不参与写操作的"过半写成功"策略 |
| 4    | 学习者(Learner)  | 和leader进行状态同步的节点统称Learner，包括:Follower和Observer |
| 5    | 客户端(client)   | 请求发起方                                                   |

**选举过程**

**节点角色状态：**

- LOOKING：寻找 Leader 状态，处于该状态需要进入选举流程
- LEADING：领导者状态，处于该状态的节点说明是角色已经是Leader 
- FOLLOWING：跟随者状态，表示 Leader已经选举出来，当前节点角色是follower 
- OBSERVER：观察者状态，表明当前节点角色是 observer

**选举** **ID**：

- ZXID（zookeeper transaction id）：每个改变 Zookeeper状态的操作都会自动生成一个对应的zxid。ZXID最大的节点优先选为Leader
- myid：服务器的唯一标识(SID)，通过配置 myid 文件指定，集群中唯一,当ZXID一样时,myid大的节点优先选为Leader

**ZooKeeper** **集群选举过程：**

- 当集群中的 zookeeper 节点启动以后，会根据配置文件中指定的 zookeeper节点地址进行leader 选择操作，过程如下：
- 每个zookeeper 都会发出投票，由于是第一次选举leader，因此每个节点都会把自己当做leader 角色进行选举，每个zookeeper 的投票中都会包含自己的myid和zxid，此时zookeeper 1 的投票为myid 为 1，初始zxid有一个初始值0x0，后期会随着数据更新而自动变化，zookeeper 2 的投票为myid 为2，初始zxid 为初始生成的值。
- 每个节点接受并检查对方的投票信息，比如投票时间、是否状态为LOOKING状态的投票。

- 对比投票，优先检查zxid，如果zxid 不一样则 zxid 大的为leader，如果zxid相同则继续对比myid，myid 大的一方为 leader

  成为 Leader 的必要条件： Leader 要具有最高的zxid；当集群的规模是 n 时，集群中大多数的机器（至少n/2+1）得到响应并从follower 中选出的 Leader。

心跳机制：Leader 与 Follower 利用 PING 来感知对方的是否存活，当 Leader无法响应PING 时，将重新发起 Leader 选举。

当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB(Zookeeper Atomic Broadcast) 协议就会进入恢复模式并选举产生新的Leader服务器。这个过程大致如下：

- Leader Election（选举阶段）：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。
- Discovery（发现阶段）：在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。
- Synchronization（同步阶段）:同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准leader 才会成为真正的 leader。
- Broadcast（广播阶段） ：到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步

在分布式系统中，有多种协议被设计来解决一致性问题，Paxos、Raft、ZAB 等分布式算法经常会被称作是“强一致性”的分布式共识协议

强一致指的是尽管系统内部节点可以存在不一致的状态，但从系统外部看来，不一致的情况并不会被观察到，所以整体上看系统是强一致性的,对于CAP模型，实现的CP。

- **ZAB**（**ZooKeeper Atomic Broadcast 原子广播）**：Zab协议是由Apache ZooKeeper项目提出的一种原子广播协议，是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。它主要用于构建一个高可用、高性能的分布式协调服务。Zab协议保证了分布式过程中的消息顺序一致性和崩溃恢复能力。它主要用在主-备模式的系统中，例如Kafka在2.8.0版本之前依赖ZooKeeper作为其元数据的存储，并使用Zab协议来保证数据的一致性。
- **Paxos**：Paxos（帕克索斯）是Leslie Lamport( 在2013 年获得计算界的最高奖图灵奖) 早在1990年提出的分布式一致性算法之一，它能解决分布式环境中的一致性问题。Paxos算法可以保证在非同步和非可靠的分布式系统中，节点间可以达成一致的决定。不过，Paxos的描述和实现被公认为相对复杂，这也是Raft等其他一致性算法出现的原因。使用Paxos或其变体协议的有如下应用Google Chubby(Google的一个锁服务),Apache Cassandra(度可扩展的分布式数据库),Microsoft Azure Cosmos DB(Microsoft Azure提供的一个全球分布式、多模型的数据库服务),Riak(Microsoft Azure提供的一个全球分布式、多模型的数据库服务)
- **Raft**：Raft协议由Diego Ongaro和John Ousterhout在2013年提出的。他们在斯坦福大学就读博士期间设计并研究了Raft。他们的主要目标是创建一个与Paxos功能相同，但更容易理解和实现的一致性算法。Raft 是一个为分布式系统提供一致性的算法。与Paxos相比，Raft的主要目标是提供一种更加易于理解和实现的一致性算法。Raft通过选举算法确保了分布式系统中的领导者唯一性。所有的写操作都通过领导者完成，这样就可以确保所有复制节点上的数据一致性。一些知名的分布式系统，如：kafka，etcd，nacos和Consul，都采用了Raft算法。

**Gossip** **谣言协议**

相比 Paxos、Raft 等算法，Gossip 的过程十分简单，它可以看作是以下两个步骤的简单循环：

- 如果有某一项信息需要在整个网络中所有节点中传播，那从信息源开始，选择一个固定的传播周期（譬如 1 秒），随机选择它相连接的k 个节点（称为 Fan-Out）来传播消息。
- 每一个节点收到消息后，如果这个消息是它之前没有收到过的，将在下一个周期内，选择除了发送消息给它的那个节点外的其他相邻 k 个节点发送相同的消息，直到最终网络中所有节点都收到了消息，尽管这个过程需要一定时间，但是理论上最终网络的所有节点都会拥有相同的消息。

**ZooKeeper** **集群特性**

整个集群中只要有超过集群数量一半的 zookeeper工作是正常的，那么整个集群对外就是可用的

假如有 2 台服务器做了一个 Zookeeper 集群，只要有任何一台故障或宕机，那么这个 ZooKeeper集群就不可用了，因为剩下的一台没有超过集群一半的数量，但是假如有三台zookeeper 组成一个集群， 那么损坏一台就还剩两台，大于 3台的一半，所以损坏一台还是可以正常运行的，但是再损坏一台就只剩一台集群就不可用了。那么要是 4 台组成一个zookeeper集群，损坏一台集群肯定是正常的，那么损坏两台就还剩两台，那么2台不大于集群数量的一半，所以 3 台的 zookeeper 集群和 4 台的 zookeeper集群损坏两台的结果都是集群不可用，以此类推 5 台和 6 台以及 7 台和 8台都是同理

**Zookeeper** **事务日志和快照**

ZooKeeper集群中的每个服务器节点每次接收到写操作请求时，都会先将这次请求发送给leader

leader将这次写操作转换为带有状态的事务，然后leader会对这次写操作广播出去以便进行协调。

当协调通过(大多数节点允许这次写)后，leader通知所有的服务器节点，让它们将这次写操作应用到内存数据库中，并将其记录到事务日志中。

当事务日志记录的次数达到一定数量后(默认10W次)，就会将内存数据库序列化一次，使其持久化保存到磁盘上，序列化后的文件称为"快照文件"。

每次拍快照都会生成新的事务日志。



```bash
#安装java环境
[root@ubuntu2404 ~]#apt update && apt install -y openjdk-21-jdk

#下载二进制文件
[root@ubuntu2404 ~]#wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/apache-zookeeper-3.8.4-bin.tar.gz
[root@ubuntu2404 ~]#tar xf apache-zookeeper-3.8.4-bin.tar.gz -C /usr/local/
[root@ubuntu2404 ~]#cd /usr/local/
[root@ubuntu2404 local]#ln -s apache-zookeeper-3.8.4-bin/ zookeeper
[root@ubuntu2404 local]#cd zookeeper
```

```bash
[root@ubuntu2404 zookeeper]#mv conf/zoo_sample.cfg conf/zoo.cfg 
[root@ubuntu2404 zookeeper]#vim conf/zoo.cfg 
[root@ubuntu2404 zookeeper]#cat conf/zoo.cfg 
# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir=/usr/local/zookeeper/data
# the port at which the clients will connect
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1

## Metrics Providers
#
# https://prometheus.io Metrics Exporter
#metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider
#metricsProvider.httpHost=0.0.0.0
#metricsProvider.httpPort=7000
#metricsProvider.exportJvmInfo=true

server.1=10.0.0.201:2888:3888
server.2=10.0.0.202:2888:3888
server.3=10.0.0.203:2888:3888

[root@ubuntu2404 zookeeper]#mkdir /usr/local/zookeeper/data/
[root@ubuntu2404 zookeeper]#echo 1 > /usr/local/zookeeper/data/myid

#另外两台执行上面的操作
echo 2 > /usr/local/zookeeper/data/myid
echo 3 > /usr/local/zookeeper/data/myid
```

```bash
#创建service文件
[root@ubuntu2404 zookeeper]#vim /lib/systemd/system/zookeeper.service
[Unit]
Description=zookeeper.service
After=network.target

[Service]
Type=forking
ExecStart=/usr/local/zookeeper/bin/zkServer.sh start
ExecStop=/usr/local/zookeeper/bin/zkServer.sh stop
ExecReload=/usr/local/zookeeper/bin/zkServer.sh restart

[Install]
WantedBy=multi-user.target

[root@ubuntu2404 ~]#systemctl daemon-reload 
[root@ubuntu2404 ~]#systemctl enable --now zookeeper.service 
```



### 客户端连接

```bash
#客户端连接
[root@ubuntu2404 ~]#/usr/local/zookeeper/bin/zkCli.sh
[zk: localhost:2181(CONNECTED) 13] help
ZooKeeper -server host:port -client-configuration properties-file cmd args
        addWatch [-m mode] path # optional mode is one of [PERSISTENT, PERSISTENT_RECURSIVE] - default is PERSISTENT_RECURSIVE
        addauth scheme auth
        close 
        config [-c] [-w] [-s]
        connect host:port
        create [-s] [-e] [-c] [-t ttl] path [data] [acl]
        delete [-v version] path
        deleteall path [-b batch size]
        delquota [-n|-b|-N|-B] path
        get [-s] [-w] path
        getAcl [-s] path
        getAllChildrenNumber path
        getEphemerals path
        history 
        listquota path
        ls [-s] [-w] [-R] path
        printwatches on|off
        quit 
        reconfig [-s] [-v version] [[-file path] | [-members serverID=host:port1:port2;port3[,...]*]] | [-add serverId=host:port1:port2;port3[,...]]* [-remove serverId[,...]*]
        redo cmdno
        removewatches path [-c|-d|-a] [-l]
        set [-s] [-v version] path data
        setAcl [-s] [-v version] [-R] path acl
        setquota -n|-b|-N|-B val path
        stat [-w] path
        sync path
        version 
        whoami 
[zk: localhost:2181(CONNECTED) 0] ls /
[zookeeper]          
[zk: localhost:2181(CONNECTED) 1] get /zookeeper
[zk: localhost:2181(CONNECTED) 2] create /app1
Created /app1
[zk: localhost:2181(CONNECTED) 3] ls /
[app1, zookeeper]
[zk: localhost:2181(CONNECTED) 4] set /app1 hello
[zk: localhost:2181(CONNECTED) 6] ls /
[app1, zookeeper]
[zk: localhost:2181(CONNECTED) 7] get /app1 
hello
[zk: localhost:2181(CONNECTED) 8] create /app1/cless
Created /app1/cless
[zk: localhost:2181(CONNECTED) 9] ls /app1 
[cless]
[zk: localhost:2181(CONNECTED) 10] set /app1/cless 61
[zk: localhost:2181(CONNECTED) 12] get /app1/cless 
61
```

### 图形工具连接

**图形化客户端** **ZooInspector**

```
https://github.com/zzhang5/zooinspector
https://gitee.com/lbtooth/zooinspector.git
```

```bash
编译 zooinspector
build
$git clone https://github.com/zzhang5/zooinspector.git
$cd zooinspector/
$mvn clean package -Dmaven.test.skip=true

Run
$chmod +x target/zooinspector-pkg/bin/zooinspector.sh
$target/zooinspector-pkg/bin/zooinspector.sh
```

```bash
#注意：只支持JAVA-8，不支持JAVA-11以上版本
[root@ubuntu2404 ~]#apt update && apt -y install openjdk-8-jdk
[root@ubuntu2404 ~]#apt update && apt -y install maven
[root@ubuntu2404 ~]#vim /etc/maven/settings.xml 
<mirrors>
    <mirror>
        <id>aliyunmaven</id>
        <mirrorOf>*</mirrorOf>
        <name>Aliyun Maven</name>
        <url>https://maven.aliyun.com/repository/public</url>
    </mirror>
</mirrors>

```



### python连接

```python
[root@ubuntu2404 ~]#apt update && apt -y install python3 python3-kazoo
[root@ubuntu2404 ~]#cat zookeeper_test.py 
#!/usr/bin/python3
from kazoo.client import KazooClient

zk = KazooClient(hosts='10.0.0.200:2181')
zk.start()
# 创建节点：makepath 设置为 True ，父节点不存在则创建，其他参数不填均为默认
zk.create('/zkapp/test',b'this is a test',makepath=True)

#查看指定数据
data=zk.get('/zkapp/test')
print(data)

#查看所有数据
all_data=zk.get_children('/')
print(all_data)

# 操作完后关闭zk连接
zk.stop()

[root@ubuntu2404 ~]#python3 zookeeper_test.py 
```



### **nc** **访问** **ZooKeeper** **管理**

ZooKeeper支持某些特定的四字命令字母与其的交互。它们大多是查询命令，用来获取 ZooKeeper服务的当前状态及相关信息。用户在客户端可以通过 netcat 或telnet向zookeeper发送下面命令

```bash
#常见命令
conf #输出相关服务配置的详细信息
cons #列出所有连接到服务器的客户端的完全的连接/会话的详细信息
envi #输出关于服务环境的详细信息
dump #列出未经处理的会话和临时节点
stat #查看哪个节点被选择作为Follower或者Leader
ruok #测试是否启动了该Server，若回复imok表示已经启动
mntr #输出一些运行时信息
reqs #列出未经处理的请求
wchs #列出服务器watch的简要信息
wchc #通过session列出服务器watch的详细信息
wchp #通过路径列出服务器watch的详细信息
srvr #输出服务的所有信息
srst #重置服务器统计信息
kill #关掉Server，当前版本无法关闭
isro #查看该服务的节点权限信息
```

**命令的安全限制**

```bash
#默认情况下，这些4字命令有可能会被拒绝，提示如下报错
xxxx is not executed because it is not in the whitelist.

#解决办法:在 zoo.cfg文件中添加如下配置,如果是集群需要在所有节点上添加下面配置
# vim conf/zoo.cfg
4lw.commands.whitelist=*

#在服务状态查看命令中有很多存在隐患的命令，为了避免生产中的安全隐患，要对这些"危险"命令进行一些安全限制，只需要编辑服务的zoo.cfg文件即可

# vim conf/zoo.cfg
4lw.commands.whitelist=conf,stat,ruok,isro
```

```bash
[root@ubuntu2404 ~]#nc 127.1 2181
conf
clientPort=2181
secureClientPort=-1
dataDir=/usr/local/zookeeper/logs/version-2
dataDirSize=67110187
dataLogDir=/usr/local/zookeeper/logs/version-2
dataLogSize=67110187
tickTime=2000
maxClientCnxns=60
minSessionTimeout=4000
maxSessionTimeout=40000
clientPortListenBacklog=-1
serverId=0

[root@ubuntu2404 ~]#echo conf | nc 127.1 2181
clientPort=2181
secureClientPort=-1
dataDir=/usr/local/zookeeper/logs/version-2
dataDirSize=67110187
dataLogDir=/usr/local/zookeeper/logs/version-2
dataLogSize=67110187
tickTime=2000
maxClientCnxns=60
minSessionTimeout=4000
maxSessionTimeout=40000
clientPortListenBacklog=-1
serverId=0
```





## kafka

在2006 年的时候，AMQP (Advanced Message Queuing Protocol)规范发布了。它是跨语言和跨平台的，真正地促进了消息队列的繁荣发展。

消息队列主要有以下应用场景

- 削峰填谷

  诸如电商业务中的秒杀、抢红包、企业开门红等大型活动时皆会带来较高的流量脉冲，或因没做相应的保护而导致系统超负荷甚至崩溃，或因限制太过导致请求大量失败而影响用户体验，消息队列可提供削峰填谷的服务来解决该问题。

- 异步解耦

  交易系统作为淘宝等电商的最核心的系统，每笔交易订单数据的产生会引起几百个下游业务系统的关注，包括物流、购物车、积分、流计算分析等等，整体业务系统庞大而且复杂，消息队列可实现异步通信和应用解耦，确保主站业务的连续性。

- 顺序收发

  细数日常中需要保证顺序的应用场景非常多，例如证券交易过程时间优先原则，交易系统中的订单创建、支付、退款等流程，航班中的旅客登机消息处理等等。与先进先出FIFO（First In First Out）原理类似，消息队列提供的顺序消息即保证消息FIFO。

- 分布式事务一致性

  交易系统、支付红包等场景需要确保数据的最终一致性，大量引入消息队列的分布式事务，既可以实现系统之间的解耦，又可以保证最终的数据一致性。

- 大数据分析

  数据在“流动”中产生价值，传统数据分析大多是基于批量计算模型，而无法做到实时的数据分析，利用消息队列与流式计算引擎相结合，可以很方便的实现业务数据的实时分析。

- 分布式缓存同步

  电商的大促，各个分会场琳琅满目的商品需要实时感知价格变化，大量并发访问数据库导致会场页面响应时间长，集中式缓存因带宽瓶颈，限制了商品变更的访问流量，通过消息队列构建分布式缓存，实时通知商品数据的变化

- 蓄流压测

  线上有些链路不方便做压力测试，可以通过堆积一定量消息再放开来压测

**Kafka** **特点和优势**

特点

- 分布式: 支持分布式多主机部署实现
- 分区: 一个消息.可以拆分出多个，分别存储在多个位置
- 多副本: 防止信息丢失，可以多来几个备份
- 多订阅者: 可以有很多应用连接kafka
- Zookeeper: 早期版本的Kafka依赖于zookeeper， 2021年4月19日Kafka 2.8.0正式发布，此版本包括了很多重要改动，最主要的是kafka通过自我管理的仲裁来替代ZooKeeper，即Kafka将不再需要ZooKeeper！

优势

- Kafka 通过 O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以 TB 级别以上的消息存储也能够保持长时间的稳定性能。
- 高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。支持通过Kafka 服务器分区消息。
- 分布式： Kafka 基于分布式集群实现高可用的容错机制，可以实现自动的故障转移
- 顺序保证：在大多数使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。 Kafka保证一个Partiton内的消息的有序性（分区间数据是无序的，如果对数据的顺序有要求，应将在创建主题时将分区数partitions设置为1）
- 支持 Hadoop 并行数据加载
- 通常用于大数据场合,传递单条消息比较大，而Rabbitmq 消息主要是传输业务的指令数据,单条数据较小

### kafkaVSRabbitMQ

**核心架构对比** 

| 特性             | Kafka                                        | RabbitMQ                                    |
| ---------------- | -------------------------------------------- | ------------------------------------------- |
| **架构**         | 分布式日志系统，基于发布-订阅（Pub/Sub）模式 | 基于 AMQP（高级消息队列协议）的传统消息代理 |
| **消息存储**     | 持久化日志（文件存储），消息会被保留一定时间 | 队列模式，消息消费后默认被删除              |
| **数据处理方式** | 批量处理（吞吐量高）                         | 即时消息处理（低延迟）                      |
| **分布式支持**   | 天然支持分布式（多 Broker 架构）             | 需要集群模式支持分布式                      |

------

**2. 主要功能对比**

| 功能             | Kafka                                        | RabbitMQ                                |
| ---------------- | -------------------------------------------- | --------------------------------------- |
| **消息模型**     | 主要基于 **日志存储**（消费者读取偏移量）    | 典型的 **消息队列**（消费者确认后删除） |
| **数据持久性**   | 支持，数据按**时间段**存储，可回溯           | 支持，**确认后删除**                    |
| **吞吐量**       | 高吞吐量（百万级消息/秒）                    | 吞吐量适中（万级消息/秒）               |
| **延迟**         | 毫秒级别（适合大规模日志处理）               | 低于 1ms（适合低延迟应用）              |
| **消息顺序**     | **保证分区内有序**（多个分区时无法全局有序） | **默认无序**，但可以使用 FIFO 队列      |
| **消息确认机制** | 由消费者维护偏移量（offset），手动管理       | ACK 机制，支持自动或手动确认            |
| **消费模式**     | 发布-订阅（多个消费者可读取同一条消息）      | 点对点和发布-订阅模式                   |
| **协议**         | 自定义 Kafka 协议                            | AMQP（支持 STOMP、MQTT）                |

------

**3. 使用场景对比**

| 场景                         | Kafka                                       | RabbitMQ                           |
| ---------------------------- | ------------------------------------------- | ---------------------------------- |
| **日志收集**                 | ✅（ELK、日志分析）                          | ❌（RabbitMQ 不是为日志存储设计的） |
| **流式数据处理**             | ✅（配合 Flink、Spark 等大数据框架）         | ❌                                  |
| **微服务通信**               | ❌（不适合 RPC）                             | ✅（常用于微服务间解耦）            |
| **高吞吐消息队列**           | ✅（亿级 TPS 的应用场景，如日志、埋点）      | ❌                                  |
| **任务队列（RPC 任务调度）** | ❌（Kafka 适合批量处理，不适合短时任务队列） | ✅（延迟低，适合 RPC 调用）         |
| **金融交易/支付**            | ❌（Kafka 无法保证消息强一致性）             | ✅（消息确认机制更安全）            |

------

**4. 适用场景总结**

- **Kafka 适合：**
  - **大数据场景**（日志收集、行为分析）
  - **高吞吐量应用**（流式数据处理）
  - **事件驱动架构**（Event Sourcing）
- **RabbitMQ 适合：**
  - **微服务间通信**
  - **即时消息队列**（订单处理、RPC 任务队列）
  - **需要复杂消息路由的应用**（如基于 AMQP 交换机的多路分发）

------

**5. 选择建议**

| 需求                     | 选择       |
| ------------------------ | ---------- |
| **高吞吐，批量处理日志** | ✅ Kafka    |
| **流式处理，事件驱动**   | ✅ Kafka    |
| **低延迟，可靠交付**     | ✅ RabbitMQ |
| **微服务通信，RPC**      | ✅ RabbitMQ |

如果你在 **大数据** 或 **日志处理** 相关项目上，Kafka 更合适。
 如果你在 **企业系统、微服务通信** 领域，RabbitMQ 更好。

### **Kafka** **角色和流程**

![image-20250309173232345](C:/Users/zhaok/AppData/Roaming/Typora/typora-user-images/image-20250309173232345.png)

![image-20250309173317081](C:/Users/zhaok/AppData/Roaming/Typora/typora-user-images/image-20250309173317081.png)



**Producer**：Producer即生产者，消息的产生者，是消息的入口。负责发布消息到Kafka broker。

**Consumer**：消费者，用于消费消息，即处理消息

**Broker**：Broker是kafka实例，每个服务器上可以有一个或多个kafka的实例，假设每个broker对应一台服务器。每个kafka集群内的broker都有一个**不重复**的编号，如: broker-0、broker-1等……

**Controller：**是整个 Kafka 集群的管理者角色，任何集群范围内的状态变更都需要通过 Controller 进行，在整个集群中是个单点的服务，可以通过选举协议进行故障转移，负责集群范围内的一些关键操作：主题的新建和删除，主题分区的新建、重新分配，Broker 的加入、退出，触发分区 Leader 选举等，每个 Broker 里都有一个 Controller 实例，多个 Broker 的集群同时最多只有一个 Controller 可以对外提供集群管理服务，Controller 可以在 Broker 之间进行故障转移，Kafka 集群管理的工作主要是由 Controller 来完成的，而 Controller 又通过监听Zookeeper 节点的变动来进行监听集群变化事件，Controller 进行集群管理需要保存集群元数据，监听集群状态变化情况并进行处理，以及处理集群中修改集群元数据的请求，这些主要都是利用 Zookeeper 来实现

**Topic** ：消息的主题，可以理解为消息的分类，一个Topic相当于数据库中的一张表,一条消息相当于关系数据库的一条记录，或者一个Topic相当于Redis中列表数据类型的一个Key，一条消息即为列表中的一个元素。kafka的数据就保存在topic。在每个broker上都可以创建多个topic。

虽然一个 topic的消息保存于一个或多个broker 上同一个目录内, 物理上不同 topic 的消息分开存储在不同的文件夹，但用户只需指定消息的topic即可生产或消费数据而不必关心数据存于何处，topic 在逻辑上对record(记录、日志)进行分组保存，消费者需要订阅相应的topic 才能消费topic中的消息。

**Consumer group:** 每个consumer 属于一个特定的consumer group（可为每个consumer 指定 group name，若不指定 group name 则属于默认的group），同一topic的一条消息只能被同一个consumer group 内的一个consumer 消费，类似于一对一的单播机制，但多个consumer group 可同时消费这一消息，类似于一对多的多播机制，默认消费组的多个消费者是共享消息

<img src="5day-png/35kafka主题.png" alt="image-20250309175127705" style="zoom:50%;" />



### 生产者消费者和发布者订阅者

#### 1.**生产者-消费者模型（Producer-Consumer）**

生产者-消费者模式是一种**点对点（P2P）** 的消息队列模型，通常用于**任务分发和负载均衡**。

##### **特点**

- **消息是点对点的**：一个生产者（Producer）发送消息到队列，**一个消费者（Consumer）接收并处理**。
- **消息消费后即被删除**，不会被多个消费者重复消费。
- **典型应用**：任务队列、RPC 调用等。

##### **架构示意**

```
生产者 (Producer)  ->  消息队列 (Queue)  ->  消费者 (Consumer)
```

**示例（RabbitMQ）**

```
Producer ---> [ Queue ] ---> Consumer1
                             Consumer2 (负载均衡)
```

如果有多个消费者，消息会在它们之间**均匀分配**（负载均衡），但**同一条消息只会被一个消费者消费**。

##### **适用场景**

- **异步任务处理**（订单支付、日志存储）
- **负载均衡**（多消费者共享任务）
- **RPC 任务队列**（微服务之间的任务分发）

------

#### **2. 发布-订阅模式（Publish-Subscribe，Pub/Sub）**

发布-订阅模式是一种**广播式**的消息模型，通常用于**事件通知和日志分发**。

##### **特点**

- **多个订阅者可以同时收到相同的消息**（广播）。
- **订阅者可以动态加入或退出**，不会影响发布者的工作。
- **典型应用**：日志监控、事件驱动架构、消息推送。

##### **架构示意**

```
发布者 (Publisher)  ->  主题 (Topic)  ->  订阅者1 (Subscriber)
                                      ->  订阅者2 (Subscriber)
                                      ->  订阅者3 (Subscriber)
```

**示例（Kafka、Redis Pub/Sub）**

```
Publisher ---> [ Topic ] ---> Subscriber1
                               Subscriber2
                               Subscriber3
```

同一条消息可以被多个订阅者消费，不会影响其他订阅者的消费情况。

##### **适用场景**

- **日志收集**（ELK、Kafka 日志中心）
- **事件驱动架构**（微服务事件总线）
- **实时推送**（股票行情、聊天系统）

------

#### **3. 生产者-消费者 vs 发布-订阅 对比**

| **对比项**     | **生产者-消费者（P2P）**         | **发布-订阅（Pub/Sub）**     |
| -------------- | -------------------------------- | ---------------------------- |
| **消息流转**   | **点对点**，一个消费者消费       | **广播**，多个订阅者消费     |
| **消息存储**   | **消费后删除**                   | **可以存储（如 Kafka）**     |
| **消费者角色** | 负载均衡，每条消息只给一个消费者 | 每个订阅者都能收到相同的消息 |
| **典型应用**   | 任务队列、RPC 调度               | 事件驱动、日志收集、推送服务 |
| **消息丢失**   | 消费后即删除，需手动存储         | 通常有持久化机制             |

------

#### **4. Kafka vs RabbitMQ**

| 特性         | **Kafka（Pub/Sub）**               | **RabbitMQ（P2P）**                |
| ------------ | ---------------------------------- | ---------------------------------- |
| **模型**     | 发布-订阅                          | 生产者-消费者                      |
| **消息存储** | 持久化存储（可回溯）               | 消费后删除                         |
| **消费模式** | 订阅模式，多个消费者可消费同一消息 | 负载均衡，消息只能被一个消费者消费 |
| **适用场景** | 日志、流式计算、大数据             | 任务调度、RPC                      |

------

#### **5. 如何选择？**

| **需求**                         | **建议模型**              |
| -------------------------------- | ------------------------- |
| **任务队列**（订单、支付、邮件） | 生产者-消费者（RabbitMQ） |
| **日志收集、数据流处理**         | 发布-订阅（Kafka）        |
| **股票行情、新闻推送**           | 发布-订阅（Kafka/Redis）  |
| **微服务任务分发**               | 生产者-消费者（RabbitMQ） |
| **事件驱动架构**（Event-Driven） | 发布-订阅（Kafka）        |

#### **总结：**

- **RabbitMQ（生产者-消费者）** 适用于任务调度、RPC、微服务通信。
- **Kafka（发布-订阅）** 适用于日志处理、事件驱动、大数据流处理。

### 消费组

Kafka 的 **消费组（Consumer Group）** 是 Kafka 消费者架构的核心概念，它用于实现**并行消费**和**负载均衡**。以下是 Kafka 消费组的图解解析：

------

#### **1. 消费组基本概念**

- **一个主题（Topic）可以被多个消费组订阅**，各个消费组之间**相互独立**。
- **同一消费组内的多个消费者（Consumer）会共同消费一个 Topic**，但**一个分区（Partition）只能被组内的一个消费者消费**。
- **消费组通过 "组协调器"（Group Coordinator）来管理**消费者的负载均衡。

------

#### **2. 消费组示意图**

##### **（1）单个消费者的情况**

当只有一个消费者（Consumer）时，它会消费整个 Topic 的所有分区：

```
Topic: my_topic (3个分区)
-----------------------------------
| P0 | P1 | P2 |   (分区 Partition)
-----------------------------------
        ↓
Consumer A  （消费所有分区）
```

- **缺点**：单个消费者的处理能力有限，无法充分利用 Kafka 的并行能力。

------

##### **（2）多个消费者组成消费组**

当多个消费者组成一个消费组时，每个分区会被**组内的不同消费者消费**：

```
Consumer Group: group_A
-----------------------------------
| P0 | P1 | P2 |   (分区 Partition)
-----------------------------------
  ↓     ↓     ↓
 C1     C2    C3   （3个消费者）
```

- **优势**：多个消费者分担负载，提升吞吐量。
- **规则**：一个分区只能被消费组内的一个消费者消费，但同一 Topic 可以被不同消费组消费。

------

##### **（3）消费者数量少于分区数**

当消费者（Consumer）数量 **少于** 分区（Partition）数量时，部分消费者需要消费多个分区：

```
Consumer Group: group_A
-----------------------------------
| P0 | P1 | P2 | P3 | P4 |  (分区)
-----------------------------------
  ↓     ↓     ↓     ↓    ↓
 C1     C2    C1    C2   C3
```

- **C1 负责 P0 和 P2**
- **C2 负责 P1 和 P3**
- **C3 负责 P4**

**特点**：

- 消费者 **数量小于等于分区数** 时，Kafka 会自动均衡分配分区。
- **消费者越多，负载越均衡**，但不应超过分区数。

------

##### **（4）消费者数量超过分区数**

当**消费者数量多于分区数**，多余的消费者会**闲置**：

```
Consumer Group: group_A
-----------------------------------
| P0 | P1 | P2 |  (分区)
-----------------------------------
  ↓     ↓     ↓   
 C1     C2    C3   
 C4 (空闲) C5 (空闲)
```

- **C1 负责 P0**
- **C2 负责 P1**
- **C3 负责 P2**
- **C4、C5 没有分区可消费（闲置）**

**特点**：

- **消费者数量超过分区数时，Kafka 不会进行分配**，多出的消费者会处于空闲状态。
- **优化方案**：增加 Kafka 主题的分区数，使消费者能够均衡消费。

------

#### **3. 消费组的特性**

- **负载均衡**：消费者组中的消费者数量可以动态调整，Kafka 会自动重新分配分区。
- **高可用性**：如果某个消费者崩溃，Kafka 会将它的分区重新分配给其他消费者。
- **多消费组支持**：多个消费组可以同时消费同一主题，互不影响。

------

#### **4. Kafka 消费组应用场景**

- **日志收集**（ELK）：一个消费组用于存储到 HDFS，另一个用于实时分析。
- **订单系统**：一个消费组处理订单，另一个消费组发送消息通知。
- **大数据流式处理**：Kafka + Flink/Spark 进行流计算。

------

#### **总结**

| **情况**          | **表现**                         |
| ----------------- | -------------------------------- |
| 消费者数 < 分区数 | 消费者承担多个分区的消费任务     |
| 消费者数 = 分区数 | 最优负载均衡，每个分区一个消费者 |
| 消费者数 > 分区数 | 多余的消费者闲置，未分配分区     |

通过**调整分区数**与**消费者数量**的比例，可以优化 Kafka 的消费性能。

### 副本

![image-20250309174810490](5day-png/35kafka副本.png)

**Partition** ：分区，是物理上的概念，每个 topic 分割为一个或多个partition，即一个topic切分为多份, 当创建 topic 时可指定 partition 数量，partition的表现形式就是一个一个的文件夹,该文件夹下存储该partition的数据和索引文件，分区的作用还可以实现负载均衡，提高kafka的吞吐量。同一个topic在不同的分区的数据是不重复的,一般Partition数不要超过节点数，注意同一个partition数据是有顺序的，但不同的partition则是无序的。

**Replication**: 副本，同样数据的副本，包括leader和follower的副本数,基本于数据安全,建议至少2个,是Kafka的高可靠性的保障，和ES的副本有所不同，**Kafka中的副(leader+follower）数包括主分片数,而Elasticsearch中的副本数(follower)**不包括主分片数**为了实现数据的高可用，比如将分区 0 的数据分散到不同的kafka 节点，每一个分区都有一个 broker 作为 Leader 和一个 broker 作为Follower，类似于ES中的主分片和副本分片，

假设分区为 3, 即分三个分区0-2，副本为3，即每个分区都有一个 leader，再加两个follower，分区 0 的leader为服务器A，则服务器 B 和服务器 C 为 A 的follower，而分区 1 的leader为服务器B，则服务器 A 和C 为服务器B 的follower，而分区 2 的leader 为C，则服务器A 和 B 为C 的follower。

**AR**： Assigned Replicas，分区中的所有副本的统称，包括leader和 follower，AR= lSR+ OSR

**lSR**：ln Sync Replicas，所有与leader副本保持同步的副本 follower和leader本身组成的集合，包括leader和 follower，是AR的子集

**OSR**：out-of-Sync Replied，所有与leader副本同步不能同步的 follower的集合，是AR的子集

**分区和副本的优势：**

- 实现存储空间的横向扩容，即将多个kafka服务器的空间组合利用
- 提升性能，多服务器并行读写
- 实区即 leader 分布在不同的kafka 服务器，并且有对应follower 分布在和leader不同的服务器上

![image-20250309175621267](5day-png/35kafka分区和副本.png)

在 Kafka 中，**分区（Partition）** 和 **副本（Replica）** 是核心概念，影响了 Kafka 的**高并发、负载均衡和高可用性**。下面详细解析它们的作用、架构和工作机制。

------

### 分区(Partition)和副本(Replica)

#### **1. Kafka 分区（Partition）**

Kafka 的 **主题（Topic）** 由多个 **分区（Partition）** 组成，每个分区可以独立存储和消费数据。
 **分区的作用：**

- **并行处理**：不同分区可以由不同的消费者消费，提高吞吐量。
- **分布式存储**：一个主题的数据被分布到多个分区，避免单点瓶颈。
- **顺序保证**：Kafka **保证同一分区内消息是有序的**（跨分区无全局顺序）。

##### **分区架构示意**

```
sql复制编辑Topic: my_topic
------------------------------------------------
| Partition 0 | Partition 1 | Partition 2 |
------------------------------------------------
|  msg1, msg2 |  msg3, msg4 |  msg5, msg6 |
```

多个消费者（Consumer）可以并行消费不同的分区：

```
sql复制编辑Consumer Group A:
  - Consumer 1 负责 Partition 0
  - Consumer 2 负责 Partition 1
  - Consumer 3 负责 Partition 2
```

##### **分区分配规则**

Kafka **根据 Key 进行 Hash 分区**，分区方式：

- **随机分区**（默认）
- **基于 Key 进行 Hash**（保证相同 Key 的消息进入同一分区）
- **手动指定分区**

##### **分区的优势**

✅ **提高吞吐量**（不同分区可由不同 Broker 处理）
 ✅ **支持水平扩展**（增加分区可提升并发）
 ✅ **保证局部有序性**（同一 Key 的消息顺序不变）

------

#### **2. Kafka 副本（Replica）**

副本是 Kafka **用于容错和高可用**的机制，每个分区都有多个副本。
 **副本的作用：**

- **防止数据丢失**（主副本宕机后从副本接管）
- **高可用性**（Leader 失效后 Follower 可选举为 Leader）

##### **副本架构**

```
pgsql复制编辑Topic: my_topic (3个分区，每个有3个副本)
------------------------------------------------
| Partition 0  (Leader)  | Replica 1 | Replica 2 |
| Partition 1  (Replica) | Leader    | Replica 3 |
| Partition 2  (Replica) | Replica 4 | Leader    |
------------------------------------------------
```

- **Leader 副本**：处理生产者和消费者的请求，所有写操作都先到 Leader。
- **Follower 副本**：同步 Leader 数据，Leader 宕机时可以选举新的 Leader。

##### **Kafka 副本同步机制**

Kafka 采用 **ISR（In-Sync Replicas）** 机制：

1. 生产者（Producer）只写入 **Leader 副本**。
2. 其他 **Follower 副本** 从 Leader 拉取数据，保持同步。
3. 当 Leader 宕机时，一个 ISR 中的 Follower 副本会被选为新的 Leader。

##### **副本机制的优势**

✅ **防止数据丢失**（至少一个副本存活时数据仍然可用）
 ✅ **Leader 选举保证高可用**（Leader 挂了自动选新的 Leader）
 ✅ **可调节副本数量**（提高可靠性，但会增加存储开销）

------

#### **3. Kafka 分区与副本关系总结**

| **特性**          | **分区（Partition）**     | **副本（Replica）**               |
| ----------------- | ------------------------- | --------------------------------- |
| **作用**          | 提高吞吐量、并行消费      | 容错、数据冗余、高可用            |
| **数量**          | 每个 Topic 由多个分区组成 | 每个分区有多个副本                |
| **存储位置**      | 分布在多个 Broker         | 同一个分区的副本分布在不同 Broker |
| **消息存储**      | 只存储该分区的部分数据    | 复制 Leader 数据，防止丢失        |
| **Leader 作用**   | 处理读写请求              | 负责同步数据给 Follower           |
| **Follower 作用** | 由不同消费者消费不同分区  | 仅复制数据，不处理读写            |

------

#### **4. Kafka 分区 & 副本 应用场景**

| **需求**                         | **解决方案**                |
| -------------------------------- | --------------------------- |
| **高吞吐、大数据处理**           | 增加 **分区数**             |
| **数据可靠性要求高**             | 增加 **副本数**（默认 3）   |
| **防止 Broker 挂掉导致数据丢失** | ISR 副本同步                |
| **保证特定 Key 的顺序性**        | 采用 **Key 进行 Hash 分区** |

##### **Kafka 分区 & 副本 经典配置**

| 需求       | **分区数** | **副本数** |
| ---------- | ---------- | ---------- |
| 日志收集   | 10+        | 2-3        |
| 订单交易   | 5-10       | 3          |
| 实时流处理 | 20+        | 3-5        |

------

#### **5. Kafka 分区 & 副本 设计最佳实践**

1. **分区数 ≈ 消费者数**（尽量均衡，提高并发）

2. **副本数 ≥ 3**（防止数据丢失）

3. **不同分区的副本分布在不同 Broker 上**（避免单点故障）

4. Kafka 配置参数优化

   ```
   properties复制编辑# 设置副本数
   default.replication.factor=3
   
   # 设置最小 ISR 副本数（防止数据丢失）
   min.insync.replicas=2
   ```

------

#### **总结**

- **分区（Partition）** 提供**并行消费、负载均衡**，提高吞吐量。
- **副本（Replica）** 提供**高可用、数据备份**，防止数据丢失。
- **最佳实践** 是合理设置分区和副本数量，以平衡**性能和可靠性**。

对于 **高吞吐大数据场景（日志、流计算）**，Kafka 通过 **分区并行化** 处理数据，而 **副本机制** 确保数据安全和高可用。



### **Kafka** **工作机制**

Kafka的领导者选举过程发生在以下情况中：

1. 当新的分区创建时，Kafka选择ISR（In-Sync Replica）列表中的第一个副本作为领导者。
2. 当领导者失败或无法与ZooKeeper通信时，会重新选举新的领导者。这个过程被称为领导者故障转移（leader failover）。

Kafka领导者选举的详细步骤：

1. 当领导者发生故障，ZooKeeper将会检测到它的会话过期。
2. ZooKeeper接着将通知所有的副本进行领导者选举。
3. 副本们会查看它们在ZooKeeper中存储的元数据并确定新的领导者，选择规则是选取副本集合（ISR）中最新的副本。
4. 一旦新的领导者被选出，ZooKeeper将通知所有的副本更新它们的元数据。

![image-20250309180556092](5day-png/35kafka工作机制.png)

**Kafka的复制配置**

Kafka的复制方式可以通过acks（acknowledgments）配置来实现：

- **acks=0**：生产者发送消息后不等确认即认为成功。这种方式相当于没有复制，只有单个副本存储在Leader上。
- **acks=1**：消息发送到Leader即认为成功，相当于异步复制。在这种情况下，生产者只需要等待Leader的确认，而不需要等待ISR（In-Sync Replicas，同步副本集）中其他副本的确认。
- **acks=all**：需要等待ISR列表中所有



### kafka部署

```bash
#下载链接
http://kafka.apache.org/downloads

kafka_<scala 版本>_<kafka 版本>
#示例:kafka_2.13-2.7.0.tgz
```



#### 单机部署

```bash
#官方文档
http://kafka.apache.org/quickstart
```

##### 基于zookeeper部署

```bash
#安装java
[root@ubuntu2404 ~]#apt update && apt install -y openjdk-21-jdk
[root@ubuntu2404 ~]#tar xf kafka_2.13-3.9.0.tgz -C /usr/local/
[root@ubuntu2404 ~]#cd /usr/local/
[root@ubuntu2404 local]#ls
bin  etc  games  include  kafka_2.13-3.9.0  lib  man  sbin  share  src
[root@ubuntu2404 local]#ln -s kafka_2.13-3.9.0/ kafka
[root@ubuntu2404 local]#cd kafka
[root@ubuntu2404 kafka]#ls 
bin  config  libs  LICENSE  licenses  NOTICE  site-docs

#修改zookeeper配置文件
[root@ubuntu2404 kafka]#vim config/zookeeper.properties 
dataDir=/usr/local/kafka/data/zookeeper

#启动zookeeper
[root@ubuntu2404 kafka]#bin/zookeeper-server-start.sh config/zookeeper.properties 
[root@ubuntu2404 ~]#ss -ntl
LISTEN                0                     50                                               *:2181                                            *:* 
[root@ubuntu2404 ~]#ls /usr/local/kafka/data/
zookeeper

#修改kafka配置文件
[root@ubuntu2404 kafka]#vim config/server.properties 
log.dirs=/usr/local/kafka/data/kafka-logs

#启动kafka
[root@ubuntu2404 kafka]#bin/kafka-server-start.sh config/server.properties 
[root@ubuntu2404 ~]#ss -tnl                                       
LISTEN                0                     50                                               *:37909                                           *:*                                          
LISTEN                0                     50                                               *:2181                                            *:*                                          
LISTEN                0                     50                                               *:35457                                           *:*                                          
LISTEN                0                     50                                               *:9092                                            *:*  
```

```bash
#生产中这样后台启动
[root@ubuntu2404 kafka]#nohup bin/kafka-server-start.sh config/server.properties  &> /dev/null &
[root@ubuntu2404 kafka]#nohup bin/kafka-server-start.sh config/server.properties  &> /dev/null &
```



##### 基于KRaft 部署

**Kafka-v4.0开始即将不再支持Zookeeper**

```
#https://kafka.apache.org/quickstart
```

```bash
#安装java
[root@ubuntu2404 ~]#apt update && apt install -y openjdk-21-jdk
#生成集群唯一ID,Generate a Cluster UUID,集群内的多个节点需使用同一集群ID
[root@ubuntu2404 kafka]#KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
[root@ubuntu2404 kafka]#echo $KAFKA_CLUSTER_ID
OZ_eAd7JSpqwfwdhq2zxBQ
#配置文件
[root@ubuntu2404 kafka]#vim config/kraft/server.properties
log.dirs=/data/kafka
advertised.listeners=PLAINTEXT://10.0.0.200:9092,CONTROLLER://10.0.0.200:9093
#数据初始化
[root@ubuntu2404 kafka]#bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties
Formatting metadata directory /data/kraft-combined-logs with metadata.version 3.9-IV0.
[root@ubuntu2404 kafka]#ls /data/kraft-combined-logs/
bootstrap.checkpoint  meta.properties
#启动kafka
[root@ubuntu2404 kafka]#bin/kafka-server-start.sh config/kraft/server.properties
```

```
[root@ubuntu2404 kafka]#ss -tnulp | grep java
tcp   LISTEN 0      50                 *:9093             *:*    users:(("java",pid=16812,fd=146))        
tcp   LISTEN 0      50                 *:9092             *:*    users:(("java",pid=16812,fd=174))        
tcp   LISTEN 0      50                 *:34443            *:*    users:(("java",pid=16812,fd=127))       
```

```
[root@ubuntu2404 kafka]#/usr/local/kafka/bin/kafka-topics.sh --create --topic kang --bootstrap-server 10.0.0.200:9092
Created topic kang.
[root@ubuntu2404 kafka]#/usr/local/kafka/bin/kafka-topics.sh --list --bootstrap-server 10.0.0.200:9092
kang
```

##### 脚本部署

```

```

#### 集群部署

##### 基于zookeeper的集群

```bash
#修改每个kafka节点的主机名
[root@ubuntu2404 ~]#hostnamectl hostname node1
[root@ubuntu2404 ~]#hostnamectl hostname node2
[root@ubuntu2404 ~]#hostnamectl hostname node3

#在所有kafka节点上实现主机名称解析
[root@node3 ~]#vim /etc/hosts
10.0.0.201 node1
10.0.0.202 node2
10.0.0.203 node3
```

```bash
#安装 JAVA
apt update && apt install -y openjdk-21-jdk

#下载kafka二进制文件，解压缩
https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/3.9.0/kafka_2.13-3.9.0.tgz
[root@node1 ~]#tar xf kafka_2.13-3.9.0.tgz  -C /usr/local/
[root@node1 ~]#cd /usr/local/
[root@node1 local]#ln -s kafka_2.13-3.9.0/ kafka
[root@node1 local]#cd kafka
[root@node1 kafka]#ls
bin  config  libs  LICENSE  licenses  NOTICE  site-docs

#修改zookeeper配置文件
[root@node1 kafka]#vim config/zookeeper.properties 
#必须添加时间相关配置
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5
dataDir=/data/zookeeper
server.1=10.0.0.201:2888:3888
server.2=10.0.0.202:2888:3888
server.3=10.0.0.203:2888:3888

[root@node1 kafka]#mkdir /usr/local/kafka/data/zookeeper/ -p
[root@node1 kafka]#echo 1 > /data/zookeeper/myid

[root@node1 kafka]#rsync -av /usr/local/kafka* 10.0.0.202:/usr/local
[root@node1 kafka]#rsync -av /usr/local/kafka* 10.0.0.203:/usr/local

#修改各个节点的MyID
[root@node2 ~]#echo 2 > /usr/local/kafka/data/zookeeper/myid
[root@node3 ~]#echo 3 > /usr/local/kafka/data/zookeeper/myid

#在所有节点上启动
/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties
```

**配置文件说明**

```bash
#配置文件 ./conf/server.properties内容说明
############################# Server Basics###############################
# broker的id，值为整数，且必须唯一，在一个集群中不能重复，此行必须修改
broker.id=1
############################# Socket ServerSettings ######################
# kafka监听端口，默认9092
listeners=PLAINTEXT://10.0.0.201:9092
# 处理网络请求的线程数量，默认为3个
num.network.threads=3
# 执行磁盘IO操作的线程数量，默认为8个
num.io.threads=8
# socket服务发送数据的缓冲区大小，默认100KB
socket.send.buffer.bytes=102400
# socket服务接受数据的缓冲区大小，默认100KB
socket.receive.buffer.bytes=102400
# socket服务所能接受的一个请求的最大大小，默认为100M
socket.request.max.bytes=104857600
############################# Log Basics###################################
# kafka存储消息数据的目录
log.dirs=../data
# 每个topic默认的partition
num.partitions=1
# 设置副本数量为3,当Leader的Replication故障，会进行故障自动转移。
default.replication.factor=3
# 在启动时恢复数据和关闭时刷新数据时每个数据目录的线程数量
num.recovery.threads.per.data.dir=1
############################# Log FlushPolicy #############################
# 消息刷新到磁盘中的消息条数阈值
log.flush.interval.messages=10000
# 消息刷新到磁盘中的最大时间间隔1s,单位是ms
log.flush.interval.ms=1000
############################# Log RetentionPolicy #########################
# 日志保留小时数，超时会自动删除，默认为7天
log.retention.hours=168
# 日志保留大小，超出大小会自动删除，默认为1G
#log.retention.bytes=1073741824
# 日志分片策略，单个日志文件的大小最大为1G，超出后则创建一个新的日志文件
log.segment.bytes=1073741824
# 每隔多长时间检测数据是否达到删除条件,300s
log.retention.check.interval.ms=300000
############################# Zookeeper ####################################
# Zookeeper连接信息，如果是zookeeper集群，则以逗号隔开，此行必须修改
zookeeper.connect=10.0.0.201:2181,10.0.0.202:2181,10.0.0.203:2181
# 连接zookeeper的超时时间,6s
zookeeper.connection.timeout.ms=6000
# 是否允许删除topic，默认为false，topic只会标记为marked for deletion
delete.topic.enable=true
```

**各节点部署** **Kafka**

```bash
#修改配置文件
[root@node1 kafka]#vim config/server.properties 
broker.id=1
listeners=PLAINTEXT://10.0.0.201:9092
log.dirs=/usrlocal/kafka/data/kafka-logs
zookeeper.connect=10.0.0.201:2181,10.0.0.202:2181,10.0.0.203,2181

[root@node2 kafka]#vim config/server.properties 
broker.id=2
listeners=PLAINTEXT://10.0.0.202:9092
log.dirs=/usrlocal/kafka/data/kafka-logs
zookeeper.connect=10.0.0.201:2181,10.0.0.202:2181,10.0.0.203,2181

[root@node3 kafka]#vim config/server.properties 
broker.id=3
listeners=PLAINTEXT://10.0.0.203:9092
log.dirs=/usrlocal/kafka/data/kafka-logs
zookeeper.connect=10.0.0.201:2181,10.0.0.202:2181,10.0.0.203,2181

#可以调整内存 生产中建议调为内存的一半，最大不超过32G
[root@node1|2|3 ~]#vim /usr/local/kafka/bin/kafka-server-start.sh
...
if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then
    export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"
fi
...

#启动服务
#前台启动
/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties
#后台启动
/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties
```

**kafka的service文件**

```bash
[root@node1 ~]#cat /lib/systemd/system/kafka.service
[unit]
Description=Apache kafka
After=network.target

[service]
Type=simple
#Environment=JAVA_HOME=/data/server/java
PIDFile=/usr/local/kafka/kafka.pid
Execstart=/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server. properties
Execstop=/bin/kill  -TERM ${MAINPID}
Restart=always
RestartSec=20

[Install]
wantedBy=multi-user.target

[root@node1 ~]#systemctl daemon-load
[root@node1 ~]#systemctl restart kafka.service
```

#### kafka基于KRaft协议

```
https://kafka.apache.org/quickstart
```

```bash
#修改每个kafka节点的主机名
[root@ubuntu2404 ~]#hostnamectl hostname node1
[root@ubuntu2404 ~]#hostnamectl hostname node2
[root@ubuntu2404 ~]#hostnamectl hostname node3

#在所有kafka节点上实现主机名称解析
[root@node3 ~]#vim /etc/hosts
10.0.0.201 node1
10.0.0.202 node2
10.0.0.203 node3
#安装 JAVA
apt update && apt install -y openjdk-21-jdk
```

```bash
#配置文件说明
[root@ubuntu2404 kafka]#vim config/kraft/server.properties
# 角色定义（同时作为Broker和Controller）
process.roles=broker,controller
# 节点唯一ID（不同节点需不同）
node.id=1
# 控制器节点列表（格式：node.id@host:port）
controller.quorum.voters=1@kafka01:9093,2@kafka02:9093,3@kafka03:9093
# 监听地址（Broker通信端口和Controller端口）
listeners=PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
# 对外暴露的地址（根据实际IP或域名配置）注意:不支持0.0.0.0
advertised.listeners=PLAINTEXT://kafka01:9092,CONTROLLER://kafka01:9093
# 日志存储目录
log.dirs=/data/kafka-logs
```

```bash
#修改配置文件
[root@node1 kafka]#vim config/kraft/server.properties 
node.id=1
controller.quorum.voters=1@10.0.0.201:9093,2@10.0.0.202:9093,3@10.0.0.203:9093
advertised.listeners=PLAINTEXT://10.0.0.201:9092,CONTROLLER://10.0.0.201:9093
log.dirs=/data/kraft-combined-logs

[root@node2 kafka]#vim config/kraft/server.properties 
node.id=2
controller.quorum.voters=1@10.0.0.201:9093,2@10.0.0.202:9093,3@10.0.0.203:9093
advertised.listeners=PLAINTEXT://10.0.0.202:9092,CONTROLLER://10.0.0.202:9093
log.dirs=/data/kraft-combined-logs

[root@node3 kafka]#vim config/kraft/server.properties 
node.id=3
controller.quorum.voters=1@10.0.0.201:9093,2@10.0.0.202:9093,3@10.0.0.203:9093
advertised.listeners=PLAINTEXT://10.0.0.203:9092,CONTROLLER://10.0.0.203:9093
log.dirs=/data/kraft-combined-logs
```

```bash
# 生成UUID
[root@node1 kafka]#./bin/kafka-storage.sh random-uuid
BP1xgpHnRkmioefCr-FQVg

所有节点需使用同一集群ID。
```

```bash
#初始化元数据存储
每个节点均需执行此操作
# 使用生成的UUID初始化存储目录
./bin/kafka-storage.sh format -t <生成的UUID> -c ./config/kraft/server.properties

[root@node1 kafka]#./bin/kafka-storage.sh format -t BP1xgpHnRkmioefCr-FQVg -c ./config/kraft/server.properties
Formatting metadata directory /data/kraft-combined-logs with metadata.version 3.9-IV0.
[root@node2 kafka]#./bin/kafka-storage.sh format -t BP1xgpHnRkmioefCr-FQVg -c ./config/kraft/server.properties
Formatting metadata directory /data/kraft-combined-logs with metadata.version 3.9-IV0.
[root@node3 kafka]#./bin/kafka-storage.sh format -t BP1xgpHnRkmioefCr-FQVg -c ./config/kraft/server.properties
Formatting metadata directory /data/kraft-combined-logs with metadata.version 3.9-IV0.
```

```bash
#启动服务
#前台启动
/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/kraft/server.properties
#后台启动
/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/kraft/server.properties
```



**测试**

```bash
[root@node1 kafka]#/usr/local/kafka/bin/kafka-topics.sh --describe --bootstrap-server 10.0.0.201:9092 --topic kang
Topic: kang     TopicId: rNtFDdTLQRGPr5pnW7IXtw PartitionCount: 3       ReplicationFactor: 2    Configs: 
        Topic: kang     Partition: 0    Leader: 3       Replicas: 3,1   Isr: 3,1        Elr: N/A        LastKnownElr: N/A
        Topic: kang     Partition: 1    Leader: 1       Replicas: 1,2   Isr: 1,2        Elr: N/A        LastKnownElr: N/A
        Topic: kang     Partition: 2    Leader: 2       Replicas: 2,3   Isr: 2,3        Elr: N/A        LastKnownElr: N/A
```

![image-20250311151525212](C:/Users/zhaok/AppData/Roaming/Typora/typora-user-images/image-20250311151525212.png)

| 分区 | **Leader（主副本）** | Follower（从副本） |
| ---- | -------------------- | ------------------ |
| 0    | **Broker 3**         | Broker 1           |
| 1    | **Broker 1**         | Broker 2           |
| 2    | **Broker 2**         | Broker 3           |



```bash
[root@node1 ~]#ls /usr/local/kafka/data/kafka-logs/
cleaner-offset-checkpoint  kang-0  kang-1  log-start-offset-checkpoint  meta.properties  recovery-point-offset-checkpoint  replication-offset-checkpoint
[root@node2 ~]#ls /usr/local/kafka/data/kafka-logs/
cleaner-offset-checkpoint  kang-1  kang-2  log-start-offset-checkpoint  meta.properties  recovery-point-offset-checkpoint  replication-offset-checkpoint
[root@node3 ~]#ls /usr/local/kafka/data/kafka-logs/
cleaner-offset-checkpoint  kang-0  kang-2  log-start-offset-checkpoint  meta.properties  recovery-point-offset-checkpoint  replication-offset-checkpoint
```



```bash
[root@node2 kafka]#/usr/local/kafka/bin/kafka-topics.sh --create --topic hzk --bootstrap-server 10.0.0.201:9092 --partitions 3 --replication-factor 3
Created topic hzk.
[root@node2 kafka]#/usr/local/kafka/bin/kafka-topics.sh --describe --bootstrap-server 10.0.0.202:9092 --topic hzk
Topic: hzk      TopicId: Y16ZvnfrTHSM1OJV3cdT_Q PartitionCount: 3       ReplicationFactor: 3    Configs: segment.bytes=1073741824
        Topic: hzk      Partition: 0    Leader: 1       Replicas: 1,2,3 Isr: 1,2,3      Elr:    LastKnownElr: 
        Topic: hzk      Partition: 1    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1      Elr:    LastKnownElr: 
        Topic: hzk      Partition: 2    Leader: 3       Replicas: 3,1,2 Isr: 3,1,2      Elr:    LastKnownElr: 
```



### kafka读写数据

```bash
kafka-topics.sh 			#消息的管理命令
kafka-console-producer.sh	#生产者的模拟命令
kafka-console-consumer.sh	#消费者的模拟命令
```

**修改host文件**，**要不去互联网上反向解析地址**

```bash
[root@ubuntu2404 ~]#cat /etc/hosts
10.0.0.200 ubuntu2404.wang.org
```

#### **创建** **Topic**

```bash
#新版命令,通过--bootstrap-server指定kafka的地址
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --create --topic kang --bootstrap-server 10.0.0.200:9092
Created topic kang.

#创建多副本
[root@node1 kafka]#/usr/local/kafka/bin/kafka-topics.sh --create --topic kang --bootstrap-server 10.0.0.201:9092 --partitions 3 --replication-factor 2
Created topic kang.

#旧版命令，通过--zookeeper指定zookeeper的地址
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --create --topic kang --zookeeper 10.0.0.200:9092
```

#### **获取所有** **Topic**

```bash
#新版命令
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --list --bootstrap-server 10.0.0.200:9092
kang

#旧版命令
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --list --zookeeper 10.0.0.200:9092
kang
```

#### **查看** **Topic** **详情**

```bash
#新版命令
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --describe --bootstrap-server 10.0.0.200:9092 --topic kang
Topic: kang     TopicId: YNAhx6ghQQ2j9nsWeQKCvw PartitionCount: 1       ReplicationFactor: 1    Configs: 
        Topic: kang     Partition: 0    Leader: 0       Replicas: 0     Isr: 0  Elr: N/A        LastKnownElr: N/A

#旧版命令
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --describe --zookeeper 10.0.0.200:9092 --topic kang
```

#### **生产** **Topic**

```bash
#发送消息命令格式:
kafka-console-producer.sh --broker-list <kafkaIP1>:<端口>,<kafkaIP2>:<端口> --topic <topic名称> --producer-property group.id=<组名>
```

范例：

```bash
#交互式输入消息,按Ctrl+C退出
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-console-producer.sh --broker-list 10.0.0.200:9092 --topic kang
>hello1
>hello2
>hello
```

#### **消费** **Topic**

```
#接收消息命令格式:
kafka-console-consumer.sh --bootstrap-server <host>:<post> --topic <topic名称> --from-beginning --consumer-property group.id=<组名称>
```

注意：

- 消息者先生产消息，消费者后续启动，也能收到之前生产的消息
- 同一个消息在同一个group内的消费者只有被一个消费者消费，比如：共100条消息，在一个group内有A，B两个消费者，其中A消费50条，B消费另外的50条消息。从而实现负载均衡，不同group内的消费者则可以同时消费同一个消息
- --from-beginning 表示消费前发布的消息也能收到，默认只能收到消费后发布的新消息

范例：

```bash
#交互式持续接收消息,按Ctrl+C退出
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-console-consumer.sh --topic kang --bootstrap-server 10.0.0.200:9092

[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-console-consumer.sh --topic kang --bootstrap-server 10.0.0.200:9092 --from-beginning
hello1
hello2
hello

#一个消息同时只能被同一个组内一个消费者消费（单播机制），实现负载均衡，而不能组可以同时消费同一个消息（多播机制）
[root@node2 ~]#/usr/local/kafka/bin/kafka-console-consumer.sh --topic kang --bootstrap-server 10.0.0.200:9092 --from-beginning --consumer-property group.id=group1
[root@node2 ~]#/usr/local/kafka/bin/kafka-console-consumer.sh --topic kang --bootstrap-server 10.0.0.200:9092 --from-beginning --consumer-property group.id=group1
```

#### **删除** **Topic**

```bash
#注意：需要修改配置文件server.properties中的delete.topic.enable=true并重启 逻辑上删除  
#新版本
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-topics.sh --delete --bootstrap-server 10.0.0.200:9092 --topic kang
```



### 消息积压

**Kafka 消息积压可能由多种原因引起，以下是一些可能的原因：**

1. **消费者处理速度慢：** 如果消费者处理消息的速度不足以跟上生产者的速度，就会导致消息积压。这可能是因为消费者逻辑复杂、消费者数量不足、消费者宕机或者网络延迟等原因引起的。
2. **消费者宕机：** 如果某个消费者宕机，其负责处理的分区将没有消费者来消费消息，导致消息在该分区上积压
3. **网络问题：** 网络故障可能导致生产者和消费者之间的通信延迟或中断，从而影响消息的传递速度
4. **硬件资源不足：** Kafka 集群所在的机器，包括生产者、消费者和 Broker 所在的机器，可能由于 CPU、内存或磁盘等资源不足，导致消息处理速度变慢
5. **分区不均匀：** 如果某些分区的负载比其他分区更高，可能导致这些分区上的消息积压。这可能是由于分区数量设置不合理、数据分布不均匀等原因引起的
6. **生产者速度过快：** 如果生产者生产消息的速度过快，而消费者无法及时处理，就会导致消息积压
7. **配置不当：** Kafka 的一些配置参数，如副本数、分区数、消费者数量等，需要根据实际情况进行合理的配置。如果配置不当，可能导致消息积压问题。
8. **异常情况：** 突发性的异常情况，如硬件故障、网络故障、软件 bug 等，都可能导致消息积压

**Kafka 消息积压可能的解决方案：**

- **增加消费者数量**：如果消费者处理速度不足导致消息积压，可以增加消费者的数量来提高处理速度。
- **扩展****Kafka****集群**：如果消息积压是由于Kafka集群的吞吐量达到极限导致的，考虑扩展Kafka集群的规模来增加其处理能力。
- **数据分区**：合理划分数据分区可以提高并行处理能力，从而减少消息积压。
- **数据清理**：定期清理过期的数据和日志文件，以释放磁盘空间并提高性能。
- **优化消费者代码**：检查消费者代码，确保它们是高效的。可能存在一些性能瓶颈或不必要的延迟。
- **调整****Kafka****配置**：根据需要调整Kafka的配置参数，例如增加分区数量、调整副本数量、调整日志清理策略等。
- **监控和警报**：设置监控系统，及时发现消息积压问题并发送警报。这样可以在问题出现之前采取行动。
- **故障排除**：检查系统日志，查找可能的问题源，并采取相应的措施解决问题。

**kafka 要发现消息积压，可以考虑以下方法：**

- **监控工具：** Kafka 提供了一些监控工具，例如 Kafka Manager、Burrow、Kafka Offset Monitor 等。这些工具可以帮助你监控每个分区的偏移量（offset）和消费者组的状态。通过检查偏移量的增长速度，你可以判断是否有消息积压
- **Consumer Lag****：** Consumer Lag 是指消费者组相对于生产者的消息偏移量的差异。通过监控 Consumer Lag，你可以了解消费者是否跟上了生产者的速度。如果 Consumer Lag 增长较快，可能表示消息积压
- **Kafka Logs** **目录：** Kafka 的每个分区都有一个日志目录，其中包含了该分区的消息数据。可以检查每个分区的日志目录，查看是否有大量的未消费的消息
- **Kafka Broker Metrics****：** Kafka 提供了一系列的 broker metrics，包括消息入队速率、出队速率等。通过监控这些指标，可以了解Kafka 集群的负载状况
- **操作系统资源：** 如果 Kafka 所在的机器资源不足，可能导致消息积压。监控 CPU、内存、磁盘等系统资源，确保它们没有达到极限
- **警报系统：** 设置警报系统，当某些指标达到预定的阈值时触发警报，通知运维人员或相关团队及时处理

范例：生成消息

```bash
#模拟生产者生产大量消息
[root@ubuntu2404 ~]#( while true; do echo $i;let i++;done ) | /usr/local/kafka/bin/kafka-console-producer.sh --broker-list 10.0.0.200:9092 --topic kang

#下面命令查看消费组中每个Topic的堆积消息数。“LAG”表示每个Topic的总堆积数
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server 10.0.0.200:9092 --describe --all-groups

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                           HOST            CLIENT-ID
group1          kang            0          1213331         1287994         74663           console-consumer-8a552f58-205a-4a6a-9e33-8bad79e751a6 /10.0.0.200     console-consumer
Consumer group 'group2' has no active members.

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
group2          kang            0          6               1285823         1285817         -               -               -
```



```bash
#查看消息积压
[root@ubuntu2404 ~]#/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server 10.0.0.200:9092 --describe --all-groups
```



### kafka 工具

#### offset explorer

```
https://www.kafkatool.com/download.html
```

#### **kafka-eagle**

```
http://www.kafka-eagle.org/
https://github.com/smartloli/kafka-eagle-bin
https://www.cnblogs.com/smartloli/
```

#### **kafka-UI**

```
https://docs.kafka-ui.provectus.io/
https://github.com/provectus/kafka-ui
https://hub.docker.com/r/provectuslabs/kafka-ui/
```

```
[root@ubuntu2404 ~]#docker run --name kafka-ui -it -p 8080:8080 -e DYNAMIC_CONFIG_ENABLED=true provectuslabs/kafka-ui
[root@ubuntu2404 ~]#docker run --name kafka-ui -it -p 8080:8080 -e DYNAMIC_CONFIG_ENABLED=true registry.cn-beijing.aliyuncs.com/wangxiaochun/kafka-ui:v0.7.2
```

#### **Kafka Manager**

```
https://github.com/yahoo/CMAK
```



## **RabbitMQ**

### RabbitMQ简介

Erlang 语言

**RabbitMQ** **优势：**

- 基于Erlang 语言开发，具有高并发优点、支持分布式
- 具有消息确认机制、消息持久化机制，消息可靠性和集群可靠性高、简单易用、运行稳定、跨平台、多语言
- 自带图形 Web 管理功能
- 开源

**RabbitMQ** **架构**

![image-20250311161746687](5day-png/35rabbirMQ构架.png)

**Message:** 消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。

**Publisher:** 消息的生产者，就是一个向交换器发布消息的客户端应用程序。

**Consumer:** 消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。

**Broker**: 接收和分发消息的应用，表示消息队列服务器实体。RabbitMQ Server 就是Message Broker。

**Virtual host**: 为了支持多租户和安全原因，当多个不同的用户使用同一个RabbitMQ Server时，可以先划分出多个 vhost，每个用户使独立的vhost创建exchange／queue 等。每一个虚拟主机表示一批交换器、消息队列和相关对象的集合。虚拟主机是共享相同的身份认证加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost是AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。

**Exchange**: 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。message 到达 broker 会由Exchange 根据分发规则，匹配查询表中的路由信息，分发消息到队列中。常用的类型：direct (point to point 点对点),topic (publish subscribe发布者订阅者)和fanout (multicast多播)等

**Queue:** 消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直队列里面，等待消费者连接到这个队列将其取走。

**Binding**: 绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将换器理解成一个由绑定构成的路由表。exchange 和 queue 之间的虚拟连接，binding 中可以包含 routing key。Binding 信息被保存到exchange 中的查询表中，用于message 的分发依据。

**Connection**: 网络连接，比如一个TCP连接。publisher／consumer 和 broker 之间的TCP 连接。

**Channel**: 信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内部的虚拟连接，AMQP 命令都是通过道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。Channel 是在 connection内部建立的逻辑连接，如果应用程序支持多线程，通常每个 thread创建单独的channel 进行通讯，AMQP method 包含了channel id 帮助客户端和messagebroker极大减少了操作系统建立TCP connection 的开销

**RabbitMQ** **生产者消费者**

<img src="5day-png/35RabbiotMQ生产者消费者.png" alt="image-20250311162023413" style="zoom:50%;" />

- 生产者发送消息到 broker server（RabbitMQ）
- 在 Broker 内部，用户创建Exchange／Queue，通过 Binding 规则将两者联系在一起
- Exchange 分发消息，根据类型／binding 的不同分发策略有区别
- 消息最后来到Queue 中，等待消费者取走。



### **RabbitMQ** **单机部署**

```
红帽系统安装
https://rabbitmq.com/install-rpm.html
Debian和Ubuntu安装
https://rabbitmq.com/install-debian.html
通用二进制安装
https://rabbitmq.com/install-generic-unix.html
github 下载地址
https://github.com/rabbitmq/rabbitmq-server/releases
```



```
https://www.rabbitmq.com/download.html
```

#### 包安装

```bash
[root@ubuntu2404 ~]#apt-cache madison rabbitmq-server
rabbitmq-server | 3.12.1-1ubuntu1.1 | http://mirrors.ustc.edu.cn/ubuntu noble-proposed/main amd64 Packages
rabbitmq-server | 3.12.1-1ubuntu1 | http://mirrors.ustc.edu.cn/ubuntu noble/main amd64 Packages
rabbitmq-server | 3.12.1-1ubuntu1 | http://cn.archive.ubuntu.com/ubuntu noble/main amd64 Packages
rabbitmq-server | 3.12.1-1ubuntu1 | http://mirrors.ustc.edu.cn/ubuntu noble/main Sources
rabbitmq-server | 3.12.1-1ubuntu1.1 | http://mirrors.ustc.edu.cn/ubuntu noble-proposed/main Sources
```

**主机名解析(可选)**

早期版本安装必须做主机名解析,否则无法启动RabbitMQ服务,并且后续不能再修改主机名

注意: 当前版本不再有此限制

在当前MQ 服务器配置本地主机名解析：

```bash
[root@ubuntu2404 ~]#hostnamectl hostname rabbitmq
```

```bash
#apt包安装
[root@rabbitmq ~]#apt install rabbitmq-server -y
```

```bash
#官方安装脚本
https://www.rabbitmq.com/install-debian.html#apt-bintray-quick-start
```

```bash
#!/bin/bash

. /etc/os-release

sudo apt update && apt-get install curl gnupg apt-transport-https -y

## Team RabbitMQ's main signing key
curl -1sLf "https://keys.openpgp.org/vks/v1/by-fingerprint/0A9AF2115F4687BD29803A206B73A36E6026DFCA" | sudo gpg --dearmor | sudo tee /usr/share/keyrings/com.rabbitmq.team.gpg > /dev/null
## Community mirror of Cloudsmith: modern Erlang repository
curl -1sLf https://github.com/rabbitmq/signing-keys/releases/download/3.0/cloudsmith.rabbitmq-erlang.E495BB49CC4BBE5B.key | sudo gpg --dearmor | sudo tee /usr/share/keyrings/rabbitmq.E495BB49CC4BBE5B.gpg > /dev/null
## Community mirror of Cloudsmith: RabbitMQ repository
curl -1sLf https://github.com/rabbitmq/signing-keys/releases/download/3.0/cloudsmith.rabbitmq-server.9F4587F226208342.key | sudo gpg --dearmor | sudo tee /usr/share/keyrings/rabbitmq.9F4587F226208342.gpg > /dev/null

## Add apt repositories maintained by Team RabbitMQ
sudo tee /etc/apt/sources.list.d/rabbitmq.list <<EOF
## Provides modern Erlang/OTP releases
##
deb [signed-by=/usr/share/keyrings/rabbitmq.E495BB49CC4BBE5B.gpg] https://ppa1.novemberain.com/rabbitmq/rabbitmq-erlang/deb/ubuntu ${VERSION_CODENAME} main
deb-src [signed-by=/usr/share/keyrings/rabbitmq.E495BB49CC4BBE5B.gpg] https://ppa1.novemberain.com/rabbitmq/rabbitmq-erlang/deb/ubuntu ${VERSION_CODENAME} main

# another mirror for redundancy
deb [signed-by=/usr/share/keyrings/rabbitmq.E495BB49CC4BBE5B.gpg] https://ppa2.novemberain.com/rabbitmq/rabbitmq-erlang/deb/ubuntu ${VERSION_CODENAME} main
deb-src [signed-by=/usr/share/keyrings/rabbitmq.E495BB49CC4BBE5B.gpg] https://ppa2.novemberain.com/rabbitmq/rabbitmq-erlang/deb/ubuntu ${VERSION_CODENAME} main

## Provides RabbitMQ
##
deb [signed-by=/usr/share/keyrings/rabbitmq.9F4587F226208342.gpg] https://ppa1.novemberain.com/rabbitmq/rabbitmq-server/deb/ubuntu ${VERSION_CODENAME} main
deb-src [signed-by=/usr/share/keyrings/rabbitmq.9F4587F226208342.gpg] https://ppa1.novemberain.com/rabbitmq/rabbitmq-server/deb/ubuntu ${VERSION_CODENAME} main

# another mirror for redundancy
deb [signed-by=/usr/share/keyrings/rabbitmq.9F4587F226208342.gpg] https://ppa2.novemberain.com/rabbitmq/rabbitmq-server/deb/ubuntu ${VERSION_CODENAME} main
deb-src [signed-by=/usr/share/keyrings/rabbitmq.9F4587F226208342.gpg] https://ppa2.novemberain.com/rabbitmq/rabbitmq-server/deb/ubuntu ${VERSION_CODENAME} main
EOF

## Update package indices
sudo apt-get update -y

## Install Erlang packages
sudo apt-get install -y erlang-base \
                        erlang-asn1 erlang-crypto erlang-eldap erlang-ftp erlang-inets \
                        erlang-mnesia erlang-os-mon erlang-parsetools erlang-public-key \
                        erlang-runtime-tools erlang-snmp erlang-ssl \
                        erlang-syntax-tools erlang-tftp erlang-tools erlang-xmerl

## Install rabbitmq-server and its dependencies
sudo apt-get install rabbitmq-server -y --fix-missing
```

```bash
[root@rabbitmq ~]#systemctl status rabbitmq-server
```



#### 二进制安装

```
通用二进制安装官方说明
RabbitMQ版本和Erlang版本对应要求
范例: 二进制安装一键安装脚本
https://www.rabbitmq.com/install-generic-unix.html
https://www.rabbitmq.com/which-erlang.html
```

### **RabbitMQ** **管理**

#### **启用远程登陆** **Web** **管理界面**

```bash
#官方文档
https://www.rabbitmq.com/management.html
```

```bash
#端口说明
5672   #客户端访问的端口
15672  #web 管理端口,默认没有打开,需要启用插件
25672  #集群状态通信端口
```

**安装** **Web** **管理插件** **rabbitmq_management**

```bash
[root@rabbitmq ~]#rabbitmq-plugins list
Listing plugins with pattern ".*" ...
 Configured: E = explicitly enabled; e = implicitly enabled
 | Status: * = running on rabbit@rabbitmq
 |/
[  ] rabbitmq_amqp1_0                  4.0.7
[  ] rabbitmq_auth_backend_cache       4.0.7
[  ] rabbitmq_auth_backend_http        4.0.7
[  ] rabbitmq_auth_backend_ldap        4.0.7
[  ] rabbitmq_auth_backend_oauth2      4.0.7
[  ] rabbitmq_auth_mechanism_ssl       4.0.7
[  ] rabbitmq_consistent_hash_exchange 4.0.7
[  ] rabbitmq_event_exchange           4.0.7
[  ] rabbitmq_federation               4.0.7
[  ] rabbitmq_federation_management    4.0.7
[  ] rabbitmq_federation_prometheus    4.0.7
[  ] rabbitmq_jms_topic_exchange       4.0.7
[  ] rabbitmq_management               4.0.7
[  ] rabbitmq_management_agent         4.0.7
[  ] rabbitmq_mqtt                     4.0.7
[  ] rabbitmq_peer_discovery_aws       4.0.7
[  ] rabbitmq_peer_discovery_common    4.0.7
[  ] rabbitmq_peer_discovery_consul    4.0.7
[  ] rabbitmq_peer_discovery_etcd      4.0.7
[  ] rabbitmq_peer_discovery_k8s       4.0.7
[  ] rabbitmq_prometheus               4.0.7
[  ] rabbitmq_random_exchange          4.0.7
[  ] rabbitmq_recent_history_exchange  4.0.7
[  ] rabbitmq_sharding                 4.0.7
[  ] rabbitmq_shovel                   4.0.7
[  ] rabbitmq_shovel_management        4.0.7
[  ] rabbitmq_shovel_prometheus        4.0.7
[  ] rabbitmq_stomp                    4.0.7
[  ] rabbitmq_stream                   4.0.7
[  ] rabbitmq_stream_management        4.0.7
[  ] rabbitmq_top                      4.0.7
[  ] rabbitmq_tracing                  4.0.7
[  ] rabbitmq_trust_store              4.0.7
[  ] rabbitmq_web_dispatch             4.0.7
[  ] rabbitmq_web_mqtt                 4.0.7
[  ] rabbitmq_web_mqtt_examples        4.0.7
[  ] rabbitmq_web_stomp                4.0.7
[  ] rabbitmq_web_stomp_examples       4.0.7
[root@rabbitmq ~]#rabbitmq-plugins enable rabbitmq_management
Enabling plugins on node rabbit@rabbitmq:
rabbitmq_management
The following plugins have been configured:
  rabbitmq_management
  rabbitmq_management_agent
  rabbitmq_web_dispatch
Applying plugin configuration to rabbit@rabbitmq...
The following plugins have been enabled:
  rabbitmq_management
  rabbitmq_management_agent
  rabbitmq_web_dispatch

started 3 plugins.
[root@rabbitmq ~]#rabbitmq-plugins list
Listing plugins with pattern ".*" ...
 Configured: E = explicitly enabled; e = implicitly enabled
 | Status: * = running on rabbit@rabbitmq
 |/
[  ] rabbitmq_amqp1_0                  4.0.7
[  ] rabbitmq_auth_backend_cache       4.0.7
[  ] rabbitmq_auth_backend_http        4.0.7
[  ] rabbitmq_auth_backend_ldap        4.0.7
[  ] rabbitmq_auth_backend_oauth2      4.0.7
[  ] rabbitmq_auth_mechanism_ssl       4.0.7
[  ] rabbitmq_consistent_hash_exchange 4.0.7
[  ] rabbitmq_event_exchange           4.0.7
[  ] rabbitmq_federation               4.0.7
[  ] rabbitmq_federation_management    4.0.7
[  ] rabbitmq_federation_prometheus    4.0.7
[  ] rabbitmq_jms_topic_exchange       4.0.7
[E*] rabbitmq_management               4.0.7
[e*] rabbitmq_management_agent         4.0.7
[  ] rabbitmq_mqtt                     4.0.7
[  ] rabbitmq_peer_discovery_aws       4.0.7
[  ] rabbitmq_peer_discovery_common    4.0.7
[  ] rabbitmq_peer_discovery_consul    4.0.7
[  ] rabbitmq_peer_discovery_etcd      4.0.7
[  ] rabbitmq_peer_discovery_k8s       4.0.7
[  ] rabbitmq_prometheus               4.0.7
[  ] rabbitmq_random_exchange          4.0.7
[  ] rabbitmq_recent_history_exchange  4.0.7
[  ] rabbitmq_sharding                 4.0.7
[  ] rabbitmq_shovel                   4.0.7
[  ] rabbitmq_shovel_management        4.0.7
[  ] rabbitmq_shovel_prometheus        4.0.7
[  ] rabbitmq_stomp                    4.0.7
[  ] rabbitmq_stream                   4.0.7
[  ] rabbitmq_stream_management        4.0.7
[  ] rabbitmq_top                      4.0.7
[  ] rabbitmq_tracing                  4.0.7
[  ] rabbitmq_trust_store              4.0.7
[e*] rabbitmq_web_dispatch             4.0.7
[  ] rabbitmq_web_mqtt                 4.0.7
[  ] rabbitmq_web_mqtt_examples        4.0.7
[  ] rabbitmq_web_stomp                4.0.7
[  ] rabbitmq_web_stomp_examples       4.0.7
```

**注意: rabbitmq从3.3.0开始禁止使用guest/guest权限通过除localhost外的访问，直接访问报错**

**开启用户** **guest** **远程登录功能**

```bash
#配置文件说明
https://www.rabbitmq.com/configure.html
```

默认管理页面只能能过本机127.0.0.1:15672访问，可以开启guest用户远程登录功能

```bash
[root@rabbitmq ~]#ls /etc/rabbitmq/
enabled_plugins

#创建配置文件开启远程登录功能
[root@rabbitmq ~]#echo "loopback_users = none" > /etc/rabbitmq/rabbitmq.conf
[root@rabbitmq ~]#systemctl restart rabbitmq-server
```

```
#打开浏览器访问下面地址,默认用用户和密码都是guest登录
http://10.0.0.200服务器:15672
```



#### **修改guest密码**

登录ui界面——》Admin——》guest——》Update this user

#### **创建用户**

- management

  用户可以访问管理插件

- policymaker

  用户可以访问管理插件并管理他们有权访问的虚拟主机的策略和参数。

- monitoring

  用户可以访问管理插件并查看所有连接和频道以及节点相关的信息。

- administrator

  用户可以执行监控可以执行的所有操作，管理用户、虚拟主机和权限，关闭其他用户的连接，以及管理所有虚拟主机的策略和参数。

```bash
[root@rabbitmq ~]#rabbitmqctl list_users
Listing users ...
user    tags
guest   [administrator]

#创建用户,但默认无权限无法登录web
[root@rabbitmq ~]#rabbitmqctl add_user admin 123456
Adding user "admin" ...
Done. Don't forget to grant the user permissions to some virtual hosts! See 'rabbitmqctl help set_permissions' to learn more.

#确认用户
[root@rabbitmq ~]#rabbitmqctl list_users
Listing users ...
user    tags
guest   [administrator]
admin   []

#修改密码
[root@rabbitmq ~]#rabbitmqctl change_password admin 666666

#加入管理员角色
[root@rabbitmq ~]#rabbitmqctl set_user_tags admin administrator

#验证加入角色，可以登录Web管理页面进行管理，但仍然无法访问virtual hosts,需要授权才可以
[root@rabbitmq ~]#rabbitmqctl list_users
Listing users ...
user    tags
guest   [administrator]
admin   [administrator]

#查看权限
[root@rabbitmq ~]#rabbitmqctl list_user_permissions admin
Listing permissions for user "admin" ...

#设置vhost权限，格式如下
rabbitmqctl [--node <node>] [--longnames] [--quiet] set_permissions [--vhost <vhost>] <username> <conf> <write> <read>
#设置对vhost “/” 给admin用户授予:configure配置，write写，read读的权限，“.*”表示正则表达式的任意字符串
#默认vhost为 "/"

#创建 deamo-vhost 虚拟主机
[root@rabbitmq ~]#rabbitmqctl add_vhost deamo-vhost
Adding vhost "deamo-vhost" ...

#给指定vhost设置权限
[root@rabbitmq ~]#rabbitmqctl set_permissions --vhost deamo-vhost admin ".*" ".*" ".*"
Setting permissions for user "admin" in vhost "deamo-vhost" ...

#验证权限
[root@rabbitmq ~]#rabbitmqctl list_user_permissions admin
Listing permissions for user "admin" ...
vhost   configure       write   read
deamo-vhost     .*      .*      .*

#列出虚拟主机
[root@rabbitmq ~]#rabbitmqctl list_vhosts
Listing vhosts ...
name
deamo-vhost
/

#删除用户
[root@rabbitmq ~]#rabbitmqctl delete_user admin
Deleting user "admin" ...
```



#### **创建** **queue**

Queues and Streams ——》Add a new queue

![image-20250311171649790](5day-png/35创建queue.png)

```bash
#创建虚拟主机
[root@rabbitmq ~]#rabbitmqctl add_vhost vhost1
Adding vhost "vhost1" ...

#验证虚拟主机
[root@rabbitmq ~]#rabbitmqctl list_vhosts
Listing vhosts ...
name
vhost1
deamo-vhost
/

#删除虚拟主机
[root@rabbitmq ~]#rabbitmqctl delete_vhost vhost1
Deleting vhost "vhost1" ...

#默认查看/ 虚拟主机的队列
[root@rabbitmq ~]#rabbitmqctl list_queues
Timeout: 60.0 seconds ...
Listing queues for vhost / ...
name messages
queue-test 0

#列出指定vhost的queue队列
[root@rabbitmq ~]#rabbitmqctl list_queues -p deamo-vhost
Timeout: 60.0 seconds ...
Listing queues for vhost deamo-vhost ...
name    messages
test    0

[root@rabbitmq ~]#rabbitmqctl list_queues --vhost /demo-vhost
Timeout: 60.0 seconds ...
Listing queues for vhost /demo-vhost ...
name messages
test    0

#删除队列
[root@rabbitmq ~]#rabbitmqctl delete_queue queue-test
```

### **Python** **客户端访问** **RabbitMQ**

#### **生产者（producter）**

```bash
[root@rabbitmq ~]#apt -y install python3 python3-pip
#加速配置
[root@rabbitmq ~]#python3 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
[root@rabbitmq ~]#apt install -y python3-pika
#以前版本
[root@mq-server ~]#pip3 install pika
```

```python
[root@rabbitmq ~]#cat rabbitmq_product.py 
#!/usr/bin/python3
import pika
import json

username = 'guest'
password = 'guest'
host = '127.0.0.1'
port = 5672

def publish_messages():
    credentials = pika.PlainCredentials(username, password)
    connection = pika.BlockingConnection(pika.ConnectionParameters(host=host, port=port, virtual_host='/', credentials=credentials))
    channel = connection.channel()
    
    queue_name = 'rabbit-test'
    channel.queue_declare(queue=queue_name)

    for i in range(10):
        message = json.dumps({'OrderId': f"1000{i}"})
        channel.basic_publish(exchange='', routing_key=queue_name, body=message)
        print(message)

    connection.close()

if __name__ == "__main__":
    publish_messages()

```

#### **消费者（consumer）**

```python
[root@rabbitmq ~]#cat rabbitmq_consumer.py 
#!/usr/bin/python3

import pika

# Define MQ parameters
username = 'guest'
password = 'guest'
host = '127.0.0.1'
port = 5672

def consume_messages():
    credentials = pika.PlainCredentials(username, password)
    connection = pika.BlockingConnection(pika.ConnectionParameters(host=host, port=port, virtual_host='/', credentials=credentials))
    channel = connection.channel()

    channel.queue_declare(queue='rabbit-test', durable=False)

    def callback(ch, method, properties, body):
        print(body.decode())
        ch.basic_ack(delivery_tag=method.delivery_tag)

    channel.basic_consume(queue='rabbit-test', on_message_callback=callback, auto_ack=False)

    channel.start_consuming()

if __name__ == "__main__":
    consume_messages()
```

```bash
[root@rabbitmq ~]#python3 rabbitmq_product.py 
{"OrderId": "10000"}
{"OrderId": "10001"}
{"OrderId": "10002"}
{"OrderId": "10003"}
{"OrderId": "10004"}
{"OrderId": "10005"}
{"OrderId": "10006"}
{"OrderId": "10007"}
{"OrderId": "10008"}
{"OrderId": "10009"}
[root@rabbitmq ~]#rabbitmqctl list_queues
Timeout: 60.0 seconds ...
Listing queues for vhost / ...
name    messages
rabbit-test     10

[root@rabbitmq ~]#python3 rabbitmq_consumer.py 
{"OrderId": "10000"}
{"OrderId": "10001"}
{"OrderId": "10002"}
{"OrderId": "10003"}
{"OrderId": "10004"}
{"OrderId": "10005"}
{"OrderId": "10006"}
{"OrderId": "10007"}
{"OrderId": "10008"}
{"OrderId": "10009"}
[root@rabbitmq ~]#rabbitmqctl list_queues
Timeout: 60.0 seconds ...
Listing queues for vhost / ...
name    messages
rabbit-test     0
```

**RabbitMQ API**

```
https://rawcdn.githack.com/rabbitmq/rabbitmq-management/rabbitmq_v3_6_9/priv/www/api/index.html
```

```bash
[root@node1 ~]#apt update && apt -y install jq
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/overview|jq
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/nodes|jq
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/cluster-name
{"name":"rabbit@node1.wang.org"}
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/connections|jq
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/channels|jq
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/queues|jq
[root@node1 ~]#curl -u guest:guest http://10.0.0.200:15672/api/whoami|jq
{"name":"guest","tags":"administrator"}
```

范例: 健康性检查

```bash
[root@rabbitmq ~]#curl -u guest:guest http://10.0.0.200:15672/api/healthchecks/node
{"status":"ok"}
[root@node1 ~]#curl -u guest:guest http://10.0.0.101:15672/api/healthchecks/node
{"status":"ok"}
[root@node1 ~]#curl -u guest:guest http://10.0.0.101:15672/api/healthchecks/node/rabbit@node3
{"status":"ok"}
[root@node3 ~]#rabbitmqctl stop_app
Stopping rabbit application on node rabbit@node3 ...
[root@node3 ~]#curl -u guest:guest http://10.0.0.101:15672/api/healthchecks/node/rabbit@node3
{"status":"failed","reason":"{'EXIT',{aborted,{no_exists,rabbit_vhost}}}"}
[root@node3 ~]#rabbitmqctl start_app
Starting node rabbit@node3 ...
[root@node1 ~]#curl -u guest:guest http://10.0.0.101:15672/api/healthchecks/node/rabbit@node3
{"status":"ok"}
[root@node3 ~]#systemctl stop rabbitmq-server.service 
[root@node1 ~]#curl -u guest:guest http://10.0.0.101:15672/api/healthchecks/node/rabbit@node3
{"status":"failed","reason":"nodedown"}
```

**利用** **RabbitMQ API** **监控集群状态**

```python
[root@rabbitmq ~]#cat rabbitmq_monitor.py 
#!/usr/bin/python3
# -*- coding: utf-8 -*-

import requests

username = "guest"
password = "guest"
host = "localhost"
port = 15672
min_running_nodes = 3  # Set the minimum number of running nodes here

def get_node_status():
    url = f"http://{username}:{password}@{host}:{port}/api/nodes"
    try:
        response = requests.get(url)
        response.raise_for_status()
        nodes = response.json()
        running_nodes = [node["name"] for node in nodes if node.get("running")]
        return running_nodes
    except Exception as e:
        print(f"Error: {e}")
        return []

def count_server():
    running_nodes = get_node_status()
    if len(running_nodes) < min_running_nodes:
        print("集群异常")
    else:
        print("集群正常")

def main():
    count_server()

if __name__ == "__main__":
    main()
```

### **RabbitMQ** **集群部署**

RabbitMQ集群模式分为两种:普通和镜像模式

- **普通集群模式：**

  创建好RabbitMQ集群之后的默认模式

  queue 创建之后，如果没有其它policy，消息实体只存在于其中一个节点

  假设 A、B 两个 Rabbitmq节点仅有相同的元数据，即队列结构，但队列的数据仅保存有一份，即创建该队列的rabbitmq节点（A 节点）

  当消息进入 A 节点的 Queue 中后，consumer 从 B节点拉取时，RabbitMQ 会临时在 A、B 间进行消息传输，把 A中的消息实体取出并经过 B 发送给consumer，所以 consumer可以连接每一个节点，从中取消息

  该模式存在一个问题就是当 A 节点故障后，B节点无法取到A 节点中还未消费的消息实体。可能会丢失数据

  缺点： 没有数据高可用

  优点：多个节点可以实现负载均衡

- **镜像集群模式：** 

  **注意: 此模式从RabbitMQ-4.X以后不再支持**

  把需要的队列做成镜像队列，存在于多个节点，属于 RabbitMQ 的 HA方案（镜像模式是在普通模式的基础上，增加一些镜像策略）

  该模式解决了普通模式中的数据丢失问题，其实质和普通模式不同之处在于，消息实体会主动在镜像节点间同步，而不是在consumer取数据时临时拉取

  该模式带来的副作用也很明显，除了降低系统性能外，如果镜像队列数量过多，加之大量的消息进入，集群内部的网络带宽将会被种同步通讯大大消耗掉，所以在对可靠性要求较高的场合中适用

  一个队列想做成镜像队列，需要先设置policy， 然后客户端创建队列的时候，rabbitmq集群根据“队列名称”自动设置是普通集群模或镜像队列。

  优点：此模式因数据有冗余

  缺点：性能不佳

**集群中两种节点类型**:

- 内存节点：只将数据保存到内存。
- 磁盘节点：保存数据到内存和磁盘。

内存节点执行效率比磁盘节点要高，集群中只需要一个磁盘节点来保存数据就可以

如果集群中只有内存节点，那么不能全部停止它们，否则所有数据消息在服务器全部停机之后都会丢失。

<img src="5day-png/35集群设计构架.png" alt="image-20250311183909034" style="zoom:50%;" />

**环境准备**

```bash
[root@ubuntu2404 ~]#hostnamectl hostname node1
[root@ubuntu2404 ~]#hostnamectl hostname node2
[root@ubuntu2404 ~]#hostnamectl hostname node3
[root@ubuntu2404 ~]#vim /etc/hosts
10.0.0.201 node1
10.0.0.202 node2
10.0.0.203 node3
```

```bash
#使用官方脚本安装Rabbitmq单机版
[root@node1 ~]#bash install_rabbitmq_for_ubuntu.sh
[root@node2 ~]#bash install_rabbitmq_for_ubuntu.sh
[root@node3 ~]#bash install_rabbitmq_for_ubuntu.sh

#停止rabbitmq
[root@node1 ~]#systemctl stop rabbitmq-server.service 
[root@node2 ~]#systemctl stop rabbitmq-server.service 
[root@node3 ~]#systemctl stop rabbitmq-server.service 

#启用 Web 管理
[root@node1 ~]#rabbitmq-plugins enable rabbitmq_management
[root@node2 ~]#rabbitmq-plugins enable rabbitmq_management
[root@node3 ~]#rabbitmq-plugins enable rabbitmq_management
#启用远程管理功能
[root@node1 ~]#echo "loopback_users = none" > /etc/rabbitmq/rabbitmq.conf
[root@node1 ~]#systemctl restart rabbitmq-server.service
[root@node2 ~]#echo "loopback_users = none" > /etc/rabbitmq/rabbitmq.conf
[root@node2 ~]#systemctl restart rabbitmq-server.service
[root@node3 ~]#echo "loopback_users = none" > /etc/rabbitmq/rabbitmq.conf
[root@node3 ~]#systemctl restart rabbitmq-server.service
```

**同步** **cookie** **文件**

RabbitMQ 的集群是依赖于 erlang 的集群来工作的，所以必须先构建起 erlang的集群环境

而 Erlang 的集群中各节点是通过一个 magic cookie来实现的

这个 cookie 存放在 /var/lib/rabbitmq/.erlang.cookie 中

文件是 400的权限,所以必须保证各节点cookie 保持一致，否则节点之间就无法通信。

```bash
[root@node1 ~]#ll /var/lib/rabbitmq/.erlang.cookie
-r-------- 1 rabbitmq rabbitmq 57 Mar 11 18:54 /var/lib/rabbitmq/.erlang.cookie
[root@node1 ~]#cat /var/lib/rabbitmq/.erlang.cookie
o2WO16Vjq1TY3ypSLDvoMSQgepSmAthPzuukYrInnn381ydrr0CXSbha


[root@node1 ~]#scp /var/lib/rabbitmq/.erlang.cookie node2:/var/lib/rabbitmq/  
[root@node1 ~]#scp /var/lib/rabbitmq/.erlang.cookie node3:/var/lib/rabbitmq/

#重启服务生效
[root@node2 ~]#systemctl restart rabbitmq-server.service
[root@node3 ~]#systemctl restart rabbitmq-server.service
```

**创建集群**

```bash
#查看每一个节点都是独立运行的
[root@node1 ~]#rabbitmqctl cluster_status
```

```bash
#在 node2,node3 节点执行以下命令作为内存节点添加到 node1 所在集群
#将node2也加入集群
[root@node2 ~]#rabbitmqctl stop_app       #停止app服务
[root@node2 ~]#rabbitmqctl reset           #清空元数据
#将node2添加到集群当中，并成为内存节点，不加--ram 默认是磁盘节点,此步依赖于/etc/hosts的名称解析
[root@node2 ~]#rabbitmqctl join_cluster rabbit@node1 --ram

#启动app服务
[root@node2 ~]#rabbitmqctl start_app      
#将node3也加入集群
[root@node3 ~]#rabbitmqctl stop_app
[root@node3 ~]#rabbitmqctl reset
[root@node3 ~]#rabbitmqctl join_cluster rabbit@node1 --ram
[root@node3 ~]#rabbitmqctl start_app
```

查看集群状态

```bash
[root@node1 ~]#rabbitmqctl cluster_status
Cluster status of node rabbit@node1 ...
Basics

Cluster name: rabbit@node1
Total CPU cores available cluster-wide: 6

Disk Nodes

rabbit@node1

RAM Nodes

rabbit@node2
rabbit@node3

Running Nodes

rabbit@node1
rabbit@node2
rabbit@node3

Versions

rabbit@node1: RabbitMQ 3.12.1 on Erlang 25.3.2.8
rabbit@node2: RabbitMQ 3.12.1 on Erlang 25.3.2.8
rabbit@node3: RabbitMQ 3.12.1 on Erlang 25.3.2.8

CPU Cores

Node: rabbit@node1, available CPU cores: 2
Node: rabbit@node2, available CPU cores: 2
Node: rabbit@node3, available CPU cores: 2

Maintenance status

Node: rabbit@node1, status: not under maintenance
Node: rabbit@node2, status: not under maintenance
Node: rabbit@node3, status: not under maintenance

Alarms

(none)

Network Partitions

(none)

Listeners

Node: rabbit@node1, interface: [::], port: 15672, protocol: http, purpose: HTTP API
Node: rabbit@node1, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication
Node: rabbit@node1, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0
Node: rabbit@node2, interface: [::], port: 15672, protocol: http, purpose: HTTP API
Node: rabbit@node2, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication
Node: rabbit@node2, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0
Node: rabbit@node3, interface: [::], port: 15672, protocol: http, purpose: HTTP API
Node: rabbit@node3, interface: [::], port: 25672, protocol: clustering, purpose: inter-node and CLI tool communication
Node: rabbit@node3, interface: [::], port: 5672, protocol: amqp, purpose: AMQP 0-9-1 and AMQP 1.0

Feature flags

Flag: classic_mirrored_queue_version, state: enabled
Flag: classic_queue_type_delivery_support, state: enabled
Flag: direct_exchange_routing_v2, state: enabled
Flag: drop_unroutable_metric, state: enabled
Flag: empty_basic_get_metric, state: enabled
Flag: feature_flags_v2, state: enabled
Flag: implicit_default_bindings, state: enabled
Flag: listener_records_in_ets, state: enabled
Flag: maintenance_mode_status, state: enabled
Flag: quorum_queue, state: enabled
Flag: restart_streams, state: enabled
Flag: stream_queue, state: enabled
Flag: stream_sac_coordinator_unblock_group, state: enabled
Flag: stream_single_active_consumer, state: enabled
Flag: tracking_records_in_ets, state: enabled
Flag: user_limits, state: enabled
Flag: virtual_host_metadata, state: enabled
```



## **Nacos**

- **服务注册与发现**：适配 Spring Cloud 服务注册与发现标准，默认集成对应 Spring Cloud 版本所支持的负载均衡组件的适配。
- **分布式配置管理**：支持分布式系统中的外部化配置，配置更改时自动刷新。

```
https://spring.io/projects/spring-cloud-alibaba
https://sca.aliyun.com/en/
https://github.com/alibaba/spring-cloud-alibaba/blob/2022.x/README-zh.md
```

### Spring Cloud Alibaba

#### 主要功能

- **服务限流降级**：默认支持 WebServlet、WebFlux、OpenFeign、RestTemplate、Spring Cloud Gateway、Dubbo 和 RocketMQ 限流降级功能的接入，可以在运行时通过控制台实时修改限流降级规则，还支持查看限流降级 Metrics 监控。
- **服务注册与发现**：适配 Spring Cloud 服务注册与发现标准，默认集成对应 Spring Cloud 版本所支持的负载均衡组件的适配。
- **分布式配置管理**：支持分布式系统中的外部化配置，配置更改时自动刷新。
- **消息驱动能力**：基于 Spring Cloud Stream 为微服务应用构建消息驱动能力。
- **分布式事务**：使用 @GlobalTransactional 注解， 高效并且对业务零侵入地解决分布式事务问题。
- **阿里云对象存储**：阿里云提供的海量、安全、低成本、高可靠的云存储服务。支持在任何应用、任何时间、任何地点存储和访问任意类型的数据。
- **分布式任务调度**：提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。同时提供分布式的任务执行模型，如网格任务。网格任务支持海量子任务均匀分配到所有 Worker（schedulerx-client）上执行。
- **阿里云短信服务**：覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。

#### 组件

**[Sentinel](https://github.com/alibaba/Sentinel)**：把流量作为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。

**[Nacos](https://github.com/alibaba/Nacos)**：一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。

**[RocketMQ](https://rocketmq.apache.org/)**：一款开源的分布式消息系统，基于高可用分布式集群技术，提供低延时的、高可靠的消息发布与订阅服务。

**[Seata](https://github.com/apache/incubator-seata)**：阿里巴巴开源产品，一个易于使用的高性能微服务分布式事务解决方案。

**[Alibaba Cloud OSS](https://www.aliyun.com/product/oss)**: 阿里云对象存储服务（Object Storage Service，简称 OSS），是阿里云提供的海量、安全、低成本、高可靠的云存储服务。您可以在任何应用、任何时间、任何地点存储和访问任意类型的数据。

**[Alibaba Cloud SchedulerX](https://cn.aliyun.com/aliware/schedulerx)**: 阿里中间件团队开发的一款分布式任务调度产品，提供秒级、精准、高可靠、高可用的定时（基于 Cron 表达式）任务调度服务。

**[Alibaba Cloud SMS](https://www.aliyun.com/product/sms)**: 覆盖全球的短信服务，友好、高效、智能的互联化通讯能力，帮助企业迅速搭建客户触达通道。

 ![image-20250311191849787](5day-png/35Spring Cloud Alibaba 的微服务架构.png)

### nacos

Nacos提供四大功能。

- **服务发现和服务健康检查**

  Nacos 使服务能够轻松地通过 DNS 或 HTTP 接口注册自己并发现其他服务。Nacos 还提供服务的实时健康检查，以防止向不健康的主机或服务实例发送请求。

- **动态配置管理**

  动态配置服务让您能够跨所有环境以集中、动态的方式管理所有服务的配置。Nacos 让您在配置更新时无需重新部署应用和服务，让配置变更更加高效、敏捷。

- **动态 DNS 服务**

  Nacos 支持加权路由，让您能够更轻松地在数据中心内的生产环境中实现中间层负载均衡、灵活的路由策略、流量控制和简单的 DNS 解析服务。它可以帮助您轻松实现基于 DNS 的服务发现，并防止应用程序与特定于供应商的服务发现 API 耦合。

- **服务和元数据管理**

  Nacos 提供了一个易于使用的服务仪表板，帮助您管理服务元数据、配置、kubernetes DNS、服务健康和指标统计。

**Nacos** **基本架构及概念**

![image-20250311192842206](5day-png/35Nacos 基本架构及概念.png)

**相关概念**

- 服务 (Service)

  服务是指一个或一组软件功能（例如特定信息的检索或一组操作的执行），其目的是不同的客户端可以为不同的目的重用（例如通过跨进程的网络调用）。Nacos 支持主流的服务生态，如 Kubernetes Service、gRPC|Dubbo RPC Service 或者 Spring Cloud RESTful service。

- 服务注册中心 (Service Registry)

  服务注册中心，它是服务，其实例及元数据的数据库。服务实例在启动时注册到服务注册表，并在关闭时注销。服务和路由器的客户端查询服务注册表以查找服务的可用实例。服务注册中心可能会调用服务实例的健康检查 API 来验证它是否能够处理请求。

- 服务元数据 (Service Metadata)

  服务元数据是指包括服务端点(endpoints)、服务标签、服务版本号、服务实例权重、路由规则、安全策略等描述服务的数据。

- 服务提供方 (Service Provider)

  是指提供可复用和可调用服务的应用方。

- 服务消费方 (Service Consumer)

  是指会发起对某个服务调用的应用方。

- 配置 (Configuration)

  在系统开发过程中通常会将一些需要变更的参数、变量等从代码中分离出来独立管理，以独立的配置文件的形式存在。目的是让静态的系统工件或者交付物（如 WAR，JAR 包等）更好地和实际的物理运行环境进行适配。配置管理一般包含在系统部署的过程中，由系统管理员或者运维人员完成这个步骤。配置变更是调整系统运行时的行为的有效手段之一。

- 配置管理 (Configuration Management)

  在数据中心中，系统中所有配置的编辑、存储、分发、变更管理、历史版本管理、变更审计等所有与配置相关的活动统称为配置管理。

- 名字服务 (Naming Service)

  提供分布式系统中所有对象(Object)、实体(Entity)的“名字”到关联的元数据之间的映射管理服务，例如 ServiceName -> Endpoints Info, Distributed Lock Name -> Lock Owner/Status Info, DNS Domain Name -> IP List, 服务发现和 DNS 就是名字服务的2大场景。

- 配置服务 (Configuration Service)

  在服务或者应用运行过程中，提供动态配置或者元数据以及配置管理的服务提供者。

**Nacos** **逻辑架构及其组件介绍**

![image-20250311193018038](5day-png/35Nacos 基本架构及概念1.png)

- 服务管理：实现服务CRUD，域名CRUD，服务健康状态检查，服务权重管理等功能
- 配置管理：实现配置管CRUD，版本管理，灰度管理，监听管理，推送轨迹，聚合数据等功能
- 元数据管理：提供元数据CURD 和打标能力
- 插件机制：实现三个模块可分可合能力，实现扩展点SPI机制
- 事件机制：实现异步化事件通知，sdk数据变化异步通知等逻辑
- 日志模块：管理日志分类，日志级别，日志可移植性（尤其避免冲突），日志格式，异常码+帮助文档
- 回调机制：sdk通知数据，通过统一的模式回调用户处理。接口和数据结构需要具备可扩展性
- 寻址模式：解决ip，域名，nameserver、广播等多种寻址模式，需要可扩展
- 推送通道：解决server与存储、server间、server与sdk间推送性能问题
- 容量管理：管理每个租户，分组下的容量，防止存储被写爆，影响服务可用性
- 流量管理：按照租户，分组等多个维度对请求频率，长链接个数，报文大小，请求流控进行控制
- 缓存机制：容灾目录，本地缓存，server缓存机制。容灾目录使用需要工具
- 启动模式：按照单机模式，配置模式，服务模式，dns模式，或者all模式，启动不同的程序+UI
- 一致性协议：解决不同数据，不同一致性要求情况下，不同一致性机制
- 存储模块：解决数据持久化、非持久化存储，解决数据分片问题
- Nameserver：解决namespace到clusterid的路由问题，解决用户环境与nacos物理环境映射问题
- CMDB：解决元数据存储，与三方cmdb系统对接问题，解决应用，人，资源关系
- Metrics：暴露标准metrics数据，方便与三方监控系统打通
- Trace：暴露标准trace，方便与SLA系统打通，日志白平化，推送轨迹等能力，并且可以和计量计费系统打通
- 接入管理：相当于阿里云开通服务，分配身份、容量、权限过程
- 用户管理：解决用户管理，登录，sso等问题
- 权限管理：解决身份识别，访问控制，角色管理等问题
- 审计系统：扩展接口方便与不同公司审计系统打通
- 通知系统：核心数据变更，或者操作，方便通过SMS系统打通，通知到对应人数据变更
- OpenAPI：暴露标准Rest风格HTTP接口，简单易用，方便多语言集成
- Console：易用控制台，做服务管理、配置管理等操作
- SDK：多语言sdk
- Agent：dns-f类似模式，或者与mesh等方案集成
- CLI：命令行对产品进行轻量化管理，像git一样好用

**Nacos** **注册中心工作机制**

![image-20250311193059111](5day-png/35Nacos 注册中心工作机制.png)

随着应用架构向微服务架构迁移,服务数量的增加和动态部署、动态扩展的特性,使得服务地址和端口在运行时随时可变

需要一个组件管理当前注册到服务注册与发现中心的微服务实例元数据信息,包括服务实例的服务名、IP 地址、端口号、服务描述和服务状态等,与注册到服务发现与注册中心的微服务实例维持心跳定期检查注册表中的服务实例是否在线，并剔除无效服务实例信息,并提供服务发现能力，为服务调用方提供服务提供方的服务实例元数据

服务提供者分类

- 临时实例: 服务提供者定期周期主动向nacos 发送心跳监测报告,如果一段时间后,nacos无法收到心跳,则删除此实例
- 非临时实例(永久实例): nacos 主动定时监测此类实例,如果提供者实例异常,则并不会删除只是标记此实例异常,等待此实例恢复

服务消费者

- 消费者定时向nacos 注册中心PULL拉取提供者信息,并加以缓存
- 如果提供者有变化,nacos会主动向消费者PUSH推送消息通知,Eureka不支持主动PUSH

集群模式

- Nacos 默认使用 AP ( Availability和Partiton tolerance)模式,存在非临时实例时,会采用CP(Consistency和Partiton tolerance)模式
- Eureka 采用 AP( Availability和Partiton tolerance)模式

集群数据一致性实现

- CP模式基于强一致性协议 Raft
- AP模式基于 阿里的Distro(基于Gossip和Eureka协议优化而来)最终一致性的AP 分布式协议

### 临时实例和非临时实例

在 **Nacos（Naming and Configuration Service）** 中，实例分为 **临时实例（Ephemeral Instance）** 和 **非临时实例（Non-Ephemeral Instance）**。它们的主要区别如下：

#### 1. **临时实例（Ephemeral Instance）**

##### **特性**

- **默认情况下，Nacos 注册的实例是临时实例。**
- 依赖于 **心跳检测** 机制，如果服务实例在一定时间内（默认 5 秒）没有发送心跳，Nacos 认为该实例不可用，并从服务列表中移除。
- **适用于短时存在的实例**，如 **微服务、Kubernetes Pod、自动伸缩实例** 等。

##### **配置**

在 Nacos 客户端注册服务时，临时实例的默认值为 `true`：

```java
namingService.registerInstance("my-service", "127.0.0.1", 8080);
```

或者：

```java
Instance instance = new Instance();
instance.setIp("127.0.0.1");
instance.setPort(8080);
instance.setEphemeral(true);  // 设置为临时实例（默认值）
namingService.registerInstance("my-service", instance);
```

##### **适用场景**

- **微服务架构**：实例数量动态变化，如 Kubernetes 自动扩缩容、Spring Cloud 服务等。
- **短生命周期的应用**：比如**计算节点、任务调度**等。

------

#### 2. **非临时实例（Non-Ephemeral Instance）**

##### **特性**

- 非临时实例**不会因心跳丢失而被自动移除**，即使服务崩溃或宕机，Nacos 仍然认为它是存在的，直到被**手动下线**。
- 适用于 **长生命周期的服务**，例如 **数据库、缓存服务器、消息队列等**。

##### **配置**

在注册时，显式设置 `ephemeral=false`：

```java
Instance instance = new Instance();
instance.setIp("127.0.0.1");
instance.setPort(8080);
instance.setEphemeral(false);  // 设置为非临时实例
namingService.registerInstance("my-service", instance);
```

如果是 **Spring Cloud Alibaba Nacos** 版本，可在 `application.properties` 配置：

```
spring.cloud.nacos.discovery.ephemeral=false
```

##### **适用场景**

- **有状态服务（Stateful Services）**：如数据库、缓存服务器、消息队列等，不希望因短暂网络波动导致服务下线。
- **人工维护的服务**：运维人员希望手动管理实例状态，而不是依赖 Nacos 的心跳机制。

------

#### 3. **对比总结**

| 类型           | 适用场景                         | 心跳机制 | 失联后自动剔除           | 需要手动注销 |
| -------------- | -------------------------------- | -------- | ------------------------ | ------------ |
| **临时实例**   | 微服务、K8s 容器、短生命周期实例 | 需要     | 是（5 秒无心跳自动剔除） | 否           |
| **非临时实例** | 数据库、缓存、人工维护的服务     | 不需要   | 否（必须手动删除）       | 是           |

------

##### 4. **临时实例 vs 非临时实例 选用建议**

- **如果服务是短时运行、弹性扩缩容的，建议使用** **临时实例**（默认）。
- **如果服务需要持久化、不会自动删除，建议使用** **非临时实例**。

例如：

- **Spring Cloud 微服务** → `ephemeral=true`（默认）
- **MySQL、Redis 等长时间运行的服务** → `ephemeral=false`

##### **如何注销非临时实例？**

如果使用了 **非临时实例**，在实例关闭时需要手动注销：

```
namingService.deregisterInstance("my-service", "127.0.0.1", 8080);
```

否则，Nacos 会一直认为这个实例是存在的，即使它已经宕机。

------

#### 5. **总结**

- **临时实例**：默认类型，基于心跳，失联会被自动剔除，适用于短生命周期的微服务。
- **非临时实例**：不会被自动剔除，需要手动管理，适用于持久化服务。

如果你的 **服务可能随时会崩溃或重启**，且 **希望它在崩溃时被自动剔除**，就用 **临时实例（默认）**。
 如果你的 **服务需要长期存在**，即使宕机了 **也不希望它自动被 Nacos 删除**，就用 **非临时实例**。



### Nacos部署

```
https://nacos.io/docs/v2.4/manual/admin/deployment/deployment-overview/?spm=5238cd80.2ef5001f.0.0.3f613b7cmDsk8X
```

#### Nacos部署手册

> Nacos定义为一个IDC内部应用组件，并非面向公网环境的产品，建议在内部隔离网络环境中部署，强烈不建议部署在公共网络环境。
>
> 以下文档中提及的VIP，网卡等所有网络相关概念均处于内部网络环境。

##### 1. Nacos部署架构

Nacos2.X版本新增了gRPC的通信方式，因此需要增加2个端口。新增端口是在配置的主端口(server.port，默认8848)基础上，进行一定偏移量自动生成，具体端口内容及偏移量请参考如下：

| 端口 | 与主端口的偏移量 | 描述                                                       |
| ---- | ---------------- | ---------------------------------------------------------- |
| 9848 | 1000             | 客户端gRPC请求服务端端口，用于客户端向服务端发起连接和请求 |
| 9849 | 1001             | 服务端gRPC请求服务端端口，用于服务间同步等                 |
| 7848 | -1000            | Jraft请求服务端端口，用于处理服务端间的Raft相关请求        |

> **使用VIP/nginx请求时，需要配置成TCP转发，不能配置http2转发，否则连接会被nginx断开。**
>
> **对外暴露端口时，只需要暴露主端口（默认8848）和gRPC端口（默认9848），其他端口为服务端之间的通信端口，请勿暴露其他端口，同时建议所有端口均不暴露在公网下。**

![nacos2_port_exposure.png](https://nacos.io/img/doc/manual/admin/deployment/deploy-port-export.svg)

客户端拥有相同的计算逻辑，用户如同1.X的使用方式，配置主端口(默认8848)，通过相同的偏移量，计算对应gRPC端口(默认9848)。

因此如果客户端和服务端之前存在端口转发，或防火墙时，需要对端口转发配置和防火墙配置做相应的调整。

##### 2. Nacos支持三种部署模式

- 单机模式 - 又称单例模式，主要用于测试和单机试用。
- 集群模式 - 主要用于生产环境，确保高可用。
- 多集群模式（TODO） - 用于多数据中心场景。

![Nacos部署模式图](https://nacos.io/img/doc/overview/deploy-structure.svg)

###### 2.1. 单机模式

单机模式又称`单例模式`, 拥有所有Nacos的功能及特性，具有极易部署、快速启动等优点。但无法与其他节点组成集群，无法在节点或网络故障时提供高可用能力。单机模式同样可以使用内置Derby数据库（默认）和外置数据库进行存储。

单机模式主要适合于工程师于本地搭建或于测试环境中搭建Nacos环境，主要用于开发调试及测试使用；也能够兼顾部分对稳定性和可用性要求不高的业务场景。

单机模式的部署参考文档: [单机模式部署](https://nacos.io/docs/v2.4/manual/admin/deployment/deployment-standalone/)

###### 2.2. 集群模式

集群模式通过自研一致性协议Distro以及Raft协议，将多个Nacos节点构建成了高可用的Nacos集群。数据将在集群中各个节点进行同步，保证数据的一致性。集群模式具有高可用、高扩展、高并发等优点，确保在故障发生时不影响业务的运行。集群模式**默认**采用外置数据库进行存储，但也可以通过内置数据库进行存储。

该模式主要适合于生产环境，也是社区最为推荐的部署模式。

集群模式的部署参考文档: [集群模式部署](https://nacos.io/docs/v2.4/manual/admin/deployment/deployment-cluster/)

###### 2.3. 多集群模式（TODO）

Nacos支持NameServer路由请求模式，通过它您可以设计一个有用的映射规则来控制请求转发到相应的集群，在映射规则中您可以按命名空间或租户等分片请求…

##### 3. 多网卡IP选择

当本地环境比较复杂的时候，Nacos服务在启动的时候需要选择运行时使用的IP或者网卡。Nacos从多网卡获取IP参考Spring Cloud设计，通过nacos.inetutils参数，可以指定Nacos使用的网卡和IP地址。目前支持的配置参数有:

- ip-address参数可以直接设置nacos的ip

```
nacos.inetutils.ip-address=10.11.105.155
```

- use-only-site-local-interfaces参数可以让nacos使用局域网ip，这个在nacos部署的机器有多网卡时很有用，可以让nacos选择局域网网卡

```
nacos.inetutils.use-only-site-local-interfaces=true
```

- ignored-interfaces支持网卡数组，可以让nacos忽略多个网卡

```
nacos.inetutils.ignored-interfaces[0]=eth0
nacos.inetutils.ignored-interfaces[1]=eth1
```

- preferred-networks参数可以让nacos优先选择匹配的ip，支持正则匹配和前缀匹配

```
nacos.inetutils.preferred-networks[0]=30.5.124.
nacos.inetutils.preferred-networks[0]=30.5.124.(25[0-5]|2[0-4]\\d|((1d{2})|([1-9]?\\d))),30.5.124.(25[0-5]|2[0-4]\\d|((1d{2})|([1-9]?\\d))
```

#### 单机Nacos基于Derby数据库部署

二进制安装

```
https://github.com/alibaba/nacos/releases/download/2.5.0/nacos-server-2.5.0.tar.gz
https://download.nacos.io/nacos-server/nacos-server-2.5.0.zip?spm=5238cd80.2ef5001f.0.0.3f613b7cmDsk8X&file=nacos-server-2.5.0.zip
```

```bash
#安装java环境
[root@ubuntu2404 ~]#apt update && apt install -y openjdk-21-jdk
```

```bash
[root@ubuntu2404 ~]#ls
nacos-server-2.5.0.tar.gz  nacos-server-2.5.0.zip
[root@ubuntu2404 ~]#unzip nacos-server-2.5.0.zip -d /usr/local/
[root@ubuntu2404 ~]#cd /usr/local/
[root@ubuntu2404 local]#cd nacos/
[root@ubuntu2404 nacos]#ll bin/
total 28
drwxr-xr-x 2 root root 4096 Jan 21 10:56 ./
drwxr-xr-x 5 root root 4096 Jan 21 10:56 ../
-rwxr-xr-x 1 root root 1189 Nov 29 10:57 shutdown.cmd*
-rwxr-xr-x 1 root root  875 Nov 29 10:57 shutdown.sh*	#停止脚本
-rwxr-xr-x 1 root root 3560 Jan 21 10:54 startup.cmd*
-rwxr-xr-x 1 root root 5713 Jan 21 10:54 startup.sh*	#启动脚本

[root@ubuntu2404 nacos]#vim conf/application.properties
#启动Prometheus监控
management.endpoints.web.exposure.include=prometheus,health

#启动nacos
[root@ubuntu2404 nacos]#bash ./bin/startup.sh -m standalone
```

```bash
#修改配置，可选
[root@ubuntu2204 ~]#vi /usr/local/nacos/conf/application.properties
#修改默认的访问URL路径
#server.servlet.contextPath=/nacos
server.servlet.contextPath=/
#***********Expose prometheus and health **************************# 
#取消下面行注释开启prometheus监控，指标路径：http://127.0.0.1:8848/nacos/actuator/prometheus
management.endpoints.web.exposure.include=prometheus,health
```



```bash
访问
http://10.0.0.200:8848/nacos

prometheus监听地址
http://10.0.0.200:8848/nacos/actuator/prometheus
```



#### 脚本部署单机Nacos基于Derby数据库

```bash
[root@ubuntu2404 ~]#cat install_nacos_single_node.sh 
#!/bin/bash
#
#********************************************************************
#Author:            wangxiaochun
#QQ:                29308620
#Date:              2023-06-05
#FileName:          install_nacos_single_node.sh
#URL:               http://www.wangxiaochun.com
#Description:       The test script
#Copyright (C):     2023 All rights reserved
#********************************************************************

#支持在线和离线安装,建议离线安装,在线可能下载很慢

NACOS_VERSION=2.5.0
#NACOS_VERSION=2.4.3
#NACOS_VERSION=2.4.0.1
#NACOS_VERSION=2.3.2
#NACOS_VERSION=2.3.0
#NACOS_VERSION=2.2.3

#NACOS_FILE=nacos-server-${NACOS_VERSION}.tar.gz
NACOS_FILE=nacos-server-${NACOS_VERSION}.zip

GITHUB_PROXY=https://mirror.ghproxy.com/

NACOS_URL=https://github.com/alibaba/nacos/releases/download/${NACOS_VERSION}/${NACOS_FILE}

INSTALL_DIR=/usr/local/nacos


HOST=`hostname -I|awk '{print $1}'`

.  /etc/os-release

color () {
    RES_COL=60
    MOVE_TO_COL="echo -en \\033[${RES_COL}G"
    SETCOLOR_SUCCESS="echo -en \\033[1;32m"
    SETCOLOR_FAILURE="echo -en \\033[1;31m"
    SETCOLOR_WARNING="echo -en \\033[1;33m"
    SETCOLOR_NORMAL="echo -en \E[0m"
    echo -n "$1" && $MOVE_TO_COL
    echo -n "["
    if [ $2 = "success" -o $2 = "0" ] ;then
        ${SETCOLOR_SUCCESS}
        echo -n $"  OK  "    
    elif [ $2 = "failure" -o $2 = "1"  ] ;then 
        ${SETCOLOR_FAILURE}
        echo -n $"FAILED"
    else
        ${SETCOLOR_WARNING}
        echo -n $"WARNING"
    fi
    ${SETCOLOR_NORMAL}
    echo -n "]"
    echo 
}


install_jdk() {
    java -version &>/dev/null && { color "JDK 已安装!" 1 ; return;  }
    if command -v yum &>/dev/null ; then
        yum -y install java-1.8.0-openjdk-devel || { color "安装JDK失败!" 1; exit 1; }
    elif command -v apt &>/dev/null ; then
        apt update
            apt install openjdk-21-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
            #apt install openjdk-17-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        #apt install openjdk-11-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        #apt install openjdk-8-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
    else
       color "不支持当前操作系统!" 1
       exit 1
    fi
    java -version && { color "安装 JDK 完成!" 0 ; } || { color "安装JDK失败!" 1; exit 1; } 
}


install_nacos() {
    if [ -f ${NACOS_FILE} ] ;then
        cp ${NACOS_FILE} /usr/local/src/
    else
        wget -P /usr/local/src/ --no-check-certificate ${GITHUB_PROXY}$NACOS_URL || { color  "下载失败!" 1 ;exit ; }
    fi
    #tar xf /usr/local/src/${NACOS_FILE} -C /usr/local
    unzip ${NACOS_FILE} -d /usr/local
    id nacos &> /dev/null || useradd -r -s /sbin/nologin nacos
        chown -R nacos:nacos ${INSTALL_DIR}
    echo "PATH=${INSTALL_DIR}/bin:$PATH" >>  /etc/profile
    .  /etc/profile
   
start_nacos () {   
    cat > /lib/systemd/system/nacos.service <<EOF
[Unit]
Description=nacos.service
After=network.target

[Service]
Type=forking
#Environment=${INSTALL_DIR}
ExecStart=${INSTALL_DIR}/bin/startup.sh -m standalone
ExecStop=${INSTALL_DIR}/bin/shutdown.sh
User=nacos
Group=nacos

[Install]
WantedBy=multi-user.target
EOF
    systemctl daemon-reload
    systemctl enable --now  nacos.service
    systemctl is-active nacos.service
    if [ $? -eq 0 ] ;then 
        color "nacos 安装成功!" 0  
        echo "-------------------------------------------------------------------"
        echo -e "请访问链接: \E[32;1mhttp://$HOST:8848/nacos/\E[0m" 
    else 
        color "nacos 安装失败!" 1
        exit 1
    fi   
}
}



install_jdk

install_nacos

start_nacos
```



#### 单机Nacos基于mysql数据库部署

```bash
#安装mysql数据库
[root@ubuntu2404 ~]#apt install mysql-server -y
[root@ubuntu2404 ~]#vim /etc/mysql/mysql.conf.d/mysqld.cnf
#bind-address       = 127.0.0.1
#mysqlx-bind-address    = 127.0.0.1
[root@ubuntu2404 ~]#systemctl restart mysql.service 

#数据库文件
[root@ubuntu2404 nacos]#ls conf/mysql*.sql
conf/mysql-schema.sql
#准备mysql数据库
[root@ubuntu2404 ~]#mysql
mysql> create database nacos;
#需要指定验证插件，不支持默认插件CachingSha2Password
mysql> create user nacos@'10.0.0.%' identified with mysql_native_password by '123456'; 
mysql> grant all on nacos.* to nacos@'10.0.0.%';
#生成表结构
[root@ubuntu2404 nacos]#mysql -unacos -p123456 -h10.0.0.200 nacos < /usr/local/nacos/conf/mysql-schema.sql 
```

```bash
#修改nacos配置文件，连接mysql
[root@ubuntu2404 ~]#vim /usr/local/nacos/conf/application.properties
spring.sql.init.platform=mysql

db.num=1

db.url.0=jdbc:mysql://10.0.0.200:3306/nacos?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useUnicode=true&useSSL=false&serverTimezone=Asia/Shanghai
db.user.0=nacos
db.password.0=123456
```

```bash
#如果启动,先关闭服务，重新启动服务
[root@ubuntu2404 ~]#bash /usr/local/nacos/bin/shutdown.sh 
#启动服务
[root@ubuntu2404 ~]#bash /usr/local/nacos/bin/startup.sh -m standalone
```



### **服务注册&发现和配置管理**

#### 服务注册

```
curl -X POST 'http://127.0.0.1:8848/nacos/v1/ns/instance?serviceName=nacos.naming.serviceName&ip=20.18.7.10&port=8080'
```

#### 服务发现

```
curl -X GET 'http://127.0.0.1:8848/nacos/v1/ns/instance/list?serviceName=nacos.naming.serviceName'
```

#### 发布配置

```
curl -X POST "http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test&content=HelloWorld"
```

#### 获取配置

```
curl -X GET "http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test"
```



### **开启鉴权**

#### 开启鉴权

```
https://nacos.io/docs/latest/manual/admin/auth/
```

```bash
#生成toke的值，至少32位，否则无法启动
[root@ubuntu2404 ~]#openssl rand -base64 33
pTjnlks5DQKF9oG16nPNxulcHw3OKNkj3ffoUxh5fWKi

[root@ubuntu2404 ~]#vim /usr/local/nacos/conf/application.properties
#修改配置文件,共四行内容
### The auth system to use, currently only 'nacos' and 'ldap' is supported:
nacos.core.auth.system.type=nacos

### If turn on auth system:
nacos.core.auth.enabled=true

### Turn on/off caching of auth information. By turning on this switch, the update of auth information would have a 15 seconds delay.
nacos.core.auth.caching.enabled=true

### Since 1.4.1, Turn on/off white auth for user-agent: nacos-server, only for upgrade from old version.
nacos.core.auth.enable.userAgentAuthWhite=false

### Since 1.4.1, worked when nacos.core.auth.enabled=true and nacos.core.auth.enable.userAgentAuthWhite=false.
### The two properties is the white list for auth and used by identity the request from other server.
nacos.core.auth.server.identity.key=kang
nacos.core.auth.server.identity.value=kang

### worked when nacos.core.auth.system.type=nacos
### The token expiration in seconds:
nacos.core.auth.plugin.nacos.token.cache.enable=false
nacos.core.auth.plugin.nacos.token.expire.seconds=18000
### The default token (Base64 String):
nacos.core.auth.plugin.nacos.token.secret.key=pTjnlks5DQKF9oG16nPNxulcHw3OKNkj3ffoUxh5fWKi
```

```bash
#重启服务生效
[root@ubuntu2404 ~]#bash /usr/local/nacos/bin/startup.sh -m standalone
```

测试访问第一次登录需要指定nacos的新密码，默认用户名和密码都是nacos

#### 权限控制

##### 创建角色

权限控制——》角色管理——》绑定角色

##### 权限管理

注意：先绑定角色，才能创建权限

权限控制——》权限管理——》添加权限



后续API访问需要经过鉴权后才能访问

```bash
#直接访问失败
[root@ubuntu2404 ~]#curl -X POST "http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test&content=HelloWorld"
{"timestamp":"2025-03-11T21:24:24.657+08:00","status":403,"error":"Forbidden","message":"user not found!","path":"/nacos/v1/cs/configs"}

#登录验证,获取Token
[root@ubuntu2404 ~]#curl -X POST 'http://127.0.0.1:8848/nacos/v1/auth/login' -d 'username=nacos&password=nacos'
{"accessToken":"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJuYWNvcyIsImV4cCI6MTc0MTcxNzUxM30._tsxSyJT19W771dz4uFXD2UKDTh_Lt_DNAJjMCtNl2I","tokenTtl":18000,"globalAdmin":true,"username":"nacos"}

#利用Token访问
[root@ubuntu2404 ~]#TOKEN=eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJuYWNvcyIsImV4cCI6MTc0MTcxNzUxM30._tsxSyJT19W771dz4uFXD2UKDTh_Lt_DNAJjMCtNl2I

#服务注册
[root@ubuntu2404 ~]#curl -X POST "http://127.0.0.1:8848/nacos/v1/ns/instance?accessToken=$TOKEN&serviceName=nacos.wang.serviceName&ip=1.2.3.4&port=8080"
ok


```



### **Nacos** **集群部署**

```
https://nacos.io/zh-cn/docs/cluster-mode-quick-start.html
```

### 集群部署架构图

因此开源的时候推荐用户把所有服务列表放到一个vip下面，然后挂到一个域名下面

[http://ip1](http://ip1/):port/openAPI 直连ip模式，机器挂则需要修改ip才可以使用。

[http://SLB](http://slb/):port/openAPI 挂载SLB模式(内网SLB，不可暴露到公网，以免带来安全风险)，直连SLB即可，下面挂server真实ip，可读性不好。

[http://nacos.com](http://nacos.com/):port/openAPI 域名 + SLB模式(内网SLB，不可暴露到公网，以免带来安全风险)，可读性好，而且换ip方便，推荐模

<img src="5day-png/35Nacos集群部署.png" alt="image-20250311214101246" style="zoom:50%;" />

![image-20250311214259761](5day-png/35Nacos集群部署1.png)

以上三种模式，推荐使用第三种，即用户把所有服务列表放到一个vip下面，然后挂到一个域名下面

**端口情况**

| 端口 | **与主端口的偏移量** | 描述                                                       |
| ---- | -------------------- | ---------------------------------------------------------- |
| 8848 | 0                    | 主端口，客户端、控制台及OpenAPI所使用的HTTP端口            |
| 9848 | 1000                 | 客户端gRPC请求服务端端口，用于客户端向服务端发起连接和请求 |
| 9849 | 1001                 | 服务端gRPC请求服务端端口，用于服务间同步等                 |
| 7848 | -1000                | raft请求服务端端口，用于处理服务端间的Raft相关请求         |

注意：

- 使用VIP/nginx请求时，需要配置成TCP转发，不能配置http2转发，否则连接会被nginx断开
- 9849和7848端口为服务端之间的通信端口，请勿暴露到外部网络环境和客户端测

### 部署nacos集群

<img src="5day-png/35Nacos集群部署2.png" alt="image-20250311214521613" style="zoom:50%;" />

| 服务               | ip         |
| ------------------ | ---------- |
| mysql              | 10.0.0.200 |
| nacos1             | 10.0.0.201 |
| nacos2             | 10.0.0.202 |
| nacos3             | 10.0.0.203 |
| haproxy1 keeplived | 10.0.0.204 |
| haproxy2 keeplived | 10.0.0.205 |
| vip                | 10.0.0.100 |



```bash
#数据库准备
#安装mysql数据库
[root@ubuntu2404 ~]#apt install mysql-server -y
[root@ubuntu2404 ~]#sed -i '/127.0.0.1/s/^/#/' /etc/mysql/mysql.conf.d/mysqld.cnf
[root@ubuntu2404 ~]#systemctl restart mysql.service 

#数据库文件
[root@ubuntu2404 nacos]#ls conf/mysql*.sql
conf/mysql-schema.sql
#准备mysql数据库
[root@ubuntu2404 ~]#mysql
mysql> create database nacos;
#需要指定验证插件，不支持默认插件CachingSha2Password
mysql> create user nacos@'10.0.0.%' identified with mysql_native_password by '123456'; 
mysql> grant all on nacos.* to nacos@'10.0.0.%';
#生成表结构
[root@ubuntu2404 ~]#unzip 
nacos-server-2.5.0.zip
[root@ubuntu2404 ~]#unzip nacos-server-2.5.0.zip 
[root@ubuntu2404 ~]#ls nacos/conf/mysql-schema.sql 
nacos/conf/mysql-schema.sql
[root@ubuntu2404 ~]#mysql -unacos -p123456 -h10.0.0.200 nacos < nacos/conf/mysql-schema.sql
```

```
https://github.com/alibaba/nacos/releases/download/2.5.0/nacos-server-2.5.0.tar.gz
https://download.nacos.io/nacos-server/nacos-server-2.5.0.zip?spm=5238cd80.2ef5001f.0.0.3f613b7cmDsk8X&file=nacos-server-2.5.0.zip
```

```bash
#修改hosts文件
[root@ubuntu2404 ~]#hostnamectl hostname node1
[root@ubuntu2404 ~]#hostnamectl hostname node2
[root@ubuntu2404 ~]#hostnamectl hostname node3
[root@node1 ~]#vim /etc/hosts
10.0.0.201 node1
10.0.0.202 node2
10.0.0.203 node3
[root@node1 ~]#scp /etc/hosts node2:/etc/hosts
[root@node1 ~]#scp /etc/hosts node3:/etc/hosts
```

```bash
#在nacos节点上安装java和nacos
[root@node1 ~]#apt update && apt -y install openjdk-21-jdk
[root@node1 ~]#ls
install_nacos_single_node.sh  nacos-server-2.5.0.zip
[root@node1 ~]scp install_nacos_single_node.sh node2:
[root@node1 ~]scp install_nacos_single_node.sh node3:
[root@node1 ~]#vim install_nacos_single_node.sh 
#!/bin/bash

#支持在线和离线安装,建议离线安装,在线可能下载很慢

NACOS_VERSION=2.5.0
#NACOS_VERSION=2.4.3
#NACOS_VERSION=2.4.0.1
#NACOS_VERSION=2.3.2
#NACOS_VERSION=2.3.0
#NACOS_VERSION=2.2.3

#NACOS_FILE=nacos-server-${NACOS_VERSION}.tar.gz
NACOS_FILE=nacos-server-${NACOS_VERSION}.zip

GITHUB_PROXY=https://mirror.ghproxy.com/

NACOS_URL=https://github.com/alibaba/nacos/releases/download/${NACOS_VERSION}/${NACOS_FILE}

INSTALL_DIR=/usr/local/nacos


HOST=`hostname -I|awk '{print $1}'`

.  /etc/os-release

color () {
    RES_COL=60
    MOVE_TO_COL="echo -en \\033[${RES_COL}G"
    SETCOLOR_SUCCESS="echo -en \\033[1;32m"
    SETCOLOR_FAILURE="echo -en \\033[1;31m"
    SETCOLOR_WARNING="echo -en \\033[1;33m"
    SETCOLOR_NORMAL="echo -en \E[0m"
    echo -n "$1" && $MOVE_TO_COL
    echo -n "["
    if [ $2 = "success" -o $2 = "0" ] ;then
        ${SETCOLOR_SUCCESS}
        echo -n $"  OK  "    
    elif [ $2 = "failure" -o $2 = "1"  ] ;then 
        ${SETCOLOR_FAILURE}
        echo -n $"FAILED"
    else
        ${SETCOLOR_WARNING}
        echo -n $"WARNING"
    fi
    ${SETCOLOR_NORMAL}
    echo -n "]"
    echo 
}


install_jdk() {
    java -version &>/dev/null && { color "JDK 已安装!" 1 ; return;  }
    if command -v yum &>/dev/null ; then
        yum -y install java-1.8.0-openjdk-devel || { color "安装JDK失败!" 1; exit 1; }
    elif command -v apt &>/dev/null ; then
        apt update
            apt install openjdk-21-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
            #apt install openjdk-17-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        #apt install openjdk-11-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
        #apt install openjdk-8-jdk -y || { color "安装JDK失败!" 1; exit 1; } 
    else
       color "不支持当前操作系统!" 1
       exit 1
    fi
    java -version && { color "安装 JDK 完成!" 0 ; } || { color "安装JDK失败!" 1; exit 1; } 
}


install_nacos() {
    if [ -f ${NACOS_FILE} ] ;then
        cp ${NACOS_FILE} /usr/local/src/
    else
        wget -P /usr/local/src/ --no-check-certificate ${GITHUB_PROXY}$NACOS_URL || { color  "下载失败!" 1 ;exit ; }
    fi
    #tar xf /usr/local/src/${NACOS_FILE} -C /usr/local
    unzip ${NACOS_FILE} -d /usr/local
    id nacos &> /dev/null || useradd -r -s /sbin/nologin nacos
        chown -R nacos:nacos ${INSTALL_DIR}
    echo "PATH=${INSTALL_DIR}/bin:$PATH" >>  /etc/profile
    .  /etc/profile
   
start_nacos () {   
    cat > /lib/systemd/system/nacos.service <<EOF
[Unit]
Description=nacos.service
After=network.target

[Service]
Type=forking
#Environment=${INSTALL_DIR}
ExecStart=${INSTALL_DIR}/bin/startup.sh -m standalone
ExecStop=${INSTALL_DIR}/bin/shutdown.sh
User=nacos
Group=nacos

[Install]
WantedBy=multi-user.target
EOF
    systemctl daemon-reload
    systemctl enable --now  nacos.service
    systemctl is-active nacos.service
    if [ $? -eq 0 ] ;then 
        color "nacos 安装成功!" 0  
        echo "-------------------------------------------------------------------"
        echo -e "请访问链接: \E[32;1mhttp://$HOST:8848/nacos/\E[0m" 
    else 
        color "nacos 安装失败!" 1
        exit 1
    fi   
}
}



install_jdk

install_nacos

start_nacos

[root@node1 ~]#bash install_nacos_single_node.sh
[root@node2 ~]#bash install_nacos_single_node.sh
[root@node3 ~]#bash install_nacos_single_node.sh
[root@node1 ~]#systemctl stop nacos.service 
[root@node2 ~]#systemctl stop nacos.service 
[root@node3 ~]#systemctl stop nacos.service 
```

```bash
#创建集群文件
[root@node2 ~]#vim /usr/local/nacos/conf/cluster.conf
# ip:port
10.0.0.201:8848
10.0.0.202:8848
10.0.0.203:8848

#所有节点同步配置
[root@node1 ~]#scp /usr/local/nacos/conf/cluster.conf node2:/usr/local/nacos/conf/cluster.conf   
[root@node1 ~]#scp /usr/local/nacos/conf/cluster.conf node3:/usr/local/nacos/conf/cluster.conf
```

**修改** **Nacos** **配置文件** **application.properties**

```bash
#修改配置连接mysql
[root@node1 ~]#vim /usr/local/nacos/conf/application.properties
### Count of DB:
db.num=1

### Connect URL of DB:
db.url.0=jdbc:mysql://10.0.0.200:3306/nacos?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useUnicode=true&useSSL=false&serverTimezone=Asia/Shanghai
db.user.0=nacos
db.password.0=123456

#所有节点同步配置
[root@node1 ~]#scp /usr/local/nacos/conf/application.properties node2:/usr/local/nacos/conf/application.properties
[root@node1 ~]#scp /usr/local/nacos/conf/application.properties node3:/usr/local/nacos/conf/application.properties
```

```bash
#修改配置文件
[root@node3 ~]#vim /lib/systemd/system/nacos.service
[Unit]
Description=nacos.service
After=network.target

[Service]
Type=forking
#Environment=/usr/local/nacos
ExecStart=/usr/local/nacos/bin/startup.sh
ExecStop=/usr/local/nacos/bin/shutdown.sh
User=nacos
Group=nacos

[Install]
WantedBy=multi-user.target

[root@node1 ~]#scp /lib/systemd/system/nacos.service node2:/lib/systemd/system/nacos.service
[root@node1 ~]#scp /lib/systemd/system/nacos.service node3:/lib/systemd/system/nacos.service

#启动nacos服务
[root@node1 ~]#systemctl daemon-reload
[root@node2 ~]#systemctl daemon-reload
[root@node3 ~]#systemctl daemon-reload
[root@node1 ~]#systemctl stop nacos
[root@node1 ~]#systemctl restart nacos
[root@node2 ~]#systemctl stop nacos
[root@node2 ~]#systemctl restart nacos
[root@node3 ~]#systemctl stop nacos
[root@node3 ~]#systemctl restart nacos
```



